[2024-04-22 08:19:03,394 INFO expe.py save_to_json l.286] Expe saved as JSON to expe/01--10Q_0C_0F_0M_0A_0HE_0AE_2024-04-22_08h19,03.json
[2024-04-22 08:21:20,028 INFO expe.py save_to_json l.286] Expe saved as JSON to expe/01. Questions/questions--10Q_0C_0F_0M_0A_0HE_0AE_2024-04-22_08h21,20.json
[2024-04-22 08:56:06,424 INFO expe.py save_to_json l.286] Expe saved as JSON to expe/01. Questions/questions--10Q_170C_0F_0M_0A_0HE_0AE_2024-04-22_08h56,06.json
[2024-04-22 09:15:08,744 DEBUG main.py <module> l.31] MAIN STARTS
[2024-04-22 09:16:57,514 DEBUG main.py <module> l.31] MAIN STARTS
[2024-04-22 09:16:57,523 INFO generators.py generate l.488] (1/10) *** AnsGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:16:57,523 INFO generators.py gen_for_qa l.558] (1/10) Reuse existing chunks
[2024-04-22 09:16:57,523 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-4"
[2024-04-22 09:16:57,524 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:16:57,524 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:01,323 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:17:01,323 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gemini-pro"
[2024-04-22 09:17:01,324 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:01,324 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:01,394 ERROR generators.py complete l.413] (1/10) The following exception occurred with prompt meta={} user='What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?' system='Contexte :  Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered \n\n LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780. \n\n Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116. \n\n 4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. \n\n John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision. \n\n In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. \n\n \n La question est What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?'
VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 334, in completion
    import vertexai
ModuleNotFoundError: No module named 'vertexai'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 336, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.VertexAIError: vertexai import failed please run `pip install google-cloud-aiplatform`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro

[2024-04-22 09:17:01,481 DEBUG generators.py generate l.386] (1/10) Reuse post-processing
[2024-04-22 09:17:01,481 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:17:01,482 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:01,482 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:03,150 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:17:03,150 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:17:03,151 INFO generators.py generate l.488] (2/10) *** AnsGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:17:03,152 INFO generators.py gen_for_qa l.558] (2/10) Reuse existing chunks
[2024-04-22 09:17:03,152 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-4"
[2024-04-22 09:17:03,153 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:03,154 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:10,011 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:17:10,012 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gemini-pro"
[2024-04-22 09:17:10,013 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:10,014 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:10,068 ERROR generators.py complete l.413] (2/10) The following exception occurred with prompt meta={} user='How is the value matrix generated in the self-attention block of Funnel-Transformer?' system='Contexte :  4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. \n\n 2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5. \n\n 4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5. \n\n 1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2. \n\n 122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24). \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads. \n\n Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T. \n\n The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. \n\n h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. \n\n The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6) \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively. \n\n \n La question est How is the value matrix generated in the self-attention block of Funnel-Transformer?'
VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 334, in completion
    import vertexai
ModuleNotFoundError: No module named 'vertexai'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 336, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.VertexAIError: vertexai import failed please run `pip install google-cloud-aiplatform`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro

[2024-04-22 09:17:10,106 DEBUG generators.py generate l.386] (2/10) Reuse post-processing
[2024-04-22 09:17:10,106 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:17:10,107 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:10,107 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:14,179 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:17:14,179 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:17:14,179 INFO generators.py generate l.488] (3/10) *** AnsGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:17:14,179 INFO generators.py gen_for_qa l.558] (3/10) Reuse existing chunks
[2024-04-22 09:17:14,180 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-4"
[2024-04-22 09:17:14,180 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:14,180 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:18,728 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:17:18,729 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gemini-pro"
[2024-04-22 09:17:18,729 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:18,729 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:18,785 ERROR generators.py complete l.413] (3/10) The following exception occurred with prompt meta={} user="How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?" system='Contexte :  60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so. \n\n When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. \n\n Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. \n\n 60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place . \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n"text ": t,\n"label" :"positive "ifbest_index == 1 else\n"negative" ,\n"score" : score_pair[best_index]\n})\nreturn results\npredict([" I love that restaurant!", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n ", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n"text ": t,\n"label" :"positive "ifbest_index == 1 else\n"negative" ,\n"score" : score_pair[best_index]\n})\nreturn results\npredict([" I love that restaurant!", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. \n\n Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well. \n\n The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. \n\n Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither . \n\n \n La question est How does Mama Ricotta\'s differ from traditional Italian restaurants in terms of its location?'
VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 334, in completion
    import vertexai
ModuleNotFoundError: No module named 'vertexai'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 336, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.VertexAIError: vertexai import failed please run `pip install google-cloud-aiplatform`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro

[2024-04-22 09:17:18,826 DEBUG generators.py generate l.386] (3/10) Reuse post-processing
[2024-04-22 09:17:18,826 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:17:18,826 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:18,826 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:21,177 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:17:21,177 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:17:21,177 INFO generators.py generate l.488] (4/10) *** AnsGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:17:21,177 INFO generators.py gen_for_qa l.558] (4/10) Reuse existing chunks
[2024-04-22 09:17:21,178 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-4"
[2024-04-22 09:17:21,179 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:21,180 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:23,430 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:17:23,431 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gemini-pro"
[2024-04-22 09:17:23,431 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:23,431 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:23,489 ERROR generators.py complete l.413] (4/10) The following exception occurred with prompt meta={} user='How many patents does Kenneth L. Graham have related to natural language processing?' system='Contexte :  He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii \n\n Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. \n\n Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529 \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n [184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n \n La question est How many patents does Kenneth L. Graham have related to natural language processing?'
VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 334, in completion
    import vertexai
ModuleNotFoundError: No module named 'vertexai'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 336, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.VertexAIError: vertexai import failed please run `pip install google-cloud-aiplatform`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7904, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: VertexAIException - vertexai import failed please run `pip install google-cloud-aiplatform`
model: gemini-pro

[2024-04-22 09:17:23,575 DEBUG generators.py generate l.386] (4/10) Reuse post-processing
[2024-04-22 09:17:23,575 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:17:23,576 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:23,576 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:17:24,674 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:17:24,674 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:17:24,675 INFO generators.py generate l.488] (5/10) *** AnsGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:17:24,675 INFO generators.py gen_for_qa l.558] (5/10) Reuse existing chunks
[2024-04-22 09:17:24,675 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gpt-4"
[2024-04-22 09:17:24,675 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:17:24,677 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:26,189 DEBUG main.py <module> l.31] MAIN STARTS
[2024-04-22 09:19:26,195 INFO generators.py generate l.488] (1/10) *** AnsGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:19:26,196 INFO generators.py gen_for_qa l.558] (1/10) Reuse existing chunks
[2024-04-22 09:19:26,196 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-4"
[2024-04-22 09:19:26,196 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:26,197 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:28,922 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:19:28,923 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gemini-pro"
[2024-04-22 09:19:28,923 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:28,923 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:48,732 ERROR generators.py complete l.413] (1/10) The following exception occurred with prompt meta={} user='What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?' system='Contexte :  Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered \n\n LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780. \n\n Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116. \n\n 4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. \n\n John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision. \n\n In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. \n\n \n La question est What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:19:48,795 DEBUG generators.py generate l.386] (1/10) Reuse post-processing
[2024-04-22 09:19:48,798 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:19:48,798 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:48,800 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:50,353 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:19:50,353 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:19:50,353 INFO generators.py generate l.488] (2/10) *** AnsGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:19:50,353 INFO generators.py gen_for_qa l.558] (2/10) Reuse existing chunks
[2024-04-22 09:19:50,354 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-4"
[2024-04-22 09:19:50,354 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:50,354 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:55,164 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:19:55,169 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gemini-pro"
[2024-04-22 09:19:55,170 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:55,170 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:55,229 ERROR generators.py complete l.413] (2/10) The following exception occurred with prompt meta={} user='How is the value matrix generated in the self-attention block of Funnel-Transformer?' system='Contexte :  4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. \n\n 2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5. \n\n 4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5. \n\n 1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2. \n\n 122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24). \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads. \n\n Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T. \n\n The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. \n\n h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. \n\n The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6) \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively. \n\n \n La question est How is the value matrix generated in the self-attention block of Funnel-Transformer?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:19:55,253 DEBUG generators.py generate l.386] (2/10) Reuse post-processing
[2024-04-22 09:19:55,253 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:19:55,254 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:55,254 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:19:58,778 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:19:58,778 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:19:58,778 INFO generators.py generate l.488] (3/10) *** AnsGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:19:58,778 INFO generators.py gen_for_qa l.558] (3/10) Reuse existing chunks
[2024-04-22 09:19:58,778 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-4"
[2024-04-22 09:19:58,779 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:19:58,779 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:02,552 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:20:02,553 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gemini-pro"
[2024-04-22 09:20:02,553 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:02,554 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:02,617 ERROR generators.py complete l.413] (3/10) The following exception occurred with prompt meta={} user="How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?" system='Contexte :  60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so. \n\n When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. \n\n Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. \n\n 60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place . \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n"text ": t,\n"label" :"positive "ifbest_index == 1 else\n"negative" ,\n"score" : score_pair[best_index]\n})\nreturn results\npredict([" I love that restaurant!", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n ", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n"text ": t,\n"label" :"positive "ifbest_index == 1 else\n"negative" ,\n"score" : score_pair[best_index]\n})\nreturn results\npredict([" I love that restaurant!", "I hate italian food."])\n#>> [{"label": ’positive’, "score": 0.99751616, "text": ’I love\nthat restaurant!’},\n# {"label ": ’negative’, "score": 0.9791407, "text": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. \n\n Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well. \n\n The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. \n\n Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither . \n\n \n La question est How does Mama Ricotta\'s differ from traditional Italian restaurants in terms of its location?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:20:02,644 DEBUG generators.py generate l.386] (3/10) Reuse post-processing
[2024-04-22 09:20:02,644 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:20:02,645 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:02,645 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:04,923 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:20:04,931 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:20:04,932 INFO generators.py generate l.488] (4/10) *** AnsGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:20:04,933 INFO generators.py gen_for_qa l.558] (4/10) Reuse existing chunks
[2024-04-22 09:20:04,933 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-4"
[2024-04-22 09:20:04,935 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:04,936 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:06,709 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:20:06,710 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gemini-pro"
[2024-04-22 09:20:06,711 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:06,711 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:06,779 ERROR generators.py complete l.413] (4/10) The following exception occurred with prompt meta={} user='How many patents does Kenneth L. Graham have related to natural language processing?' system='Contexte :  He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii \n\n Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. \n\n Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529 \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n [184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n \n La question est How many patents does Kenneth L. Graham have related to natural language processing?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:20:06,834 DEBUG generators.py generate l.386] (4/10) Reuse post-processing
[2024-04-22 09:20:06,834 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:20:06,839 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:06,841 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:07,835 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:20:07,835 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:20:07,835 INFO generators.py generate l.488] (5/10) *** AnsGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:20:07,835 INFO generators.py gen_for_qa l.558] (5/10) Reuse existing chunks
[2024-04-22 09:20:07,835 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gpt-4"
[2024-04-22 09:20:07,836 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:07,837 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:13,934 DEBUG generators.py generate l.383] (5/10) Post-process Answer
[2024-04-22 09:20:13,935 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gemini-pro"
[2024-04-22 09:20:13,935 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:13,936 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:20,000 ERROR generators.py complete l.413] (5/10) The following exception occurred with prompt meta={} user='In what scenarios is global attention beneficial in transformer models?' system='Contexte :  xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR? \n\n •Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures. They are also rapidly employed in other applications such as\ncomputer vision and audio. \n\n xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. \n\n •Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld. \n\n Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 . \n\n • ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction. \n\n In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. \n\n 285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020). \n\n · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. \n\n • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier. \n\n 3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. \n\n The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n 208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent. \n\n This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses. The Stanford Sentiment Treebank \n\n Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult. \n\n Furthermore,\nself-attention, the critical innovation in the transformers, helps in paral-\nlelizingthecomputationofper-symbolcontext-basedvectorsandcreates\na global receptive ﬁeld where the symbol gets information from all the\nsymbols. On the other hand, the absence of recurrent inductive bias of\nRNNs becomes an issue when solving tasks with inherent hierarchical\nstructures or when the lengths vary signiﬁcantly between the training\nand the unseen data the model predicts. Also, the number of sequen-\ntial computations in transformers is independent of the input size but\nonly dependent on the number of layers, making it computationally non-\nuniversal or Turing incomplete. Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. \n\n \n La question est In what scenarios is global attention beneficial in transformer models?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:20:20,030 DEBUG generators.py generate l.386] (5/10) Reuse post-processing
[2024-04-22 09:20:20,030 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:20:20,031 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:20,032 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:24,982 DEBUG generators.py generate l.383] (5/10) Post-process Answer
[2024-04-22 09:20:24,983 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:20:24,983 INFO generators.py generate l.488] (6/10) *** AnsGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:20:24,983 INFO generators.py gen_for_qa l.558] (6/10) Reuse existing chunks
[2024-04-22 09:20:24,984 INFO generators.py gen_for_qa l.565] (6/10) * Start with LLM "gpt-4"
[2024-04-22 09:20:24,984 DEBUG generators.py generate l.362] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:24,985 DEBUG generators.py generate l.371] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:37,025 DEBUG generators.py generate l.383] (6/10) Post-process Answer
[2024-04-22 09:20:37,032 INFO generators.py gen_for_qa l.565] (6/10) * Start with LLM "gemini-pro"
[2024-04-22 09:20:37,033 DEBUG generators.py generate l.362] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:37,039 DEBUG generators.py generate l.371] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:37,156 ERROR generators.py complete l.413] (6/10) The following exception occurred with prompt meta={} user='Explain the importance of consistent distance between two time-steps across sentences of various lengths.' system='Contexte :  One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective. \n\n Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. \n\n The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages. \n\n 84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM. \n\n The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights. \n\n With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column. \n\n Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD). \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d. \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT. \n\n As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n 124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges. \n\n Next Sentence Prediction (NSP) Many downstream NLP tasks\nrequire understanding the relationship between two sentences, such as\nQuestion Answering (QA) and Natural Language Inference (NLI). Stan-\ndard language models do not pick up this type of knowledge. This moti-\nvatestheNSPtask,whereBERTisfedpairsofsentencesandpre-trained\nto predict if the second sentence should follow the ﬁrst one in a contin-\nuous context. As discussed earlier, the ﬁrst sentence is preﬁxed with the\n[CLS]token, then the two sentences are delimited by the special token\n[SEP]. During NSP task pre-training, the model is given sentence pairs\nwhere 50% of the time the second sentence comes after the ﬁrst sentence\nand the other 50% the second sentence is a random sentence from the\nfull training corpus. The self-attention of Transformer layers encourages \n\n Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. \n\n \n La question est Explain the importance of consistent distance between two time-steps across sentences of various lengths.'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:20:37,205 DEBUG generators.py generate l.386] (6/10) Reuse post-processing
[2024-04-22 09:20:37,205 INFO generators.py gen_for_qa l.565] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:20:37,206 DEBUG generators.py generate l.362] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:37,206 DEBUG generators.py generate l.371] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:42,338 DEBUG generators.py generate l.383] (6/10) Post-process Answer
[2024-04-22 09:20:42,339 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:20:42,340 INFO generators.py generate l.488] (7/10) *** AnsGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:20:42,340 INFO generators.py gen_for_qa l.558] (7/10) Reuse existing chunks
[2024-04-22 09:20:42,340 INFO generators.py gen_for_qa l.565] (7/10) * Start with LLM "gpt-4"
[2024-04-22 09:20:42,341 DEBUG generators.py generate l.362] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:42,341 DEBUG generators.py generate l.371] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:51,257 DEBUG generators.py generate l.383] (7/10) Post-process Answer
[2024-04-22 09:20:51,260 INFO generators.py gen_for_qa l.565] (7/10) * Start with LLM "gemini-pro"
[2024-04-22 09:20:51,260 DEBUG generators.py generate l.362] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:51,269 DEBUG generators.py generate l.371] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:20:57,397 ERROR generators.py complete l.413] (7/10) The following exception occurred with prompt meta={} user='Describe the computation process for each hidden unit in layer normalization.' system='Contexte :  2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. \n\n Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch. \n\n 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]. \n\n For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. \n\n Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6. \n\n ˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3. \n\n maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2. \n\n Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22) \n\n 3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2. \n\n 3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6. \n\n 32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4. \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831. \n\n Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4) \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020). \n\n \n La question est Describe the computation process for each hidden unit in layer normalization.'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:20:57,692 DEBUG generators.py generate l.386] (7/10) Reuse post-processing
[2024-04-22 09:20:57,693 INFO generators.py gen_for_qa l.565] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:20:57,694 DEBUG generators.py generate l.362] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:20:57,695 DEBUG generators.py generate l.371] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:03,494 DEBUG generators.py generate l.383] (7/10) Post-process Answer
[2024-04-22 09:21:03,496 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:21:03,497 INFO generators.py generate l.488] (8/10) *** AnsGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:21:03,497 INFO generators.py gen_for_qa l.558] (8/10) Reuse existing chunks
[2024-04-22 09:21:03,498 INFO generators.py gen_for_qa l.565] (8/10) * Start with LLM "gpt-4"
[2024-04-22 09:21:03,498 DEBUG generators.py generate l.362] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:03,498 DEBUG generators.py generate l.371] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:18,308 DEBUG generators.py generate l.383] (8/10) Post-process Answer
[2024-04-22 09:21:18,358 INFO generators.py gen_for_qa l.565] (8/10) * Start with LLM "gemini-pro"
[2024-04-22 09:21:18,713 DEBUG generators.py generate l.362] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:18,831 DEBUG generators.py generate l.371] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:24,895 ERROR generators.py complete l.413] (8/10) The following exception occurred with prompt meta={} user='Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications.' system='Contexte :  xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3. \n\n Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength. \n\n And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. \n\n The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length. \n\n 5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. \n\n It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69) \n\n 4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n The Longformer model,\ndiscussed in section 5.2.2.1, and Big Bird model, discussed in section\n5.2.2.4arebothexamplesofattentionwithpriors,sinceeachusesspeciﬁc\nattention patterns, like sliding window attention in sections 5.2.2.1 and\n5.2.2.4. We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. \n\n 2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space. \n\n So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. \n\n Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing. \n\n K)V,∈RL1×dv×h(5.59)\nwhereαis the attention logits, A(Q, K)are the attention weights,\nandC(Q,K,V)is the “context” vector representing the output of the\nhattention heads prior to concatenation of the attention heads and the\nﬁnal projection layer.\nPartitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv. \n\n Partitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv.\nProjecting the attention logits Next, the attention logits αare\nprojected with a linear layer that mixes the query-key attention heads\nwith the attention logit/weight heads, Wα∈Rhk×h, and the attention \n\n The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs. \n\n \n La question est Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications.'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:21:24,931 DEBUG generators.py generate l.386] (8/10) Reuse post-processing
[2024-04-22 09:21:24,931 INFO generators.py gen_for_qa l.565] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:21:24,932 DEBUG generators.py generate l.362] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:24,932 DEBUG generators.py generate l.371] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:32,608 DEBUG generators.py generate l.383] (8/10) Post-process Answer
[2024-04-22 09:21:32,610 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:21:32,610 INFO generators.py generate l.488] (9/10) *** AnsGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:21:32,611 INFO generators.py gen_for_qa l.558] (9/10) Reuse existing chunks
[2024-04-22 09:21:32,612 INFO generators.py gen_for_qa l.565] (9/10) * Start with LLM "gpt-4"
[2024-04-22 09:21:32,612 DEBUG generators.py generate l.362] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:32,613 DEBUG generators.py generate l.371] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:42,614 DEBUG generators.py generate l.383] (9/10) Post-process Answer
[2024-04-22 09:21:42,614 INFO generators.py gen_for_qa l.565] (9/10) * Start with LLM "gemini-pro"
[2024-04-22 09:21:42,615 DEBUG generators.py generate l.362] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:42,615 DEBUG generators.py generate l.371] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:42,674 ERROR generators.py complete l.413] (9/10) The following exception occurred with prompt meta={} user='What is the role of the "embedding" and "label" variables in the compute_loss function?' system='Contexte :  It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen- \n\n introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. \n\n int]):\n"Returns a function that encodes each text example and each\nlabel "\ndef encode(batch):\nbatch[ "embedding" ] = embed_text(batch["text"])\nbatch[ "label" ] = [label2int[str(x)] for xin\nbatch[ "label" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric("accuracy")\nself.val_acc = load_metric("accuracy")\nself.test_acc = load_metric("accuracy")\nself.hidden_dims = hidden_dims\nself. \n\n test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n"Returns a function that encodes each text example and each\nlabel "\ndef encode(batch):\nbatch[ "embedding" ] = embed_text(batch["text"])\nbatch[ "label" ] = [label2int[str(x)] for xin\nbatch[ "label" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric("accuracy")\nself. \n\n Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n"Returns a function that encodes each text example and each\nlabel "\ndef encode(batch):\nbatch[ "embedding" ] = embed_text(batch["text"])\nbatch[ "label" ] = [label2int[str(x)] for xin\nbatch[ "label" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5, \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. \n\n Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi. \n\n The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks. \n\n Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers. \n\n The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not. \n\n 86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\uf8fa\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi. \n\n Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\uf8fa\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C). \n\n (a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. This populates\nthe gradients back through the model to embedded inputs, showing the\nresulting distribution. Finally, we repeat this process for each generated\ntoken and visualize the resulting matrix. \n\n found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token. \n\n Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. \n\n \n La question est What is the role of the "embedding" and "label" variables in the compute_loss function?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:21:42,716 DEBUG generators.py generate l.386] (9/10) Reuse post-processing
[2024-04-22 09:21:42,717 INFO generators.py gen_for_qa l.565] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:21:42,717 DEBUG generators.py generate l.362] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:42,717 DEBUG generators.py generate l.371] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:46,275 DEBUG generators.py generate l.383] (9/10) Post-process Answer
[2024-04-22 09:21:46,276 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:21:46,276 INFO generators.py generate l.488] (10/10) *** AnsGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:21:46,276 INFO generators.py gen_for_qa l.558] (10/10) Reuse existing chunks
[2024-04-22 09:21:46,277 INFO generators.py gen_for_qa l.565] (10/10) * Start with LLM "gpt-4"
[2024-04-22 09:21:46,277 DEBUG generators.py generate l.362] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:46,277 DEBUG generators.py generate l.371] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:53,772 DEBUG generators.py generate l.383] (10/10) Post-process Answer
[2024-04-22 09:21:53,772 INFO generators.py gen_for_qa l.565] (10/10) * Start with LLM "gemini-pro"
[2024-04-22 09:21:53,772 DEBUG generators.py generate l.362] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:21:53,773 DEBUG generators.py generate l.371] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:21:59,857 ERROR generators.py complete l.413] (10/10) The following exception occurred with prompt meta={} user='How does the attention mechanism address the issue of long-distance associations in language processing?' system='Contexte :  As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\uf8fa\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations. \n\n The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. \n\n Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8) \n\n Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the "heterogeneous"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n 7.4 ATTENTION AND EXPLANATION\nAs discussed in the previous section, one of the emerging patterns, es-\npecially in NLP, is to associate the magnitude of the attention weights\nwith the inputs and use it to interpret the model behavior. Next, we dis-\ncuss few papers and the research that impacts how one views attention\nmechanisms and their contribution towards explainability.\n7.4.1 Attention is Not an Explanation\nIn this paper, Jain and Wallace try to ask fundamental questions on\nattention and their interpretations [132]. For example, when we create\nan attention map as shown in Fig. 7.4 that correlates attention weights\ndirectly to the input tokens or weights, the impact of many transforma-\ntions or computations such as intermediate hidden states, query vectors,\nattention techniques is not taken into account. The paper poses two cru-\ncial questions—(i) do the attention heat maps reveal the importance of\nwords/tokens? \n\n \n La question est How does the attention mechanism address the issue of long-distance associations in language processing?'
VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 416, in completion
    creds, _ = google.auth.default(quota_project_id=vertex_project)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/google/auth/_default.py", line 691, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 1703, in completion
    model_response = vertex_ai.completion(
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/llms/vertex_ai.py", line 790, in completion
    raise VertexAIError(status_code=500, message=str(e))
litellm.llms.vertex_ai.VertexAIError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 410, in complete
    ans:dict = completion(messages=messages, model=self.name,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2949, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2177, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2977, in wrapper
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 2875, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/main.py", line 2144, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 8663, in exception_type
    raise e
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/litellm/utils.py", line 7912, in exception_type
    raise APIError(
litellm.exceptions.APIError: VertexAIException - Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
model: gemini-pro

[2024-04-22 09:22:00,493 DEBUG generators.py generate l.386] (10/10) Reuse post-processing
[2024-04-22 09:22:00,860 INFO generators.py gen_for_qa l.565] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:22:00,861 DEBUG generators.py generate l.362] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:22:00,861 DEBUG generators.py generate l.371] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:22:04,168 DEBUG generators.py generate l.383] (10/10) Post-process Answer
[2024-04-22 09:22:04,169 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:24:23,714 DEBUG main.py <module> l.30] MAIN STARTS
[2024-04-22 09:24:23,720 INFO generators.py generate l.488] (1/10) *** AnsGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:24:23,721 INFO generators.py gen_for_qa l.558] (1/10) Reuse existing chunks
[2024-04-22 09:24:23,721 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-4"
[2024-04-22 09:24:23,721 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:23,721 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:26,316 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:24:26,316 INFO generators.py gen_for_qa l.565] (1/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:24:26,318 DEBUG generators.py generate l.362] (1/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:26,318 DEBUG generators.py generate l.371] (1/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:27,777 DEBUG generators.py generate l.383] (1/10) Post-process Answer
[2024-04-22 09:24:27,778 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:24:27,778 INFO generators.py generate l.488] (2/10) *** AnsGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:24:27,778 INFO generators.py gen_for_qa l.558] (2/10) Reuse existing chunks
[2024-04-22 09:24:27,779 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-4"
[2024-04-22 09:24:27,779 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:27,780 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:34,973 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:24:34,973 INFO generators.py gen_for_qa l.565] (2/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:24:34,973 DEBUG generators.py generate l.362] (2/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:34,973 DEBUG generators.py generate l.371] (2/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:39,355 DEBUG generators.py generate l.383] (2/10) Post-process Answer
[2024-04-22 09:24:39,356 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:24:39,356 INFO generators.py generate l.488] (3/10) *** AnsGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:24:39,357 INFO generators.py gen_for_qa l.558] (3/10) Reuse existing chunks
[2024-04-22 09:24:39,357 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-4"
[2024-04-22 09:24:39,357 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:39,358 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:45,587 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:24:45,588 INFO generators.py gen_for_qa l.565] (3/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:24:45,588 DEBUG generators.py generate l.362] (3/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:45,589 DEBUG generators.py generate l.371] (3/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:47,687 DEBUG generators.py generate l.383] (3/10) Post-process Answer
[2024-04-22 09:24:47,687 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:24:47,687 INFO generators.py generate l.488] (4/10) *** AnsGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:24:47,688 INFO generators.py gen_for_qa l.558] (4/10) Reuse existing chunks
[2024-04-22 09:24:47,688 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-4"
[2024-04-22 09:24:47,688 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:47,689 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:50,053 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:24:50,054 INFO generators.py gen_for_qa l.565] (4/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:24:50,055 DEBUG generators.py generate l.362] (4/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:50,056 DEBUG generators.py generate l.371] (4/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:51,140 DEBUG generators.py generate l.383] (4/10) Post-process Answer
[2024-04-22 09:24:51,141 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:24:51,141 INFO generators.py generate l.488] (5/10) *** AnsGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:24:51,141 INFO generators.py gen_for_qa l.558] (5/10) Reuse existing chunks
[2024-04-22 09:24:51,141 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gpt-4"
[2024-04-22 09:24:51,142 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:51,142 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:24:59,126 DEBUG generators.py generate l.383] (5/10) Post-process Answer
[2024-04-22 09:24:59,126 INFO generators.py gen_for_qa l.565] (5/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:24:59,126 DEBUG generators.py generate l.362] (5/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:24:59,127 DEBUG generators.py generate l.371] (5/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:03,841 DEBUG generators.py generate l.383] (5/10) Post-process Answer
[2024-04-22 09:25:03,843 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:25:03,844 INFO generators.py generate l.488] (6/10) *** AnsGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:25:03,845 INFO generators.py gen_for_qa l.558] (6/10) Reuse existing chunks
[2024-04-22 09:25:03,845 INFO generators.py gen_for_qa l.565] (6/10) * Start with LLM "gpt-4"
[2024-04-22 09:25:03,845 DEBUG generators.py generate l.362] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:03,846 DEBUG generators.py generate l.371] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:16,734 DEBUG generators.py generate l.383] (6/10) Post-process Answer
[2024-04-22 09:25:16,734 INFO generators.py gen_for_qa l.565] (6/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:25:16,735 DEBUG generators.py generate l.362] (6/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:16,736 DEBUG generators.py generate l.371] (6/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:21,730 DEBUG generators.py generate l.383] (6/10) Post-process Answer
[2024-04-22 09:25:21,731 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:25:21,731 INFO generators.py generate l.488] (7/10) *** AnsGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:25:21,731 INFO generators.py gen_for_qa l.558] (7/10) Reuse existing chunks
[2024-04-22 09:25:21,732 INFO generators.py gen_for_qa l.565] (7/10) * Start with LLM "gpt-4"
[2024-04-22 09:25:21,732 DEBUG generators.py generate l.362] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:21,733 DEBUG generators.py generate l.371] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:34,241 DEBUG generators.py generate l.383] (7/10) Post-process Answer
[2024-04-22 09:25:34,242 INFO generators.py gen_for_qa l.565] (7/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:25:34,242 DEBUG generators.py generate l.362] (7/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:34,243 DEBUG generators.py generate l.371] (7/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:39,925 DEBUG generators.py generate l.383] (7/10) Post-process Answer
[2024-04-22 09:25:39,925 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:25:39,926 INFO generators.py generate l.488] (8/10) *** AnsGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:25:39,926 INFO generators.py gen_for_qa l.558] (8/10) Reuse existing chunks
[2024-04-22 09:25:39,927 INFO generators.py gen_for_qa l.565] (8/10) * Start with LLM "gpt-4"
[2024-04-22 09:25:39,928 DEBUG generators.py generate l.362] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:39,928 DEBUG generators.py generate l.371] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:25:53,390 DEBUG generators.py generate l.383] (8/10) Post-process Answer
[2024-04-22 09:25:53,391 INFO generators.py gen_for_qa l.565] (8/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:25:53,391 DEBUG generators.py generate l.362] (8/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:25:53,392 DEBUG generators.py generate l.371] (8/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:26:01,788 DEBUG generators.py generate l.383] (8/10) Post-process Answer
[2024-04-22 09:26:01,789 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:26:01,790 INFO generators.py generate l.488] (9/10) *** AnsGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:26:01,790 INFO generators.py gen_for_qa l.558] (9/10) Reuse existing chunks
[2024-04-22 09:26:01,791 INFO generators.py gen_for_qa l.565] (9/10) * Start with LLM "gpt-4"
[2024-04-22 09:26:01,791 DEBUG generators.py generate l.362] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:26:01,791 DEBUG generators.py generate l.371] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:26:09,588 DEBUG generators.py generate l.383] (9/10) Post-process Answer
[2024-04-22 09:26:09,589 INFO generators.py gen_for_qa l.565] (9/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:26:09,589 DEBUG generators.py generate l.362] (9/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:26:09,589 DEBUG generators.py generate l.371] (9/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:26:13,332 DEBUG generators.py generate l.383] (9/10) Post-process Answer
[2024-04-22 09:26:13,333 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:26:13,333 INFO generators.py generate l.488] (10/10) *** AnsGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:26:13,333 INFO generators.py gen_for_qa l.558] (10/10) Reuse existing chunks
[2024-04-22 09:26:13,333 INFO generators.py gen_for_qa l.565] (10/10) * Start with LLM "gpt-4"
[2024-04-22 09:26:13,333 DEBUG generators.py generate l.362] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:26:13,334 DEBUG generators.py generate l.371] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:26:19,909 DEBUG generators.py generate l.383] (10/10) Post-process Answer
[2024-04-22 09:26:19,910 INFO generators.py gen_for_qa l.565] (10/10) * Start with LLM "gpt-3.5-turbo"
[2024-04-22 09:26:19,910 DEBUG generators.py generate l.362] (10/10) Either no Answer / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:26:19,910 DEBUG generators.py generate l.371] (10/10) Either no Answer / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:26:25,013 DEBUG generators.py generate l.383] (10/10) Post-process Answer
[2024-04-22 09:26:25,013 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:26:25,023 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/02. Answers/questions--10Q_170C_0F_2M_20A_0HE_0AE_2024-04-22_09h26,25.json
[2024-04-22 09:29:27,710 DEBUG main.py <module> l.30] MAIN STARTS
[2024-04-22 09:29:27,746 INFO expe.py save_to_html l.299] Expe saved as HTML to /Users/admin/Downloads/Pdf_QA_tester/expe/02. Answers/questions--10Q_170C_0F_2M_20A_0HE_0AE_2024-04-22_09h29,27.html
[2024-04-22 09:29:28,118 INFO expe.py save_to_spreadsheet l.379] Expe saved as Spreadsheet to /Users/admin/Downloads/Pdf_QA_tester/expe/02. Answers/questions--10Q_170C_0F_2M_20A_0HE_0AE_2024-04-22_09h29,27.xlsx
[2024-04-22 09:29:28,120 DEBUG main.py <module> l.42] MAIN ENDS
[2024-04-22 09:29:28,120 DEBUG main.py <module> l.44] Pdf_QA_tester STARTS
[2024-04-22 09:41:56,484 DEBUG main_facts_evals.py <module> l.21] MAIN STARTS
[2024-04-22 09:41:56,502 INFO generators.py generate l.488] (1/10) *** FactGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:41:56,503 INFO generators.py gen_for_qa l.600] (1/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:41:56,503 DEBUG generators.py generate l.362] (1/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:41:56,504 DEBUG generators.py generate l.371] (1/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:41:59,933 DEBUG generators.py generate l.383] (1/10) Post-process Facts
[2024-04-22 09:41:59,934 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:41:59,934 INFO generators.py generate l.488] (2/10) *** FactGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:41:59,934 INFO generators.py gen_for_qa l.600] (2/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:41:59,935 DEBUG generators.py generate l.362] (2/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:41:59,935 DEBUG generators.py generate l.371] (2/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:07,947 DEBUG generators.py generate l.383] (2/10) Post-process Facts
[2024-04-22 09:42:07,948 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:42:07,948 INFO generators.py generate l.488] (3/10) *** FactGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:42:07,948 INFO generators.py gen_for_qa l.600] (3/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:07,948 DEBUG generators.py generate l.362] (3/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:07,949 DEBUG generators.py generate l.371] (3/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:14,737 DEBUG generators.py generate l.383] (3/10) Post-process Facts
[2024-04-22 09:42:14,738 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:42:14,738 INFO generators.py generate l.488] (4/10) *** FactGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:42:14,739 INFO generators.py gen_for_qa l.600] (4/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:14,739 DEBUG generators.py generate l.362] (4/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:14,740 DEBUG generators.py generate l.371] (4/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:19,090 DEBUG generators.py generate l.383] (4/10) Post-process Facts
[2024-04-22 09:42:19,091 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:42:19,091 INFO generators.py generate l.488] (5/10) *** FactGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:42:19,092 INFO generators.py gen_for_qa l.600] (5/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:19,092 DEBUG generators.py generate l.362] (5/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:19,092 DEBUG generators.py generate l.371] (5/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:28,728 DEBUG generators.py generate l.383] (5/10) Post-process Facts
[2024-04-22 09:42:28,729 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:42:28,729 INFO generators.py generate l.488] (6/10) *** FactGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:42:28,729 INFO generators.py gen_for_qa l.600] (6/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:28,729 DEBUG generators.py generate l.362] (6/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:28,729 DEBUG generators.py generate l.371] (6/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:40,495 DEBUG generators.py generate l.383] (6/10) Post-process Facts
[2024-04-22 09:42:40,496 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:42:40,496 INFO generators.py generate l.488] (7/10) *** FactGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:42:40,496 INFO generators.py gen_for_qa l.600] (7/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:40,496 DEBUG generators.py generate l.362] (7/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:40,497 DEBUG generators.py generate l.371] (7/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:42:51,055 DEBUG generators.py generate l.383] (7/10) Post-process Facts
[2024-04-22 09:42:51,057 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:42:51,058 INFO generators.py generate l.488] (8/10) *** FactGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:42:51,058 INFO generators.py gen_for_qa l.600] (8/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:42:51,058 DEBUG generators.py generate l.362] (8/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:42:51,059 DEBUG generators.py generate l.371] (8/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:43:02,222 DEBUG generators.py generate l.383] (8/10) Post-process Facts
[2024-04-22 09:43:02,224 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:43:02,224 INFO generators.py generate l.488] (9/10) *** FactGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:43:02,225 INFO generators.py gen_for_qa l.600] (9/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:43:02,225 DEBUG generators.py generate l.362] (9/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:43:02,226 DEBUG generators.py generate l.371] (9/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:43:08,924 DEBUG generators.py generate l.383] (9/10) Post-process Facts
[2024-04-22 09:43:08,924 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:43:08,925 INFO generators.py generate l.488] (10/10) *** FactGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:43:08,925 INFO generators.py gen_for_qa l.600] (10/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:43:08,925 DEBUG generators.py generate l.362] (10/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:43:08,926 DEBUG generators.py generate l.371] (10/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:43:16,578 DEBUG generators.py generate l.383] (10/10) Post-process Facts
[2024-04-22 09:43:16,580 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:43:16,775 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/03. Facts/questions--10Q_170C_72F_2M_20A_20HE_0AE_2024-04-22_09h43,16.json
[2024-04-22 09:44:56,236 DEBUG main_facts_evals.py <module> l.21] MAIN STARTS
[2024-04-22 09:44:56,248 INFO generators.py generate l.488] (1/10) *** FactGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:44:56,249 INFO generators.py gen_for_qa l.600] (1/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:44:56,249 DEBUG generators.py generate l.362] (1/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:44:56,250 DEBUG generators.py generate l.371] (1/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:01,160 DEBUG generators.py generate l.383] (1/10) Post-process Facts
[2024-04-22 09:45:01,160 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:45:01,161 INFO generators.py generate l.488] (2/10) *** FactGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:45:01,161 INFO generators.py gen_for_qa l.600] (2/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:01,161 DEBUG generators.py generate l.362] (2/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:01,162 DEBUG generators.py generate l.371] (2/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:09,174 DEBUG generators.py generate l.383] (2/10) Post-process Facts
[2024-04-22 09:45:09,175 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:45:09,175 INFO generators.py generate l.488] (3/10) *** FactGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:45:09,175 INFO generators.py gen_for_qa l.600] (3/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:09,176 DEBUG generators.py generate l.362] (3/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:09,176 DEBUG generators.py generate l.371] (3/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:14,916 DEBUG generators.py generate l.383] (3/10) Post-process Facts
[2024-04-22 09:45:14,918 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:45:14,918 INFO generators.py generate l.488] (4/10) *** FactGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:45:14,918 INFO generators.py gen_for_qa l.600] (4/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:14,919 DEBUG generators.py generate l.362] (4/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:14,919 DEBUG generators.py generate l.371] (4/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:19,380 DEBUG generators.py generate l.383] (4/10) Post-process Facts
[2024-04-22 09:45:19,380 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:45:19,381 INFO generators.py generate l.488] (5/10) *** FactGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:45:19,381 INFO generators.py gen_for_qa l.600] (5/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:19,382 DEBUG generators.py generate l.362] (5/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:19,383 DEBUG generators.py generate l.371] (5/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:29,011 DEBUG generators.py generate l.383] (5/10) Post-process Facts
[2024-04-22 09:45:29,011 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:45:29,012 INFO generators.py generate l.488] (6/10) *** FactGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:45:29,013 INFO generators.py gen_for_qa l.600] (6/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:29,013 DEBUG generators.py generate l.362] (6/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:29,013 DEBUG generators.py generate l.371] (6/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:43,294 DEBUG generators.py generate l.383] (6/10) Post-process Facts
[2024-04-22 09:45:43,295 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:45:43,295 INFO generators.py generate l.488] (7/10) *** FactGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:45:43,295 INFO generators.py gen_for_qa l.600] (7/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:43,296 DEBUG generators.py generate l.362] (7/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:43,296 DEBUG generators.py generate l.371] (7/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:45:54,808 DEBUG generators.py generate l.383] (7/10) Post-process Facts
[2024-04-22 09:45:54,811 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:45:54,811 INFO generators.py generate l.488] (8/10) *** FactGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:45:54,812 INFO generators.py gen_for_qa l.600] (8/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:45:54,812 DEBUG generators.py generate l.362] (8/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:45:54,812 DEBUG generators.py generate l.371] (8/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:46:04,966 DEBUG generators.py generate l.383] (8/10) Post-process Facts
[2024-04-22 09:46:04,968 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:46:04,968 INFO generators.py generate l.488] (9/10) *** FactGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:46:04,969 INFO generators.py gen_for_qa l.600] (9/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:46:04,970 DEBUG generators.py generate l.362] (9/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:46:04,971 DEBUG generators.py generate l.371] (9/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:46:15,057 DEBUG generators.py generate l.383] (9/10) Post-process Facts
[2024-04-22 09:46:15,063 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:46:15,063 INFO generators.py generate l.488] (10/10) *** FactGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:46:15,063 INFO generators.py gen_for_qa l.600] (10/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:46:15,063 DEBUG generators.py generate l.362] (10/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:46:15,064 DEBUG generators.py generate l.371] (10/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:46:21,972 DEBUG generators.py generate l.383] (10/10) Post-process Facts
[2024-04-22 09:46:21,972 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:46:22,009 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/03. Facts/questions--10Q_170C_70F_2M_20A_20HE_0AE_2024-04-22_09h46,21.json
[2024-04-22 09:46:22,029 INFO generators.py generate l.488] (1/10) *** EvalGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:46:22,029 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:46:22,029 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:46:22,030 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:46:24,486 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 09:46:24,487 ERROR generators.py generate l.493] (1/10) Exception caught - saving what has been done so far:
sequence item 0: expected str instance, int found
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 489, in generate
    self.gen_for_qa(qa=qa, start_from=start_from,  b_missing_only=b_missing_only, only_llms=only_llms)
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 634, in gen_for_qa
    ans.eval = self.llm.generate(cur_obj=Eval(), prev_obj=prev_eval,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 384, in generate
    self.prompter.post_process(qa=qa, cur_obj=result)
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 260, in post_process
    cur_obj.meta["missing"] = ', '.join(list(true_facts_not_in_answer))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: sequence item 0: expected str instance, int found
[2024-04-22 09:46:24,500 INFO expe.py save_to_json l.286] (1/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/03. Facts/Stopped_at_1_of_10_questions--10Q_170C_72F_2M_20A_20HE_0AE_2024-04-22_09h46,24.json
[2024-04-22 09:54:32,137 DEBUG main_facts_evals.py <module> l.21] MAIN STARTS
[2024-04-22 09:54:32,157 INFO generators.py generate l.488] (1/10) *** EvalGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:54:32,159 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:54:32,160 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:54:32,160 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:54:35,283 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 09:54:35,286 ERROR generators.py generate l.493] (1/10) Exception caught - saving what has been done so far:
sequence item 0: expected str instance, int found
Traceback (most recent call last):
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 489, in generate
    self.gen_for_qa(qa=qa, start_from=start_from,  b_missing_only=b_missing_only, only_llms=only_llms)
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 634, in gen_for_qa
    ans.eval = self.llm.generate(cur_obj=Eval(), prev_obj=prev_eval,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 384, in generate
    self.prompter.post_process(qa=qa, cur_obj=result)
  File "/Users/admin/anaconda3/envs/myenv/lib/python3.12/site-packages/ragtime/generators.py", line 260, in post_process
    cur_obj.meta["missing"] = ', '.join(list(true_facts_not_in_answer))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: sequence item 0: expected str instance, int found
[2024-04-22 09:54:35,327 INFO expe.py save_to_json l.286] (1/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/03. Facts/Stopped_at_1_of_10_questions--10Q_170C_72F_2M_20A_20HE_0AE_2024-04-22_09h54,35.json
[2024-04-22 09:57:43,704 DEBUG main_facts_evals.py <module> l.22] MAIN STARTS
[2024-04-22 09:57:43,720 INFO generators.py generate l.488] (1/10) *** EvalGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:57:43,721 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:43,721 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:43,721 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:44,989 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 09:57:44,990 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:57:44,990 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:44,991 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:46,123 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 09:57:46,124 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:57:46,125 INFO generators.py generate l.488] (2/10) *** EvalGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:57:46,125 DEBUG generators.py gen_for_qa l.630] (2/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:46,126 DEBUG generators.py generate l.362] (2/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:46,126 DEBUG generators.py generate l.371] (2/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:47,826 DEBUG generators.py generate l.383] (2/10) Post-process Eval
[2024-04-22 09:57:47,827 DEBUG generators.py gen_for_qa l.630] (2/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:57:47,827 DEBUG generators.py generate l.362] (2/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:47,827 DEBUG generators.py generate l.371] (2/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:48,866 DEBUG generators.py generate l.383] (2/10) Post-process Eval
[2024-04-22 09:57:48,867 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:57:48,867 INFO generators.py generate l.488] (3/10) *** EvalGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:57:48,868 DEBUG generators.py gen_for_qa l.630] (3/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:48,868 DEBUG generators.py generate l.362] (3/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:48,868 DEBUG generators.py generate l.371] (3/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:51,001 DEBUG generators.py generate l.383] (3/10) Post-process Eval
[2024-04-22 09:57:51,002 DEBUG generators.py gen_for_qa l.630] (3/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:57:51,002 DEBUG generators.py generate l.362] (3/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:51,002 DEBUG generators.py generate l.371] (3/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:52,842 DEBUG generators.py generate l.383] (3/10) Post-process Eval
[2024-04-22 09:57:52,842 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:57:52,842 INFO generators.py generate l.488] (4/10) *** EvalGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:57:52,843 DEBUG generators.py gen_for_qa l.630] (4/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:52,843 DEBUG generators.py generate l.362] (4/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:52,843 DEBUG generators.py generate l.371] (4/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:54,518 DEBUG generators.py generate l.383] (4/10) Post-process Eval
[2024-04-22 09:57:54,518 DEBUG generators.py gen_for_qa l.630] (4/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:57:54,518 DEBUG generators.py generate l.362] (4/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:54,518 DEBUG generators.py generate l.371] (4/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:55,863 DEBUG generators.py generate l.383] (4/10) Post-process Eval
[2024-04-22 09:57:55,863 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:57:55,864 INFO generators.py generate l.488] (5/10) *** EvalGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:57:55,864 DEBUG generators.py gen_for_qa l.630] (5/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:55,864 DEBUG generators.py generate l.362] (5/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:55,864 DEBUG generators.py generate l.371] (5/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:57,673 DEBUG generators.py generate l.383] (5/10) Post-process Eval
[2024-04-22 09:57:57,674 DEBUG generators.py gen_for_qa l.630] (5/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:57:57,675 DEBUG generators.py generate l.362] (5/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:57,675 DEBUG generators.py generate l.371] (5/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:57:58,946 DEBUG generators.py generate l.383] (5/10) Post-process Eval
[2024-04-22 09:57:58,946 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:57:58,947 INFO generators.py generate l.488] (6/10) *** EvalGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:57:58,947 DEBUG generators.py gen_for_qa l.630] (6/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:57:58,947 DEBUG generators.py generate l.362] (6/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:57:58,947 DEBUG generators.py generate l.371] (6/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:01,578 DEBUG generators.py generate l.383] (6/10) Post-process Eval
[2024-04-22 09:58:01,580 DEBUG generators.py gen_for_qa l.630] (6/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:58:01,581 DEBUG generators.py generate l.362] (6/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:01,582 DEBUG generators.py generate l.371] (6/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:03,908 DEBUG generators.py generate l.383] (6/10) Post-process Eval
[2024-04-22 09:58:03,908 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 09:58:03,909 INFO generators.py generate l.488] (7/10) *** EvalGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:58:03,909 DEBUG generators.py gen_for_qa l.630] (7/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:58:03,909 DEBUG generators.py generate l.362] (7/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:03,910 DEBUG generators.py generate l.371] (7/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:06,464 DEBUG generators.py generate l.383] (7/10) Post-process Eval
[2024-04-22 09:58:06,464 DEBUG generators.py gen_for_qa l.630] (7/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:58:06,465 DEBUG generators.py generate l.362] (7/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:06,465 DEBUG generators.py generate l.371] (7/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:08,810 DEBUG generators.py generate l.383] (7/10) Post-process Eval
[2024-04-22 09:58:08,811 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 09:58:08,811 INFO generators.py generate l.488] (8/10) *** EvalGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:58:08,811 DEBUG generators.py gen_for_qa l.630] (8/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:58:08,811 DEBUG generators.py generate l.362] (8/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:08,812 DEBUG generators.py generate l.371] (8/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:10,948 DEBUG generators.py generate l.383] (8/10) Post-process Eval
[2024-04-22 09:58:10,949 DEBUG generators.py gen_for_qa l.630] (8/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:58:10,949 DEBUG generators.py generate l.362] (8/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:10,949 DEBUG generators.py generate l.371] (8/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:12,648 DEBUG generators.py generate l.383] (8/10) Post-process Eval
[2024-04-22 09:58:12,649 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 09:58:12,649 INFO generators.py generate l.488] (9/10) *** EvalGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:58:12,649 DEBUG generators.py gen_for_qa l.630] (9/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:58:12,649 DEBUG generators.py generate l.362] (9/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:12,650 DEBUG generators.py generate l.371] (9/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:14,301 DEBUG generators.py generate l.383] (9/10) Post-process Eval
[2024-04-22 09:58:14,314 DEBUG generators.py gen_for_qa l.630] (9/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:58:14,315 DEBUG generators.py generate l.362] (9/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:14,320 DEBUG generators.py generate l.371] (9/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:16,003 DEBUG generators.py generate l.383] (9/10) Post-process Eval
[2024-04-22 09:58:16,003 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 09:58:16,004 INFO generators.py generate l.488] (10/10) *** EvalGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:58:16,004 DEBUG generators.py gen_for_qa l.630] (10/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 09:58:16,005 DEBUG generators.py generate l.362] (10/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:16,005 DEBUG generators.py generate l.371] (10/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:18,247 DEBUG generators.py generate l.383] (10/10) Post-process Eval
[2024-04-22 09:58:18,248 DEBUG generators.py gen_for_qa l.630] (10/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 09:58:18,249 DEBUG generators.py generate l.362] (10/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:58:18,249 DEBUG generators.py generate l.371] (10/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:58:19,783 DEBUG generators.py generate l.383] (10/10) Post-process Eval
[2024-04-22 09:58:19,784 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 09:58:19,802 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/04. Evals/questions--10Q_170C_72F_2M_20A_20HE_20AE_2024-04-22_09h58,19.json
[2024-04-22 09:59:31,898 DEBUG main_facts_evals.py <module> l.22] MAIN STARTS
[2024-04-22 09:59:31,910 INFO generators.py generate l.488] (1/10) *** FactGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:59:31,910 INFO generators.py gen_for_qa l.600] (1/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:59:31,911 DEBUG generators.py generate l.362] (1/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:59:31,911 DEBUG generators.py generate l.371] (1/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:59:36,727 DEBUG generators.py generate l.383] (1/10) Post-process Facts
[2024-04-22 09:59:36,728 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 09:59:36,728 INFO generators.py generate l.488] (2/10) *** FactGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:59:36,728 INFO generators.py gen_for_qa l.600] (2/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:59:36,728 DEBUG generators.py generate l.362] (2/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:59:36,729 DEBUG generators.py generate l.371] (2/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:59:43,355 DEBUG generators.py generate l.383] (2/10) Post-process Facts
[2024-04-22 09:59:43,358 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 09:59:43,358 INFO generators.py generate l.488] (3/10) *** FactGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:59:43,359 INFO generators.py gen_for_qa l.600] (3/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:59:43,359 DEBUG generators.py generate l.362] (3/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:59:43,359 DEBUG generators.py generate l.371] (3/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:59:48,782 DEBUG generators.py generate l.383] (3/10) Post-process Facts
[2024-04-22 09:59:48,783 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 09:59:48,783 INFO generators.py generate l.488] (4/10) *** FactGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:59:48,785 INFO generators.py gen_for_qa l.600] (4/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:59:48,786 DEBUG generators.py generate l.362] (4/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:59:48,787 DEBUG generators.py generate l.371] (4/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 09:59:53,160 DEBUG generators.py generate l.383] (4/10) Post-process Facts
[2024-04-22 09:59:53,167 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 09:59:53,167 INFO generators.py generate l.488] (5/10) *** FactGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 09:59:53,167 INFO generators.py gen_for_qa l.600] (5/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 09:59:53,168 DEBUG generators.py generate l.362] (5/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 09:59:53,169 DEBUG generators.py generate l.371] (5/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:03,252 DEBUG generators.py generate l.383] (5/10) Post-process Facts
[2024-04-22 10:00:03,253 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 10:00:03,254 INFO generators.py generate l.488] (6/10) *** FactGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 10:00:03,254 INFO generators.py gen_for_qa l.600] (6/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 10:00:03,255 DEBUG generators.py generate l.362] (6/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:03,255 DEBUG generators.py generate l.371] (6/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:14,637 DEBUG generators.py generate l.383] (6/10) Post-process Facts
[2024-04-22 10:00:14,658 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 10:00:14,659 INFO generators.py generate l.488] (7/10) *** FactGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 10:00:14,659 INFO generators.py gen_for_qa l.600] (7/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 10:00:14,659 DEBUG generators.py generate l.362] (7/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:14,659 DEBUG generators.py generate l.371] (7/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:28,165 DEBUG generators.py generate l.383] (7/10) Post-process Facts
[2024-04-22 10:00:28,167 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 10:00:28,168 INFO generators.py generate l.488] (8/10) *** FactGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 10:00:28,168 INFO generators.py gen_for_qa l.600] (8/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 10:00:28,168 DEBUG generators.py generate l.362] (8/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:28,169 DEBUG generators.py generate l.371] (8/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:39,097 DEBUG generators.py generate l.383] (8/10) Post-process Facts
[2024-04-22 10:00:39,100 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 10:00:39,101 INFO generators.py generate l.488] (9/10) *** FactGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 10:00:39,101 INFO generators.py gen_for_qa l.600] (9/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 10:00:39,102 DEBUG generators.py generate l.362] (9/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:39,102 DEBUG generators.py generate l.371] (9/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:47,316 DEBUG generators.py generate l.383] (9/10) Post-process Facts
[2024-04-22 10:00:47,319 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 10:00:47,320 INFO generators.py generate l.488] (10/10) *** FactGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 10:00:47,320 INFO generators.py gen_for_qa l.600] (10/10) Generate Facts since it has a human validated answer (eval.human == 1.0) associated with answer from model gpt-4-0613
[2024-04-22 10:00:47,320 DEBUG generators.py generate l.362] (10/10) Either no Facts / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:47,321 DEBUG generators.py generate l.371] (10/10) Either no Facts / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:56,691 DEBUG generators.py generate l.383] (10/10) Post-process Facts
[2024-04-22 10:00:56,692 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 10:00:56,845 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/03. Facts/questions--10Q_170C_69F_2M_20A_20HE_0AE_2024-04-22_10h00,56.json
[2024-04-22 10:00:56,905 INFO generators.py generate l.488] (1/10) *** EvalGenerator for question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 10:00:56,905 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:00:56,907 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:56,909 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:00:58,195 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 10:00:58,197 DEBUG generators.py gen_for_qa l.630] (1/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:00:58,197 DEBUG generators.py generate l.362] (1/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:00:58,198 DEBUG generators.py generate l.371] (1/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:00,363 DEBUG generators.py generate l.383] (1/10) Post-process Eval
[2024-04-22 10:01:00,364 INFO generators.py generate l.490] (1/10) End question "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
[2024-04-22 10:01:00,364 INFO generators.py generate l.488] (2/10) *** EvalGenerator for question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 10:01:00,364 DEBUG generators.py gen_for_qa l.630] (2/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:00,364 DEBUG generators.py generate l.362] (2/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:00,365 DEBUG generators.py generate l.371] (2/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:02,309 DEBUG generators.py generate l.383] (2/10) Post-process Eval
[2024-04-22 10:01:02,309 DEBUG generators.py gen_for_qa l.630] (2/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:02,309 DEBUG generators.py generate l.362] (2/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:02,310 DEBUG generators.py generate l.371] (2/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:03,141 DEBUG generators.py generate l.383] (2/10) Post-process Eval
[2024-04-22 10:01:03,141 INFO generators.py generate l.490] (2/10) End question "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
[2024-04-22 10:01:03,141 INFO generators.py generate l.488] (3/10) *** EvalGenerator for question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 10:01:03,142 DEBUG generators.py gen_for_qa l.630] (3/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:03,142 DEBUG generators.py generate l.362] (3/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:03,142 DEBUG generators.py generate l.371] (3/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:05,212 DEBUG generators.py generate l.383] (3/10) Post-process Eval
[2024-04-22 10:01:05,213 DEBUG generators.py gen_for_qa l.630] (3/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:05,213 DEBUG generators.py generate l.362] (3/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:05,213 DEBUG generators.py generate l.371] (3/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:06,697 DEBUG generators.py generate l.383] (3/10) Post-process Eval
[2024-04-22 10:01:06,697 INFO generators.py generate l.490] (3/10) End question "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
[2024-04-22 10:01:06,697 INFO generators.py generate l.488] (4/10) *** EvalGenerator for question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 10:01:06,697 DEBUG generators.py gen_for_qa l.630] (4/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:06,698 DEBUG generators.py generate l.362] (4/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:06,698 DEBUG generators.py generate l.371] (4/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:08,331 DEBUG generators.py generate l.383] (4/10) Post-process Eval
[2024-04-22 10:01:08,332 DEBUG generators.py gen_for_qa l.630] (4/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:08,332 DEBUG generators.py generate l.362] (4/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:08,333 DEBUG generators.py generate l.371] (4/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:09,547 DEBUG generators.py generate l.383] (4/10) Post-process Eval
[2024-04-22 10:01:09,547 INFO generators.py generate l.490] (4/10) End question "How many patents does Kenneth L. Graham have related to natural language processing?"
[2024-04-22 10:01:09,547 INFO generators.py generate l.488] (5/10) *** EvalGenerator for question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 10:01:09,547 DEBUG generators.py gen_for_qa l.630] (5/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:09,548 DEBUG generators.py generate l.362] (5/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:09,548 DEBUG generators.py generate l.371] (5/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:11,186 DEBUG generators.py generate l.383] (5/10) Post-process Eval
[2024-04-22 10:01:11,195 DEBUG generators.py gen_for_qa l.630] (5/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:11,196 DEBUG generators.py generate l.362] (5/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:11,196 DEBUG generators.py generate l.371] (5/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:12,352 DEBUG generators.py generate l.383] (5/10) Post-process Eval
[2024-04-22 10:01:12,352 INFO generators.py generate l.490] (5/10) End question "In what scenarios is global attention beneficial in transformer models?"
[2024-04-22 10:01:12,353 INFO generators.py generate l.488] (6/10) *** EvalGenerator for question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 10:01:12,353 DEBUG generators.py gen_for_qa l.630] (6/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:12,353 DEBUG generators.py generate l.362] (6/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:12,353 DEBUG generators.py generate l.371] (6/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:14,675 DEBUG generators.py generate l.383] (6/10) Post-process Eval
[2024-04-22 10:01:14,675 DEBUG generators.py gen_for_qa l.630] (6/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:14,676 DEBUG generators.py generate l.362] (6/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:14,676 DEBUG generators.py generate l.371] (6/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:16,360 DEBUG generators.py generate l.383] (6/10) Post-process Eval
[2024-04-22 10:01:16,360 INFO generators.py generate l.490] (6/10) End question "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
[2024-04-22 10:01:16,361 INFO generators.py generate l.488] (7/10) *** EvalGenerator for question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 10:01:16,361 DEBUG generators.py gen_for_qa l.630] (7/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:16,361 DEBUG generators.py generate l.362] (7/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:16,362 DEBUG generators.py generate l.371] (7/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:18,777 DEBUG generators.py generate l.383] (7/10) Post-process Eval
[2024-04-22 10:01:18,778 DEBUG generators.py gen_for_qa l.630] (7/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:18,778 DEBUG generators.py generate l.362] (7/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:18,778 DEBUG generators.py generate l.371] (7/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:21,358 DEBUG generators.py generate l.383] (7/10) Post-process Eval
[2024-04-22 10:01:21,358 INFO generators.py generate l.490] (7/10) End question "Describe the computation process for each hidden unit in layer normalization."
[2024-04-22 10:01:21,358 INFO generators.py generate l.488] (8/10) *** EvalGenerator for question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 10:01:21,359 DEBUG generators.py gen_for_qa l.630] (8/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:21,360 DEBUG generators.py generate l.362] (8/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:21,360 DEBUG generators.py generate l.371] (8/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:23,429 DEBUG generators.py generate l.383] (8/10) Post-process Eval
[2024-04-22 10:01:23,430 DEBUG generators.py gen_for_qa l.630] (8/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:23,430 DEBUG generators.py generate l.362] (8/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:23,430 DEBUG generators.py generate l.371] (8/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:25,184 DEBUG generators.py generate l.383] (8/10) Post-process Eval
[2024-04-22 10:01:25,186 INFO generators.py generate l.490] (8/10) End question "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
[2024-04-22 10:01:25,186 INFO generators.py generate l.488] (9/10) *** EvalGenerator for question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 10:01:25,186 DEBUG generators.py gen_for_qa l.630] (9/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:25,187 DEBUG generators.py generate l.362] (9/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:25,187 DEBUG generators.py generate l.371] (9/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:26,896 DEBUG generators.py generate l.383] (9/10) Post-process Eval
[2024-04-22 10:01:26,896 DEBUG generators.py gen_for_qa l.630] (9/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:26,897 DEBUG generators.py generate l.362] (9/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:26,897 DEBUG generators.py generate l.371] (9/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:27,982 DEBUG generators.py generate l.383] (9/10) Post-process Eval
[2024-04-22 10:01:27,984 INFO generators.py generate l.490] (9/10) End question "What is the role of the "embedding" and "label" variables in the compute_loss function?"
[2024-04-22 10:01:27,984 INFO generators.py generate l.488] (10/10) *** EvalGenerator for question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 10:01:27,985 DEBUG generators.py gen_for_qa l.630] (10/10) Generate Eval for answer generated with "gpt-4"
[2024-04-22 10:01:27,985 DEBUG generators.py generate l.362] (10/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:27,985 DEBUG generators.py generate l.371] (10/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:29,739 DEBUG generators.py generate l.383] (10/10) Post-process Eval
[2024-04-22 10:01:29,739 DEBUG generators.py gen_for_qa l.630] (10/10) Generate Eval for answer generated with "gpt-3.5-turbo"
[2024-04-22 10:01:29,739 DEBUG generators.py generate l.362] (10/10) Either no Eval / LLMAnswer / Prompt exists yet, or you asked to regenerate Prompt ==> generate prompt
[2024-04-22 10:01:29,739 DEBUG generators.py generate l.371] (10/10) Either no Eval / LLMAnswer exists yet, or you asked to regenerate it ==> generate LLMAnswer
[2024-04-22 10:01:31,631 DEBUG generators.py generate l.383] (10/10) Post-process Eval
[2024-04-22 10:01:31,632 INFO generators.py generate l.490] (10/10) End question "How does the attention mechanism address the issue of long-distance associations in language processing?"
[2024-04-22 10:01:31,648 INFO expe.py save_to_json l.286] (10/10) Expe saved as JSON to /Users/admin/Downloads/Pdf_QA_tester/expe/04. Evals/questions--10Q_170C_72F_2M_20A_20HE_20AE_2024-04-22_10h01,31.json
[2024-04-22 10:01:31,773 INFO expe.py save_to_html l.299] (10/10) Expe saved as HTML to /Users/admin/Downloads/Pdf_QA_tester/expe/04. Evals/questions--10Q_170C_72F_2M_20A_20HE_20AE_2024-04-22_10h01,31.html
[2024-04-22 10:01:32,042 INFO expe.py save_to_spreadsheet l.379] (10/10) Expe saved as Spreadsheet to /Users/admin/Downloads/Pdf_QA_tester/expe/04. Evals/questions--10Q_170C_72F_2M_20A_20HE_20AE_2024-04-22_10h01,31.xlsx
[2024-04-22 10:01:32,043 DEBUG main_facts_evals.py <module> l.37] MAIN ENDS
[2024-04-22 10:01:32,043 DEBUG main_facts_evals.py <module> l.39] Pdf_QA_tester STARTS
