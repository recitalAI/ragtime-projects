<!--
This Jinja templates needs the "expe" variable to generate the HTML.
Moreover, it uses several boolean variables to control the rendering:
- "show_answers": True to show the answers for each model
- "show_chunks": True to show the Chunks in a table (images on the left column, text on the right)
- "show_facts": True to show the Facts
- "show_evals": True to show the Evaluations
-->
<html>
<head>
  <style>
    h3 {
      background-color: LightGray;
      padding: 0px;
    }

    table {
      border-collapse: collapse;
      background-color: #FFFFFF;
    }

    tr {
      border-bottom: 1px solid #ccc;
    }

    img {
      width: 100%;
      width: 800;
      display: block;
    }

    .my_img {
      width: 100px;
      vertical-align: middle;
    }

    * {
      font-family: Arial
    }

    .collapsible {
      background-color: #777;
      border-bottom: 1px solid #ccc;
      color: white;
      cursor: pointer;
      padding: 3px;
      width: 100%;
      text-align: left;
      outline: none;
      font-size: 15px;
    }

    .active,
    .collapsible:hover {
      background-color: #555;
    }

    .content {
      padding: 0px;
      display: none;
      overflow: hidden;
      /*background-color: #f1f1f1;*/
    }
    .facts {
      text-decoration: underline;
      color: #0000EE;
      font-weight: bold;
    }
    .hallus {
      text-decoration: underline;
      color: #FF1111;
      font-weight: bold;
    } 
    .model_name {
      background-color: #ffffa0;
      font-weight: bold;
    }
  </style>
</head>

<body>
  <span style="font-size: 20px;">30Q_600C_0F_2M_60A_0HE_0AE_2024-04-24_14h17,45</span>
  <!-- For each QA -->
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>1. What is the significance of the value of replacement of a vehicle in the context of estimating damages?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the significance of the value of replacement of a vehicle in the context of estimating damages.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          In the context of estimating damages, the value of replacement of a vehicle is significant for several reasons:

1. **Determining Total Loss**: The value of replacement helps in determining whether a vehicle is considered a total loss. If the cost of repairing the damages exceeds the value of replacing the vehicle, it may be deemed a total loss by insurance companies.

2. **Insurance Claims**: When filing an insurance claim for damages to a vehicle, the value of replacement is used to assess the amount of compensation the policyholder is entitled to receive. It helps in determining the fair market value of the vehicle before the damage occurred.

3. **Negotiations**: In cases where damages are being settled outside of insurance claims, such as in legal disputes or negotiations with a third party at fault, the value of replacement serves as a benchmark for determining the amount of compensation or damages owed.

4. **Financial Impact**: Understanding the value of replacement provides insight into the financial impact of the damages. It helps in assessing the cost-effectiveness of repairing the vehicle versus replacing it.

5. **Resale Value**: The value of replacement also factors into the resale value of the vehicle post-repair. A lower replacement value may affect the resale value of the vehicle in the future.

Overall, the value of replacement of a vehicle plays a crucial role in the process of estimating damages, determining compensation, and making decisions regarding repairs or replacements after an incident.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It performs better
on GLUE that RoBERTa, but not SQuAD, where it is slightly worse.
As number of clusters increases, the approximation becomes more
accurate.Itconvergesuptotwiceasfastasthestandardtransformer,for
longsequencelengthsand,forshortsequencelengths,clusteredattention
isnotfaster than the standard transformer.
5.2.6 Compressed Key-Value Memory
5.2.6.1 Luna: Linear Uniﬁed Nested Attention
Luna [177], which stands for Linear Uniﬁed Nested Attention, replaces
theattentionweightcomputationineachattentionheadwithtwonested
linear attention computations using an extra, learnable, input sequence
that learns to encode contextual information: P∈Rl×d, wherelis the
length of the sequence.
As discussed earlier, the output of an attention head between a query
sequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written
as
Y=Attn(X, C) = softmax(
XWq(CWk)T
√
dk/h(
CV,∈Rn×d(5.69)</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig. 2.3, an attention mechanism can be considered as
a memory with keys and values and a layer which, when someone queries
it, generates an output from value whose keys map the input [17].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformer Modiﬁcations ■141
each of the Cclusters. And, for each of the top-k keys for a cluster,
compute the attention with the queries in that cluster:
At
il={ ˆmjexp(
qiklT)
∑L
r=1Tjrexp(
qikrT),ifTjl= 1
Ac
jl, otherwise(5.68)
where ˆmj=∑L
i=1TijAc
ijandT∈{0,1}C×L: ifTij= 1, then kiis one of
the top-k keys for in cluster j.
Then compute the context vectors (weighted average of the values)
of the clustered attention and use it as the value matrix: ˆV=AtV,∈
RL×d v. This makes the complexity of the clustered attention calculation
toO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence
length.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The bidirectional
attention of the transformer encoder forces the [MASK]prediction task to
usethecontextprovidedbytheothernon-maskedtokensinthesequence.
BERT is pre-trained with a 15% mask-out rate. This simple MLM task
has a downside where a mismatch is created between pre-training and
ﬁne-tuning tasks as the special [MASK]token does not appear during
ﬁne-tuning. To overcome this issue, every token in the 15% masked-out
tokens is subjected to the following heuristic:
•With a probability of 80%, the token is replaced with the special
[MASK]token.
•With a probability of 10%, the token is replaced with a random
token.
•With a probability of 10%, the token is left unchanged.
The occasional insertion of random tokens (i.e., noise) pushes BERT to
be less biased towards the masked token, especially when the masked
token is left unchanged, in its bidirectional context attention.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig. 2.3, an attention mechanism can be considered as
a memory with keys and values and a layer which, when someone queries
it, generates an output from value whose keys map the input [17].
To formalize, let us consider the memory unit consisting of nkey-
value pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT is pre-trained with a 15% mask-out rate. This simple MLM task
has a downside where a mismatch is created between pre-training and
ﬁne-tuning tasks as the special [MASK]token does not appear during
ﬁne-tuning. To overcome this issue, every token in the 15% masked-out
tokens is subjected to the following heuristic:
•With a probability of 80%, the token is replaced with the special
[MASK]token.
•With a probability of 10%, the token is replaced with a random
token.
•With a probability of 10%, the token is left unchanged.
The occasional insertion of random tokens (i.e., noise) pushes BERT to
be less biased towards the masked token, especially when the masked
token is left unchanged, in its bidirectional context attention. The MLM
task uses cross-entropy loss only over the masked tokens and ignores the
prediction of all non-masked ones.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />5.1 Clustered attention 140
5.2.6 Compressed Key-Value Memory 141
5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141
5.2.7 Low-Rank Approximations 143
5.2.7.1 Linformer 143
5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145
5.3.1 ELECTRA 145
5.3.1.1 Replaced token detection 145
5.3.2 T5 146
5.4 TRANSFORMER SUBMODULE CHANGES 146
5.4.1 Switch Transformer 146
5.5 CASE STUDY: SENTIMENT ANAL YSIS 148
5.5.1 Goal 148
5.5.2 Data, Tools, and Libraries 148
5.5.3 Experiments, Results, and Analysis 150
5.5.3.1 Visualizing attention head weights 150
5.5.3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The loss function Lprovides a metric of how close the explanation
model approximates the real model fin the locality of instance x.
•SHapley Additive exPlanations (SHAP): Shapley values
have foundations in game theory, particularly cooperative game
theory. One can view each feature as a player in a game, and the
goal is to distribute payouts fairly among players who form a coali-
tion based on their contribution to the total payout. The marginal
contribution of each feature averaged across the set of all possible
coalitions of features gives the Shapley values.
Let us represent the entire deep learning model through function
f, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj
for thej-th feature is given by
φj(f) =|S|!(d−|S|−1)!
d![f(S∪{xj})−f(S)](7.12)</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />By an-
alyzing the impact, one can identify essential representations that
signiﬁcantly contribute to the models’ decisions. The erasure can
be performed on various levels of representation, such as the input
word-vector dimensions, input words or group of words, and inter-
mediatehiddenunits.Thestraightforwardtechniqueistocompute
the diﬀerence in log-likelihood on the labels when representations
are erased. A more sophisticated method uses the reinforcement
learning model to ﬁnd the minimal set of words that must be
erased to change the model’s decision.
•Meaningful Perturbation : Fong and Vedaldi propose a meta-
predictor using the perturbation technique to give local explana-
tions based on the sensitive areas in the input that predicted the
output [89]. The authors propose three classes of perturbations to
generate visual explanations for image classiﬁcation: (i) constant,
replacing a region in the image with a constant value, (ii) noise,
adding small noise to the region, and (iii) blur, blurring the region.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Representation Erasure: Li et al. propose a generic technique
for an explanation by examining the eﬀect of erasing the input rep-
resentation to see how such changes aﬀect the output [157]. By an-
alyzing the impact, one can identify essential representations that
signiﬁcantly contribute to the models’ decisions. The erasure can
be performed on various levels of representation, such as the input
word-vector dimensions, input words or group of words, and inter-
mediatehiddenunits.Thestraightforwardtechniqueistocompute
the diﬀerence in log-likelihood on the labels when representations
are erased. A more sophisticated method uses the reinforcement
learning model to ﬁnd the minimal set of words that must be
erased to change the model’s decision.
•Meaningful Perturbation : Fong and Vedaldi propose a meta-
predictor using the perturbation technique to give local explana-
tions based on the sensitive areas in the input that predicted the
output [89].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />7.3.1.2 Perturbation-based
Altering the input and computing the feature relevance by comparing
the diﬀerence in the output between the original and altered input is the
core of all perturbation-based methods.
•Representation Erasure: Li et al. propose a generic technique
for an explanation by examining the eﬀect of erasing the input rep-
resentation to see how such changes aﬀect the output [157]. By an-
alyzing the impact, one can identify essential representations that
signiﬁcantly contribute to the models’ decisions. The erasure can
be performed on various levels of representation, such as the input
word-vector dimensions, input words or group of words, and inter-
mediatehiddenunits.Thestraightforwardtechniqueistocompute
the diﬀerence in log-likelihood on the labels when representations
are erased. A more sophisticated method uses the reinforcement
learning model to ﬁnd the minimal set of words that must be
erased to change the model’s decision.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />194■Transformers for Machine Learning: A Deep Dive
whereαissmoothlydistributedintherange [0,1]andisassociated
with the path from x′tox. The above equation can be approx-
imated using Riemann summation using appropriate number of
steps as described in the paper.
7.3.1.2 Perturbation-based
Altering the input and computing the feature relevance by comparing
the diﬀerence in the output between the original and altered input is the
core of all perturbation-based methods.
•Representation Erasure: Li et al. propose a generic technique
for an explanation by examining the eﬀect of erasing the input rep-
resentation to see how such changes aﬀect the output [157]. By an-
alyzing the impact, one can identify essential representations that
signiﬁcantly contribute to the models’ decisions. The erasure can
be performed on various levels of representation, such as the input
word-vector dimensions, input words or group of words, and inter-
mediatehiddenunits.Thestraightforwardtechniqueistocompute
the diﬀerence in log-likelihood on the labels when representations
are erased.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />194■Transformers for Machine Learning: A Deep Dive
whereαissmoothlydistributedintherange [0,1]andisassociated
with the path from x′tox. The above equation can be approx-
imated using Riemann summation using appropriate number of
steps as described in the paper.
7.3.1.2 Perturbation-based
Altering the input and computing the feature relevance by comparing
the diﬀerence in the output between the original and altered input is the
core of all perturbation-based methods.
•Representation Erasure: Li et al. propose a generic technique
for an explanation by examining the eﬀect of erasing the input rep-
resentation to see how such changes aﬀect the output [157]. By an-
alyzing the impact, one can identify essential representations that
signiﬁcantly contribute to the models’ decisions.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>2. Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The specifics of when an insured individual can choose their own defense lawyer in legal actions related to penal defense and recourse guarantee can vary based on the terms of the insurance policy and local laws. However, generally, the insured may have the freedom to choose their defense lawyer in the following circumstances:

1. If the insurance policy explicitly allows for it: Some insurance policies may have a clause that allows the insured to choose their own lawyer. This is often the case in liability insurance policies.

2. If there is a conflict of interest: If the insurance company's lawyer has a conflict of interest, the insured may be able to choose their own lawyer. This could occur if the same lawyer is representing both the insured and the insurer, and their interests diverge.

3. If the legal action is a criminal case: In some jurisdictions, the insured has the right to choose their own lawyer in criminal cases, even if their insurance policy would otherwise require them to use a lawyer provided by the insurance company.

4. If the insured has a recourse guarantee: A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss. If the insured has this guarantee, they may be able to choose their own lawyer to pursue this recourse action.

It's important to note that even when the insured can choose their own lawyer, the insurance company may still have to approve the choice, and the insurer will typically only cover legal fees up to a certain amount. Always refer to the specific insurance policy and consult with a legal professional for advice.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The insured typically has the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee when they have a provision in their insurance policy that allows for such choice. This provision is often found in policies that provide legal defense coverage, where the insured can select their own legal representation.

However, it's important to review the specific terms and conditions of the insurance policy to determine the extent of this freedom. Some policies may have restrictions on the selection of defense lawyers, such as a list of pre-approved attorneys or a requirement for the insurance company to approve the chosen lawyer.

In cases where the insured has the freedom to choose their defense lawyer, it is advisable to select a lawyer with experience in the relevant legal area to ensure effective representation and protection of their rights.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Diﬀerent attention heads can
use diﬀerent values of d, which mitigates the issue mentioned above. Fig.
5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.
Global attention The global attention pattern chooses lets some to-
kens attend to any other token in the sequence. In such cases, all tokens
in the sequence attend to that token. This corresponds to choosing spe-
ciﬁc rows of an attention weight matrix and their transposed columns.
Longformerdecideswhichtokensareallowedtohaveglobalattention
basedonthetrainingtask.Forinstance,inquestionandanswertasks,all
tokens in the question have global attention. The number of tokens with
global attention is generally independent of sequence length, so global
attention is also linear in sequence length. The Longformer combines
this global attention with the sliding window attention.
Weights for the global and short-ranged attention patterns are com-
puted separately by giving the global and sliding window patterns their</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />178■Transformers for Machine Learning: A Deep Dive
ronment can take. The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Note that even though
the
overall context width has grown, this attention pattern also prevents
a token from attending to its immediate neighbor, or to any token that
is a distance d−1away from the center of the window or to any position
it attends to that are inside the window. Diﬀerent attention heads can
use diﬀerent values of d, which mitigates the issue mentioned above. Fig.
5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.
Global attention The global attention pattern chooses lets some to-
kens attend to any other token in the sequence. In such cases, all tokens
in the sequence attend to that token. This corresponds to choosing spe-
ciﬁc rows of an attention weight matrix and their transposed columns.
Longformerdecideswhichtokensareallowedtohaveglobalattention
basedonthetrainingtask.Forinstance,inquestionandanswertasks,all
tokens in the question have global attention.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Thus, the left context of w5would be
(w1,w3)and the right context would be (w7, w 9). Note that even though
the
overall context width has grown, this attention pattern also prevents
a token from attending to its immediate neighbor, or to any token that
is a distance d−1away from the center of the window or to any position
it attends to that are inside the window. Diﬀerent attention heads can
use diﬀerent values of d, which mitigates the issue mentioned above. Fig.
5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.
Global attention The global attention pattern chooses lets some to-
kens attend to any other token in the sequence. In such cases, all tokens
in the sequence attend to that token. This corresponds to choosing spe-
ciﬁc rows of an attention weight matrix and their transposed columns.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The encoder structure is shown on the left. The multi-head at-
tention mechanism is shown on the right. As described in section 6.6.1.1,
Laplacian embeddings are only applied to the input layer, l= 0.
6.7 REINFORCEMENT LEARNING
Figure 6.5 Depiction of the interaction between the agent and environ-
ment in reinforcement learning. At time t, the state observed by the
agent isst. The agent then selects an action at. In the next time step,
t+ 1, the environment updates it state to st+1and issues a reward to
the agent,rt+1. After this, the cycle can repeat.
ReinforcementLearning(RL)isamachinelearningmethodthatuses
rewards to train an autonomous agent that can choose its next action
so that it gets the best cumulative reward, given its current state.
To understand the process of reinforcement learning, we can start
withaﬁniteMarkovdecisionprocess[239],whichisaformalwayoflook-
ing at a sequence of decisions that an agent which interacts with an envi-</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■177
Figure 6.4 Diagram of the Graph Transformer encoder, with edge at-
tributes. The encoder structure is shown on the left. The multi-head at-
tention mechanism is shown on the right. As described in section 6.6.1.1,
Laplacian embeddings are only applied to the input layer, l= 0.
6.7 REINFORCEMENT LEARNING
Figure 6.5 Depiction of the interaction between the agent and environ-
ment in reinforcement learning. At time t, the state observed by the
agent isst. The agent then selects an action at. In the next time step,
t+ 1, the environment updates it state to st+1and issues a reward to
the agent,rt+1. After this, the cycle can repeat.
ReinforcementLearning(RL)isamachinelearningmethodthatuses
rewards to train an autonomous agent that can choose its next action
so that it gets the best cumulative reward, given its current state.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Hewitt and Liang, in their work, devise a control task strategy as
a measure to address the probing confounder problem [110]. Probing
confounder problem can be deﬁned as—given the main neural architec-
ture that needs to be explained, the probing classiﬁer such as MLP or
logistic regression and the supervised auxiliary task, how do we assign
the credit for performance such as test accuracy to either of the three?
They devise control tasks that can be easily learned by a probing di-
agnostic classiﬁer but not encoded in the representations (actual neural
model or layers). Evaluating the performance (testing accuracy) diﬀer-
ence between the control tasks and auxiliary tasks with diﬀerent probing
classiﬁer choices as a selectivity measure can easily assign the credit of
learning to either the representation or the probing classiﬁer. The work
also answers questions such as “how does the probe design aﬀect probing
task performance?” and “can the probes pick spurious signals?”.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[75]W. B. Dolan and C. Brockett ,Automatically constructing a
corpus of sentential paraphrases , in IJCNLP, 2005.
[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,
Boosting adversarial attacks with momentum ,inProceedingsofthe
IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 9185–9193.
[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-
pretability of deep neural networks with semantic information , in
Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017, pp. 4306–4314.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Evaluating the performance (testing accuracy) diﬀer-
ence between the control tasks and auxiliary tasks with diﬀerent probing
classiﬁer choices as a selectivity measure can easily assign the credit of
learning to either the representation or the probing classiﬁer. The work
also answers questions such as “how does the probe design aﬀect probing
task performance?” and “can the probes pick spurious signals?”.
Attention Probing Probing either by adding an attention layer on top
of an existing neural architecture or using existing attention weights
from a layer of deep learning, mapping it to the inputs as “attention
maps” to explore the relationship between the two is soon developing as
an eﬀective explanation technique.
Rocktäschel et al. proposed a neural word-by-word attention mech-
anism in a sequence-to-sequence network for reasoning over entailments
of pairs of words and phrases [211].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-
plaining and harnessing adversarial examples , arXiv preprint
arXiv:1412.6572, (2014).
[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>3. In what situations does MAAF Assistance intervene for psychological trauma?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          MAAF Assistance provides support in cases of psychological trauma by organizing and covering the costs of a telephone consultation with a clinical psychologist. If necessary, they can also arrange for a face-to-face meeting with a psychologist, and even a follow-up meeting if required. The specific situations or conditions under which this assistance is provided are not explicitly stated in the text.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          MAAF Assistance intervenes for psychological trauma by organizing and providing support such as a telephone conversation with a clinical psychologist, face-to-face meetings if necessary, and additional face-to-face meetings if needed. They also offer assistance at home, including household help, the presence of a loved one in France or Monaco, transferring the beneficiary to a loved one's home in France or Monaco (excluding medical transport), and taking care of children or disabled children by arranging for a loved one to come to the home, transferring the children to a loved one's home, or providing childcare at home by an authorized caregiver.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Les délais d’intervention et les prestations de MAAF 
Assistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.
     MAAF Assistance n’intervient pas dans les cas sui-
vants : 
     MAAF Assistance ne peut intervenir que dans  
la limite des accords donnés par les autorités locales.
     MAAF Assistance ne peut en aucun cas se subs-
tituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.
     MAAF Assistance ne sera pas tenue d’intervenir 
dans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Lorsque MAAF Assistance organise et prend en charge 
un rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.
     Lorsque MAAF Assistance a assuré à ses frais le retour 
du bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.
     Les délais d’intervention et les prestations de MAAF 
Assistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.
     MAAF Assistance n’intervient pas dans les cas sui-
vants : 
     MAAF Assistance ne peut intervenir que dans  
la limite des accords donnés par les autorités locales.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />et Protection Juridique Automobile  “.
À défaut de règlement amiable ,nous poursui -
vons notre assistance dans le cadre judiciaire.
> En cas de procédure judiciaire
Lorsqu’aucune issue amiable n’est possible, ou 
lorsque vous faites l’objet d’une action judiciaire, 
nous vous proposons de saisir un avocat. 
Vous disposez du libre choix de votre avocat. Vous 
devez nous communiquer par écrit ses coordon -
nées. Devant les juridictions de France métropo -
litaine, si vous le souhaitez, nous pouvons, sur 
votre demande écrite, vous communiquer les 
coordonnées d’un avocat. 
Nous vous recommandons de demander notre 
accord écrit préalable avant de le saisir. En effet, 
nous refuserons de prendre en charge les frais 
et honoraires de votre conseil pour les interven -
tions qu’il aura effectuées avant votre décla -
ration de litige , sauf si vous êtes en mesure de 
justifier d’une situation d’urgence avérée.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />CONDITIONS D’INTERVENTION 
     Les dépenses engagées sans l’accord préalable de 
MAAF Assistance, resteront à votre charge, de même que les dépenses que vous auriez dû normalement engager en l’absence de l’événement donnant lieu à l’intervention.
     Lorsque MAAF Assistance organise et prend en charge 
un rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.
     Lorsque MAAF Assistance a assuré à ses frais le retour 
du bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.
     Les délais d’intervention et les prestations de MAAF 
Assistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />144–150.
[132]S. Jain and B. C. Wallace ,Attention is not explanation ,arXiv
preprint arXiv:1902.10186, (2019).
[133]W. James and F. H. Burkhardt ,The principles of psychology,
the works of William James , Transactions of the Charles S. Peirce
Society, 19 (1983).
[134]H. Jiang, B. Kim, M. Y. Guan, and M. R. Gupta ,To trust
or not to trust a classiﬁer. , in NeurIPS, 2018, pp. 5546–5557.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Lorsque MAAF Assistance a assuré à ses frais le retour 
du bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.
     Les délais d’intervention et les prestations de MAAF 
Assistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.
     MAAF Assistance n’intervient pas dans les cas sui-
vants : 
     MAAF Assistance ne peut intervenir que dans  
la limite des accords donnés par les autorités locales.
     MAAF Assistance ne peut en aucun cas se subs-
tituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />b) Soutien psychologique  
Organisation et prise en charge selon la situation d’un entretien téléphonique avec un psychologue clinicien et, si nécessaire, d’un 
entretien en vis -à-vis, voire d’un entretien en vis -à-vis complémentaire.  
 
c) Aide à domicile  
Les prestations garanties sont les suivantes :  
 aide -ménagère ,  
 présence d’un proche en France  ou dans la Principauté de Monaco,  
 transfert du bénéficiaire chez un proche en France  ou dans la Principauté de Monaco,  
(hors transport médicalisé)  
 prise en charge des enfants (de moins de 16 ans) ou des enfants  atteints d’un handicap (sans limite d’âge) :  
- déplacement d’un proche au domicile,  
- transfert des enfants au domicile d’un proche,  
- garde des enfants au domicile par un intervenant habilité,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Les délais d’intervention et les prestations de MAAF 
Assistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.
     MAAF Assistance n’intervient pas dans les cas sui-
vants : 
     MAAF Assistance ne peut intervenir que dans  
la limite des accords donnés par les autorités locales.
     MAAF Assistance ne peut en aucun cas se subs-
tituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.
     MAAF Assistance ne sera pas tenue d’intervenir 
dans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement.
     MAAF Assistance ne peut se substituer aux ser- 
vices publics, sapeurs-pompiers notamment, auxquels il doit être fait appel en cas d’incendie, explosions, etc.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Maladie : altération soudaine et imprévisible de la santé, 
consécutive ou non à une situation préexistante, n’ayant pas pour origine un accident corporel*, constatée par une auto-rité médicale compétente et qui empêche la continuation normale du voyage ou du séjour.
Exclusions
Ni les voyages à visée diagnostique et/ou thérapeutique, c'est-à-dire ayant pour objectif de consulter un praticien ou d’être hospitalisé, ni les retours pour greffe d’organe, ne peuvent être considérés comme des événements don-nant droit à une assistance au titre de la maladie si celle-ci n’est pas justifiée par une altération soudaine et imprévisible de l’état de santé au cours du voyage.
  Prestations
Rapatriement :  sur décision de ses médecins, MAAF 
Assistance organise et prend en charge le rapatriement du bénéficiaire jusqu’à son domicile en France** ou dans un hôpital adapté le plus proche de son domicile en France**.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pour bénéficier d’une prise en charge financière 
des frais de justice tout au long de votre dossier et 
quelle que soit la nature de la dépense envisagée 
(frais d’expertise amiable ou judiciaire, commissaire 
de justice * intervenant pour tous les actes relevant 
anciennement des missions d’huissier de justice. 
avocat…), vous devez recueillir notre accord écrit 
préalable avant qu’elle ne soit engagée, sauf si vous 
pouvez justifier d’une situation d’urgence avérée.
Vous pouvez nous contacter au 01 76 62 45 69. Un 
juriste vous donnera toute information utile pour la 
constitution de votre dossier.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4]. Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Then train an adversarial model ( Ma) that minimizes the output predic-
tionscorefromthebasemodelyetmaximizingthechangesinthelearned
attention distribution from the base model using following instance-wide
loss function:
L(Ma,Mb) =TVD (ˆy(i)
1,ˆy(i)
2)−λKL(α(i)
a∥α(i)
b)(7.24)
where ˆy(i)andα(i) denote predictions and attention distributions for an
instance i, and λcontrols the trade-oﬀ between the prediction distance
and attention distribution changes.
Forthediabetesdataset(andtheanemiaandtheIMDB),theyﬁndit
challenging to ﬁnd adversarial weights that produce divergent attention
weights and not losing the predictive performance, thus supporting the
use of attention for the faithful explanation. On the contrary, Stanford
Sentiment Treebank (SST) dataset shows no use of attention weights for
the faithful explanation.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Then, in the second step, it repairs the encoder and trains the decoder
using the DAE and XAE tasks. This method eﬀectively pre-trains all
parameters, and also bridges the gap between MLM pre-training and
autoregressive decoding ﬁne-tuning.
4.2 MULTILINGUAL DATA
4.2.1 Pre-Training Data
Multilingual Language Models (mLMs) use diﬀerent sources of data dur-
ing the pre-training stage. More speciﬁcally, large monolingual corpora
are commonly used in individual languages and parallel corpora are
used between some languages. The source of the monolingual corpora
varies for the diﬀerent existent mLMs. For example, mBERT [72] uses
Wikipedia for pre-training. Whereas, in [61], Conneau et al. used a much
larger common-crawl corpus to train XML-R. Other models use cus-
tom crawled data.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Model Debugging : Akin to software debugging, model debug-
ging corresponds to the inspection of the model architecture, data
processing through the networks, and the errors introduced dur-
ing the training and runtime process [140]. A common research
approach is to build auxiliary models that act as a diagnostic and
inspection probe [2]. The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4]. Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />88■Transformers for Machine Learning: A Deep Dive
Figure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-
ing translation tasks [284].
mUSE uses dual-encoders to perform multitask learning on tasks
that can be represented as ranking input-response sentence pairs. Some
tasks fall naturally within this framework such as QA and translation
tasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-
response ranking can be formally described as follows: For an input-
response pair (sI
i,sR
i), the goal is to get sR
ito higher than all possible re-
sponsessR
j∈SR. In other words, the conditional probability P(sR
i|sI
i)
should be deﬁned as:
P(
sR
i|sI
i)
=eφ(sI
i,sR
i)
∑
sR
j∈SReφ(sR
i,sR
j)
φ(
sI
i,sR
j)
=gI(
sI
i)⊤gR(
sR
j)(4.17)
wheregIis the input sentence encoding function and gRis response
sentence encoding function, gIandgRmake up the dual-encoder.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />88■Transformers for Machine Learning: A Deep Dive
Figure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-
ing translation tasks [284].
mUSE uses dual-encoders to perform multitask learning on tasks
that can be represented as ranking input-response sentence pairs. Some
tasks fall naturally within this framework such as QA and translation
tasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-
response ranking can be formally described as follows: For an input-
response pair (sI
i,sR
i), the goal is to get sR
ito higher than all possible re-
sponsessR
j∈SR.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It initially trains the encoder using MLM and TLM jobs.
Then, in the second step, it repairs the encoder and trains the decoder
using the DAE and XAE tasks. This method eﬀectively pre-trains all
parameters, and also bridges the gap between MLM pre-training and
autoregressive decoding ﬁne-tuning.
4.2 MULTILINGUAL DATA
4.2.1 Pre-Training Data
Multilingual Language Models (mLMs) use diﬀerent sources of data dur-
ing the pre-training stage. More speciﬁcally, large monolingual corpora
are commonly used in individual languages and parallel corpora are
used between some languages. The source of the monolingual corpora
varies for the diﬀerent existent mLMs. For example, mBERT [72] uses
Wikipedia for pre-training. Whereas, in [61], Conneau et al.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>4. What additional options and costs are the responsibility of the renter when using a rental vehicle?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on what additional options and costs are the responsibility of the renter when using a rental vehicle.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          When using a rental vehicle, there are several additional options and costs that are typically the responsibility of the renter. These may include:

1. **Insurance**: The renter is usually responsible for purchasing insurance coverage for the rental vehicle. This may include options such as Collision Damage Waiver (CDW), Loss Damage Waiver (LDW), and Supplemental Liability Insurance (SLI).

2. **Fuel**: The renter is generally responsible for refueling the vehicle before returning it. If the vehicle is returned with less fuel than when it was rented, the renter may be charged a refueling fee.

3. **Additional Drivers**: Adding extra drivers to the rental agreement may come with an additional cost. Each additional driver needs to be listed on the rental agreement and may incur a fee.

4. **Tolls and Traffic Violations**: Any tolls, parking fees, or traffic violations incurred during the rental period are typically the responsibility of the renter.

5. **Upgrades**: If the renter chooses to upgrade to a larger vehicle, add GPS navigation, or opt for other optional features, there may be additional costs associated with these upgrades.

6. **Young Driver Fees**: Drivers under a certain age (often 25) may incur additional fees due to their higher risk profile.

7. **One-Way Rentals**: If the renter picks up the vehicle at one location and returns it to another, there may be a one-way drop-off fee.

It's essential for renters to carefully review the rental agreement and understand all the terms and conditions, including any additional options and costs that they may be responsible for when using a rental vehicle.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />C H A P T E R 3
Bidirectional Encoder
Representations from
Transformers (BERT)
Theadvent of Bidirectional Encoder Representations from Trans-
former (BERT) [72] is considered the onset of a revolution in the
ﬁeld of Natural Language Processing (NLP). BERT uses unlabeled text
to pre-train deep bidirectional contextual representations. This resulted
in rich pre-trained language models that can be ﬁne-tuned with a simple
additional output layer and a reasonably sized labeled dataset to pro-
duce state-of-the-art performance in a broad range of NLP tasks. These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT uses unlabeled text
to pre-train deep bidirectional contextual representations. This resulted
in rich pre-trained language models that can be ﬁne-tuned with a simple
additional output layer and a reasonably sized labeled dataset to pro-
duce state-of-the-art performance in a broad range of NLP tasks. These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■161
and after the position-wise feedforward network, as part of the resid-
ual connection. In GPT-2, the layer norm module instead comes before
the multi-head attention and before the position-wise feedforward. The
residual connection now only includes addition, rather than addition and
layer norm. One additional layer norm module is placed in the ﬁnal de-
coder block, after the multi-head attention. The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The
residual connection now only includes addition, rather than addition and
layer norm. One additional layer norm module is placed in the ﬁnal de-
coder block, after the multi-head attention. The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes. Computing with the raw UTF-8 bytes was not done here, since
byte-level language models were not performing at the level of word-level
language models.1
Four variants with the described architecture were trained. The
largest model of the four is the one called "GPT-2". It has 1.542 billion
parameters and uses 48 transformer decoder layers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■161
and after the position-wise feedforward network, as part of the resid-
ual connection. In GPT-2, the layer norm module instead comes before
the multi-head attention and before the position-wise feedforward. The
residual connection now only includes addition, rather than addition and
layer norm. One additional layer norm module is placed in the ﬁnal de-
coder block, after the multi-head attention. The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes. Computing with the raw UTF-8 bytes was not done here, since
byte-level language models were not performing at the level of word-level
language models.1
Four variants with the described architecture were trained.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The reformulation of training tasks means you can use the same
model and loss function on any NLP task. Using text-to-text trans-
formations (sequence transduction) allows you to train one model on
multiple tasks at once, reusing model architecture, loss functions, and
hyperparameters. Appendix D of Raﬀel et al. has examples of how in-
put was formatted for each of the datasets the T5 model was trained or
ﬁne-tuned on.
5.4 TRANSFORMER SUBMODULE CHANGES
ThissectiondiscussesmodiﬁcationstotheTransformerthatdonotmod-
ify the attention mechanism or the model’s memory proﬁle.
5.4.1 Switch Transformer
The Mixture of Experts (MoE) model has contributed to many suc-
cesses at the expense of complexity and training cost [225]. However,
the mixture of expert models does not share the parameters, resulting</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Additionally, XNLG does a two-stage optimization of
parameters. It initially trains the encoder using MLM and TLM jobs.
Then, in the second step, it repairs the encoder and trains the decoder
using the DAE and XAE tasks. This method eﬀectively pre-trains all
parameters, and also bridges the gap between MLM pre-training and
autoregressive decoding ﬁne-tuning.
4.2 MULTILINGUAL DATA
4.2.1 Pre-Training Data
Multilingual Language Models (mLMs) use diﬀerent sources of data dur-
ing the pre-training stage. More speciﬁcally, large monolingual corpora
are commonly used in individual languages and parallel corpora are
used between some languages. The source of the monolingual corpora
varies for the diﬀerent existent mLMs. For example, mBERT [72] uses
Wikipedia for pre-training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />18■Transformers for Machine Learning: A Deep Dive
Figure 2.5 Encoder-decoder with attention layer.
a sigmoid function to combine them with the values, as given by
α(q,k) =vTtanh(Wkk+Wqq) (2.14)
Scaled dot product, or dot product-based scoring, are faster and more
memory eﬃcient than additive attention mechanisms.
2.3.3 Attention-Based Sequence-to-Sequence
The general changes to an encoder-decoder with the addition of an at-
tention layer and the mapping to query, keys, and values are shown in
Fig. 2.5 and are
1. The output of the last encoder states are used as keys kand values
v
2. The output of the last decoder state, at time t−1is used as query
q
3. The output from the attention layer o, the context variable, is used
for the next decoder state t</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In the recent past, training NLP
algorithms worked only with labeled data, which hindered their progress
for years due to the limited availability and cost of collecting data for
every new task and every new project. One major contribution of BERT
is introducing a new pipeline for training NLP systems where algorithms
can learn about core and generic natural language concepts from huge
amount of cheap unlabeled data, a process also known as self-supervised
learning or model pre-training. A pre-trained model is then ﬁne-tuned
for any speciﬁc downstream task using much smaller task-speciﬁc
labeled datasets. This new pipeline of pre-training followed by ﬁne-
tuning constitutes the core of most advances in the machine learning
ﬁeld. The major success of BERT for English NLP tasks has motivated
its use for other languages. However, using BERT’s pipeline is only pos-
sible for languages with suﬃciently large unlabeled data for pre-training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Performance Talking-Heads Attention (THA) was evaluated by
training a T5 model using THA and the same hyperparameters as the
T5 paper (except for omitting dropout during pre-training) and simi-
larly training an ALBERT model. The authors found that THA consis-
tently performed better than multi-head attention [227]. Just projecting
the attention logits or just the attention weights is only slightlybetter
than using pure multi-head attention. Signiﬁcant performance increases
came from using both projections. Using talking-heads attention on the
encoder block’s self-attention layers has a larger eﬀect on model perfor-
mance than using talking-heads attention on the decoder block’s atten-
tion layers.
Multi-headattentionalreadycomesatacostandtheprojectionsthat
THA adds, WαandWA, increase that cost.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !
Festivals .Fun .Beer .Lots ofbeer .Charlotte Center City
Partners (orFind Your Center )puts onalot ofthese types
offestivals Uptown and inSouth End .When you check out
their website ortheir weekly newsletter you ’llbeable to
seelots ofevents coming uplike Beer ,Blues and BBQ ,
Taste ofCharlotte ,Speedstreet and the like .\n\nMany of
these events and festivals usually have beer available ,
hence why I’mafan .And ,yeah ,Ialso really like
supporting the local organization that ’sresponsible for
Uptown ’sdevelopment .Ifonly there was aPBR festival ...!</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To test the hypothesized ro-
bustness, they authors add six kinds of noise to the data and see how it
aﬀects performance:
1. Make deletions by giving each character has a 10%chance of being
deleted.
2. Give each character a 10%chance of added, deleted, or mutated
(with equal likelihood).
3. Give each character a 20%chance of being duplicated 1-3 times.
4. Capitalize each character and pad it with spaces.
5. Make each character uppercase, when the language uses case.
6. Set the case of each character randomly when the language uses
case.
The noise is injected in one of two ways: into the ﬁne-tuning and
evaluation data or just into the evaluation data. For the question an-
swering task, noise is added to the context but not to the question or
answer. For the sentence entailment training task, noise is added to the
premise and hypothesis.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>5. Discuss the distinction between a "passager à titre gratuit" and a regular passenger in terms of insurance coverage.</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          A "passager à titre gratuit" refers to a passenger who is transported for free, without having to pay any form of compensation. This person can, however, choose to contribute to travel expenses on their own initiative. In terms of insurance coverage, this passenger is usually covered under the vehicle owner's insurance policy. This means that in the event of an accident, the insurance company would cover the costs of any injuries or damages incurred by this passenger.

On the other hand, a regular passenger, who might be paying for the transportation service, may not be covered under the vehicle owner's insurance policy. This is because the insurance policy might only cover the vehicle owner and not the passengers. In this case, the passenger would need to have their own personal insurance coverage to cover any potential injuries or damages.

However, the specifics of insurance coverage can vary greatly depending on the terms of the insurance policy and local regulations. Therefore, it's always important to check the details of an insurance policy to understand who is covered.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          In the context of insurance coverage, there is a distinction between a "passager à titre gratuit" (a passenger transported free of charge) and a regular passenger. Here are some key points to consider regarding their insurance coverage:

1. **Passager à Titre Gratuit (Passenger Transported Free of Charge):**
   - A passager à titre gratuit is someone who is being transported in a vehicle without having to pay any fare or fee for the transportation.
   - This type of passenger is typically not considered a commercial passenger and is not paying for the transportation service.
   - In terms of insurance coverage, a passager à titre gratuit may still be covered under the vehicle owner's insurance policy, especially in cases of accidents, mechanical breakdowns, theft, vandalism, or other covered events.
   - The insurance coverage for a passager à titre gratuit may vary depending on the specific terms and conditions of the insurance policy.

2. **Regular Passenger:**
   - A regular passenger is someone who is being transported in a vehicle and may be paying a fare or fee for the transportation service.
   - Regular passengers are often covered under the vehicle owner's insurance policy, especially in cases of accidents or other covered events.
   - The insurance coverage for regular passengers may also depend on the specific terms and conditions of the insurance policy, including any limitations or exclusions related to passengers.

3. **Insurance Coverage Considerations:**
   - Insurance policies typically outline who is covered under the policy, including passengers, and the extent of coverage provided.
   - It's important for vehicle owners to review their insurance policies to understand who is covered as a passenger and under what circumstances.
   - In the case of passagers à titre gratuit, it's advisable for both the vehicle owner and the passenger to clarify any insurance coverage questions or concerns before the transportation takes place.

In summary, while both passagers à titre gratuit and regular passengers may be covered under a vehicle owner's insurance policy, there may be differences in coverage or considerations based on the specific circumstances of the transportation and the terms of the insurance policy. It's essential for all parties involved to be aware of the insurance coverage in place to ensure protection in case of any unforeseen events.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />PERSONNE TRANSPORTÉE À TITRE GRATUIT
Tout passager transporté gratuitement sans avoir à verser une quelconque 
rétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais 
de route de sa propre initiative.
PERTE TOTALE
Un véhicule est considéré en perte totale lorsque le montant des réparations à 
effectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est 
contrôlée par expert.PRESCRIPTION
Laps de temps au terme duquel un droit non exercé est perdu.
S                                                     
SINISTRE
Évènement susceptible d’entraîner l’application des garanties du contrat.
SOUSCRIPTEUR
Personne qui a conclu et signé le contrat avec l’assureur.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />P                                                            
PANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE
Incident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, 
non imputable à une erreur de son utilisateur.
PERSONNE TRANSPORTÉE À TITRE GRATUIT
Tout passager transporté gratuitement sans avoir à verser une quelconque 
rétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais 
de route de sa propre initiative.
PERTE TOTALE
Un véhicule est considéré en perte totale lorsque le montant des réparations à 
effectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est 
contrôlée par expert.PRESCRIPTION
Laps de temps au terme duquel un droit non exercé est perdu.
S                                                     
SINISTRE
Évènement susceptible d’entraîner l’application des garanties du contrat.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Elle peut être consécutive ou non à une situation préexistante. Elle n’a 
pas pour origine un accident corporel.
P                                                            
PANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE
Incident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, 
non imputable à une erreur de son utilisateur.
PERSONNE TRANSPORTÉE À TITRE GRATUIT
Tout passager transporté gratuitement sans avoir à verser une quelconque 
rétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais 
de route de sa propre initiative.
PERTE TOTALE
Un véhicule est considéré en perte totale lorsque le montant des réparations à 
effectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est 
contrôlée par expert.PRESCRIPTION
Laps de temps au terme duquel un droit non exercé est perdu.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />PERSONNE TRANSPORTÉE À TITRE GRATUIT
Tout passager transporté gratuitement sans avoir à verser une quelconque 
rétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais 
de route de sa propre initiative.
PERTE TOTALE
Un véhicule est considéré en perte totale lorsque le montant des réparations à 
effectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est 
contrôlée par expert.PRESCRIPTION
Laps de temps au terme duquel un droit non exercé est perdu.
S                                                     
SINISTRE
Évènement susceptible d’entraîner l’application des garanties du contrat.
SOUSCRIPTEUR
Personne qui a conclu et signé le contrat avec l’assureur. C’est son nom qui apparaît 
sur le contrat et il est généralement désigné par le terme « vous ».</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Qui bénéficie des garanties d’Assistance ?
• Le souscripteur  du contrat d’assurance AUTO .
• Son conjoint  de droit ou de fait.
•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge 
et vivant sous le même toit.
•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule 
assuré  en cas de panne mécanique, électronique ou électrique , d’accident 
de la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. 
Le bénéficiaire est généralement désigné par le terme « Vous » dans les 
conditions ci-après. 
Pour quel véhicule ?
• Le véhicule assuré  désigné aux conditions particulières .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge 
et vivant sous le même toit.
•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule 
assuré  en cas de panne mécanique, électronique ou électrique , d’accident 
de la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. 
Le bénéficiaire est généralement désigné par le terme « Vous » dans les 
conditions ci-après. 
Pour quel véhicule ?
• Le véhicule assuré  désigné aux conditions particulières .
•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule 
assuré  en cas d’indisponibilité fortuite de ce dernier.
•  Les caravanes ou les remorques attelées au véhicule assuré  d’un poids total 
autorisé en charge maximum de 750 kg.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Qui bénéficie des garanties d’Assistance ?
• Le souscripteur  du contrat d’assurance AUTO .
• Son conjoint  de droit ou de fait.
•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge 
et vivant sous le même toit.
•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule 
assuré  en cas de panne mécanique, électronique ou électrique , d’accident 
de la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. 
Le bénéficiaire est généralement désigné par le terme « Vous » dans les 
conditions ci-après. 
Pour quel véhicule ?
• Le véhicule assuré  désigné aux conditions particulières .
•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule 
assuré  en cas d’indisponibilité fortuite de ce dernier.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />50  CONDITIONS GÉNÉRALES   
Le symbole renvoie à un terme  
défini au lexi que (article 1 ) TITRE III 
        GARANTIES DE PROTECTION JURIDIQUE  
Les seuils de déclenchement et les plafonds applicables aux garanties de Protection Juridique figurent à l’article 3  des présentes 
conditions  générales . 
 
ARTICLE  28    Protection Juridique suite à accident  
 
La gestion des sinistres de Protection Juridique suite à accident  est effectuée dans le cadre de la première des modalités de  gestion 
prévues par l’article L. 322 -2-3 du Code des assurances : elle est confiée à  un personnel distinct au sein de l’entreprise.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />L                                                      
LOCATAIRE
Personne qui obtient le droit d’utiliser le bien loué contre le versement d’une 
somme d’argent appelée loyer.
M                                                       
MALADIE
Altération soudaine et imprévisible de la santé, constatée par une autorité médicale 
compétente. Elle peut être consécutive ou non à une situation préexistante. Elle n’a 
pas pour origine un accident corporel.
P                                                            
PANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE
Incident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, 
non imputable à une erreur de son utilisateur.
PERSONNE TRANSPORTÉE À TITRE GRATUIT
Tout passager transporté gratuitement sans avoir à verser une quelconque 
rétribution.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Nous
La société d’assurances désignée aux Conditions 
personnelles.
Nullité
Voir " Sanctions  ".
Option
Voir " Accessoires  ".
Passager
Personne transportée à l’intérieur du véhicule.
 • Passager à titre gratuit : il ne verse pas de rému -
nération, même s’il participe aux frais de route.
Il peut s’agir de co-voiturage, c’est-à-dire l’utili -
sation conjointe et organisée d’un véhicule par un 
conducteur non professionnel et un ou plusieurs 
tiers passagers, dans le but d’effectuer un trajet 
commun. Le co-voiturage ne doit pas être rému -
néré, hormis le partage des frais liés au trajet.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■37
(a) Example showing the En-
glish word “going” pays atten-
tion to “je” and “vais”.
(b) Example showing the En-
glishword“be”paysattentionto
“en” and “etre”.
(c) English word “library” pays
attention to multiple words.
(d) The words “two” and “deux”
are matched.
Figure 2.17 Attention examples and plots.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■39
Figure 2.19 Loss and perplexity on the test set.
Fig. 2.20 shows RNN vs. Transformer for the translation and it can
be seen that the RNN pairs the formal/plural “serez” with the infor-
mal/singular “tu” while transformer matches “seras tu”.
Fig. 2.21 shows another RNN vs. Transformer for the translation
and it can be seen that the RNN doesn’t capture the “snowing” portion
of the sentence and produces a diﬀerent phrasing of “my house”.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />40■Transformers for Machine Learning: A Deep Dive
Figure 2.20 Explaining translations for—How long you will be here?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp. 599–604.
[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel ,Backpropagation
applied to handwritten zip code recognition , Neural Computation,
1 (1989), pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />I think
that everyone with a City-Pass crams inthere insearch of
the best two-for-one deal on a massive mound of macaroni
slathered indreadful red sauce and salty cheese. \n\nI
actually ordered a salad as my main the last time that I
dined there because I know how universally disappointing
the pasta dishes were and they actually screwed up a salad.
I am not sure what on earth it was supposed to be, but they
called it a chopped salad and it had a little M next to it
inthe menu as ifit were a specialty of the house. I asked
for grilled chicken on top and received a dried out piece
of leather sitting above a mess of lettuce, beans, nuts,
cheese and peppers. Just plain salty and awful. Everything
was either from a can ora jar. \n\nI do agree with others
who have said that the service isvery fast and friendly.
They kept the beer and wine flowing at our table at every
visit.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).
[212]F. Rosenblatt ,The perceptron: A probabilistic model for infor-
mation storage and organization in the brain , Psychological Re-
view, (1958), pp. 65–386.
[213]D. Rothman ,Transformers for Natural Language Processing ,
Packt, 2021.
[214]A.Roy,M.Saffar,A.Vaswani,andD.Grangier ,Eﬃcient
content-based sparse attention with routing transformers , Transac-
tions of the Association for Computational Linguistics, 9 (2021),
pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp. 599–604.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>6. What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          If the dispute persists after attempting to resolve it internally, the insured, if they are a non-professional individual who has subscribed to the contract for non-professional needs, can then seize the Insurance Mediation. The contact details are as follows:
- Address: TSA 50110 75441 Paris Cedex 09
- Internet: http://www.mediation-assurance.org<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          If a dispute persists between the insured and the insurer after attempting to resolve it internally, the next course of action would be to seek external resolution through the following steps:

1. **Commission de recours interne**: The insured can address the Commission de recours interne. The insurer will provide the insured with the contact details upon request.

2. **Médiation de l’Assurance**: If the disagreement continues and the insured is an individual who has taken out the contract for non-professional purposes, they can contact the Médiation de l’Assurance. The insured can find the details as follows:
   - Address: TSA 50110 75441 Paris Cedex 09
   - Website: http://www.mediation-assurance.org

3. **Legal Action**: If the dispute remains unresolved even after the above steps, the insured can consider taking legal action to address the issue.

It's important to follow the specific procedures outlined by the insurer and regulatory bodies to ensure a fair resolution of the dispute.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.
For oﬄine RL, the trajectories in the dataset are of the form
(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)
where ˆRt=∑T
t′=trt′isthe“return-to-go”,whichistheamountofreward
that needs to be generated to get to the end of the trajectory from t.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la
Commission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.
Si ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des
besoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :
- Adresse : TSA 50110 75441 Paris Cedex 09 ;
- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la
Commission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.
Si ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des
besoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :
- Adresse : TSA 50110 75441 Paris Cedex 09 ;
- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The ﬁrst two
time steps are shown. ER,ES, andEArepresent the combined embed-
ding layer positional encoding, and layer norm for the returns-to-go,
states, and actions, respectively. The layer after GPT-2 contains three
linear submodules for predicting the next element of the sequence. Next
state, action, and return-to-go are predicted using diﬀerent submodules.
These ﬁxed trajectories are used for autoregressive training. The pre-
viousKstepsarepassedtotheDecisionTransformer.Eachstepcontains
three “tokens”: the state, action, and return-to-go. Each type of token
has its own linear embedding layer, which is followed up by a Layer-
Norm. There are also learned positional embeddings added to each type
of token embeddings that are diﬀerent that in the standard transformer
because each time step has three tokens instead of one. The embeddings
are then passed to a GPT-2 model, for autoregressive training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■179
Figure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two
time steps are shown. ER,ES, andEArepresent the combined embed-
ding layer positional encoding, and layer norm for the returns-to-go,
states, and actions, respectively. The layer after GPT-2 contains three
linear submodules for predicting the next element of the sequence. Next
state, action, and return-to-go are predicted using diﬀerent submodules.
These ﬁxed trajectories are used for autoregressive training. The pre-
viousKstepsarepassedtotheDecisionTransformer.Eachstepcontains
three “tokens”: the state, action, and return-to-go. Each type of token
has its own linear embedding layer, which is followed up by a Layer-
Norm.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />La politique de l’Assureur en matière de satisfaction client est disponible sur s on site à l’adresse suivante : http://www.aig.com.  
 
Conformément à la recommandation 2011-R05 de l’ACPR, une réponse sera apportée à l’Assuré dans un délai maximum de 2 mois (sauf circonstances particulières dont l’Assuré sera tenu informé).   Si le désaccord persiste après la réponse donnée par l’Assureur, l’Assuré peut, sans préjudice de 
ses droits à intenter une action en justice, saisir le Médiateur : La Médiation de l’Assurance - TSA 
50110 - 75441 Paris Cedex 09. http://www.mediation-assurance.org
/. Ce recours est gratuit.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Préjudice d’affection  
Souffrances morales subies par le/les proches, ayant la qualité de bénéficiaire(s) au titre du contrat, suite au décès de l’a ssuré.  
 
Préjudice écologique  
Atteinte non négligeable aux éléments ou aux fonctions des écosystèmes ou aux bénéfices c ollectifs tirés par l’homme de 
l’environnement.  
 
Préjudice esthétique permanent  
Atteintes altérant l’apparence physique de l’assuré persistant après consolidation.  
 
Préposé  
Personne qui accomplit un acte ou une formation déterminée sous la direction ou le contrôle d’une autre.  
 
Prescription  
Délai à l’issue duquel le titulaire d’un droit ne dispose plus d’action pour le faire valoir.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Next
state, action, and return-to-go are predicted using diﬀerent submodules.
These ﬁxed trajectories are used for autoregressive training. The pre-
viousKstepsarepassedtotheDecisionTransformer.Eachstepcontains
three “tokens”: the state, action, and return-to-go. Each type of token
has its own linear embedding layer, which is followed up by a Layer-
Norm. There are also learned positional embeddings added to each type
of token embeddings that are diﬀerent that in the standard transformer
because each time step has three tokens instead of one. The embeddings
are then passed to a GPT-2 model, for autoregressive training. Note that
forimageinputs,theembeddinglayerisaconvolutionalencoder,ascom-
monly used in computer vision. A diagram of the Decision Transformer
architecture is shown in Fig. 6.6.
Since the input tokens are a sequence of rewards-to-go, states, and
actions, the multi-head self-attention mechanism computes attention be-
tween rewards, states, and actions.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Foreword
Renowned AI pioneer and Nobel laureate Herbert Simon underscored
“attention” as the most valuable resource of the information econ-
omy, as necessary to allocate attention eﬃciently among the over-
abundance of information resources. Having written the foundational
paper on meaning-aware AI and recently having served as MIT-
Princeton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-
lishing by invitation in the same journal’s special issue of ASQ, and of
being the Malcolm Baldrige National Quality Award administrator, as
well as being ranked along with Dr. Simon in the same global academic
citation impact studies.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>7. What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          Failing to pay one installment of your insurance premium can have several implications:

1. Cancellation of Policy: If you miss a payment and do not make up for it within the grace period, your insurance company may cancel your policy. This means you will no longer be covered under the policy.

2. Loss of Coverage: If your policy is cancelled, you will lose your coverage. This means you will be financially responsible for any losses or damages that occur after the cancellation.

3. Higher Premiums in the Future: If your policy is cancelled due to non-payment, it could lead to higher premiums in the future. Insurance companies often consider your payment history when determining your premium rates.

4. Difficulty Getting New Insurance: If your policy is cancelled due to non-payment, it could make it more difficult for you to get insurance in the future. Some insurance companies may not be willing to insure you if you have a history of non-payment.

5. Legal Consequences: Depending on the laws in your area, driving without insurance could result in fines, license suspension, or even jail time.

6. Credit Score Impact: Some insurance companies report missed payments to credit bureaus, which could negatively impact your credit score. 

It's important to contact your insurance company as soon as possible if you're unable to make a payment. They may be able to work out a payment plan or offer other solutions to help you keep your coverage.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Failing to pay one installment of your insurance premium when it is divided into multiple payments can have several implications:

1. **Policy Lapse**: Missing a payment could lead to your insurance policy lapsing. This means that your coverage will no longer be active, and you will not be protected in case of any incidents.

2. **Late Fees**: Insurance companies often charge late fees for missed payments. These fees can add up over time, increasing the overall cost of your insurance.

3. **Reinstatement**: If your policy lapses due to missed payments, you may have the option to reinstate it by paying the overdue amount along with any applicable fees. However, this process can be cumbersome and may involve additional paperwork.

4. **Impact on Coverage**: Even if you reinstate your policy, missing payments can still have an impact on your coverage. Some insurance companies may restrict coverage or increase premiums for policyholders with a history of missed payments.

5. **Credit Score**: Failing to pay insurance premiums on time could also have a negative impact on your credit score. Insurance companies may report late payments to credit bureaus, affecting your creditworthiness.

It is essential to communicate with your insurance provider if you are facing difficulties making payments. They may offer options such as payment extensions or restructuring your payment plan to help you maintain your coverage without negative consequences.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■17
Figure 2.4 Attention mechanism showing query, keys, values, and output
vector interactions.
2.3.2.1 Dot product (multiplicative)
The dot product-based scoring function is the simplest one and has no
parameters to tune [176].
α(q,k) =q·k (2.12)
2.3.2.2 Scaled dot product or multiplicative
The scaled dot product-based scoring function divides the dot product
by√dkto remove the inﬂuence of dimension dk[254]. According to
Vaswani et al., as the dimension increases, the dot products grow larger,
which pushes the softmax function into regions with extreme gradients.
α(q,k) =q·k√dk(2.13)
2.3.2.3 Linear, MLP , or Additive
Luong et al. also experimented by projecting the query and keys to a
hidden layer of dimension h, learning the weights (W k,Wq), and using</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Note that for the multihead case, QnKT
nis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the
query and key spaces to dk/h. The attention heads are then combined
as described in (2.19).
5.2.1.2 Space and time complexity
Computing the attention weight matrix described in section 5.27 takes
O(L2·dk)matrix multiplications and computing the context vector in
section 5.23 needsO(L2·dv)matrix multiplications, so the time com-
plexity of self-attention is O(L2·dk+L2·dv).
Consider a single input sequence of Ltokens and that the query,
key, and value share the same dimensionality, so dk=dv=dmodel. This
means that Q,K,VareL×dmodelmatrices and the attention weight
matrix from (5.23) is L×L.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformer Modiﬁcations ■123
Figure 5.6 The query matrix, Q, can be partitioned into hheads, as de-
scribed in (5.28).
Note that for the multihead case, QnKT
nis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the
query and key spaces to dk/h. The attention heads are then combined
as described in (2.19).
5.2.1.2 Space and time complexity
Computing the attention weight matrix described in section 5.27 takes
O(L2·dk)matrix multiplications and computing the context vector in
section 5.23 needsO(L2·dv)matrix multiplications, so the time com-
plexity of self-attention is O(L2·dk+L2·dv).
Consider a single input sequence of Ltokens and that the query,
key, and value share the same dimensionality, so dk=dv=dmodel.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This is because multiplication of matri-
ces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices
whose sparsity is grouped into blocks can be eﬃciently multiplied on
GPUs [100].
To address this, Big Bird groups blocks of queries and keys together
and then adds sparsity by block rather than by individual sequence po-
sition [292]. Select a block size, b, and then divide the sequence length
LintoL/bblocks. So, instead of Lqueries and keys, there will be L/b
queries and keys. This modiﬁes each attention pattern discussed above
in relatively simple ways:
1.Random attention The random number of keys for a query to
attend to, r, becomes the random number of key blocks that a
query block attends to.
2.Sliding window attention The query block iattends to key
blocksi−(w−1)/2throughi+ (w−1)/2.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformer Modiﬁcations ■137
Eﬃcient sparse matrix multiplication By making the adjacency
matrix (and the corresponding attention matrix) sparse, the speedups
provided by GPUs are removed. This is because multiplication of matri-
ces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices
whose sparsity is grouped into blocks can be eﬃciently multiplied on
GPUs [100].
To address this, Big Bird groups blocks of queries and keys together
and then adds sparsity by block rather than by individual sequence po-
sition [292]. Select a block size, b, and then divide the sequence length
LintoL/bblocks. So, instead of Lqueries and keys, there will be L/b
queries and keys. This modiﬁes each attention pattern discussed above
in relatively simple ways:
1.Random attention The random number of keys for a query to
attend to, r, becomes the random number of key blocks that a
query block attends to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■37
(a) Example showing the En-
glish word “going” pays atten-
tion to “je” and “vais”.
(b) Example showing the En-
glishword“be”paysattentionto
“en” and “etre”.
(c) English word “library” pays
attention to multiple words.
(d) The words “two” and “deux”
are matched.
Figure 2.17 Attention examples and plots.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■161
and after the position-wise feedforward network, as part of the resid-
ual connection. In GPT-2, the layer norm module instead comes before
the multi-head attention and before the position-wise feedforward. The
residual connection now only includes addition, rather than addition and
layer norm. One additional layer norm module is placed in the ﬁnal de-
coder block, after the multi-head attention. The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes. Computing with the raw UTF-8 bytes was not done here, since
byte-level language models were not performing at the level of word-level
language models.1
Four variants with the described architecture were trained. The
largest model of the four is the one called "GPT-2". It has 1.542 billion
parameters and uses 48 transformer decoder layers. Each was evaluated
on several language modeling datasets without any additional training.
GPT-2 achieved state-of-the-art on seven out of eight datasets.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Finally,Section 4.4 provides a practical case study on zero-shot multi-
lingual sentiment classiﬁcation.
4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES
This section discusses the diﬀerent architectural designs for multilingual
Transformers. We divide multilingual transformers into (i) Natural Lan-
guage Understanding (NLU) architectures and (ii) Natural Language
Generation (NLG) architectures. Table 4.1 [74] provides a summary of
the impactful multilingual transformer models available at the time of
publishing this book.
4.1.1 Basic Multilingual Transformer
Multilingual transformer models are typically based on the mBERT ar-
chitecture [72]. In this section, we describe the basic components of a
multilingual transformer architecture.
Input Layer A series of tokens is provided as input to the multilin-
gual transformer. The token input is derived from a one-shot represen-
tation of a limited vocabulary, often a subword vocabulary.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The
residual connection now only includes addition, rather than addition and
layer norm. One additional layer norm module is placed in the ﬁnal de-
coder block, after the multi-head attention. The weights of the residual
layers are initialized diﬀerently than in the GPT model. The weights for
a residual layer are divided by 1/√
N, whereNis the number of residual
layers in the entire model.
GPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any
UTF-8 string can be represented using a vocabulary that of only 256
bytes. Computing with the raw UTF-8 bytes was not done here, since
byte-level language models were not performing at the level of word-level
language models.1
Four variants with the described architecture were trained. The
largest model of the four is the one called "GPT-2". It has 1.542 billion
parameters and uses 48 transformer decoder layers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi. The importance of the feature xi
is measured using:
Diﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)
7.3.2 Model Distillation
Xie et al. refer to the model distillation category as a post-training
method where the encoded knowledge in the model is distilled into a rep-
resentation agreeable for an explanation by a user, as shown in Fig. 7.3.
Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Multilingual Transformer Architectures ■91
is obvious that the number of available articles in English is much higher
than other languages such as Persian or Urdu. Similarly, the amount of
parallel data available for pairs of languages diﬀer highly depending of
the popularity of such languages. To overcome these challenges, most
mLMs use exponentially smoothed weighting of the data while creat-
ing the pre-training data. This weighting prevents the low resource lan-
guagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal
pre-training data belongs to language i, then the probability of this lan-
guage ispi=k
100. Each probability is then modulated by an exponen-
tial factor α, then the resulting values are normalized to provide the
ﬁnal probability distribution over the languages. This probability dis-
tribution is used to the sample the pre-training data from the diﬀerent
languages.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■37
(a) Example showing the En-
glish word “going” pays atten-
tion to “je” and “vais”.
(b) Example showing the En-
glishword“be”paysattentionto
“en” and “etre”.
(c) English word “library” pays
attention to multiple words.
(d) The words “two” and “deux”
are matched.
Figure 2.17 Attention examples and plots.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization. Pre-training tasks has
also been examined as a possible confounding factor, it was shown that
models trained with parallel corpora pre-training tasks such as XLM re-
tain language-agnostic representations within the higher encoder layers
in a stronger manner than models trained on monolingual corpora such
as mBERT and XLMR [52].
Parallel Corpora Representations Another way to approach the
language-agnostic representation hypothesis is to use parallel corpora in
diﬀerent languages and examine their parallel representations for align-
ment in the model embedding space.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■39
Figure 2.19 Loss and perplexity on the test set.
Fig. 2.20 shows RNN vs. Transformer for the translation and it can
be seen that the RNN pairs the formal/plural “serez” with the infor-
mal/singular “tu” while transformer matches “seras tu”.
Fig. 2.21 shows another RNN vs. Transformer for the translation
and it can be seen that the RNN doesn’t capture the “snowing” portion
of the sentence and produces a diﬀerent phrasing of “my house”.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They demon-
strate this by showing that when a mLM is ﬁne-tuned for POS tagging,
its cross-lingual retrieval performance decreases signiﬁcantly. To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained. They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance. Taking this
into account, Wang et al. [263] provide an explicit strategy for align-
ing the representations of matched word pairs across languages during
mBERT training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In [260] and [45] it is contended that since mLMs have limited model
capacity that gets used by multiple languages, they cannot capture all
the subtleties of several languages as a pre-trained monolingual model
can. They demonstrate how knowledge distillation from a monolingual
model may enhance a mLM’s cross-lingual performance.
4.3.1.3 Model tasks factors
Fine-tuning Strategies In [171] it is discussed that ﬁne-tuning a
mLM changes its parameters, impairing its cross-lingual capacity by
erasing part of the alignments learned during pre-training. They demon-
strate this by showing that when a mLM is ﬁne-tuned for POS tagging,
its cross-lingual retrieval performance decreases signiﬁcantly. To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>8. What type of guarantee does the insurance provide for legal defense in criminal cases?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on what type of guarantee the insurance provides for legal defense in criminal cases.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The insurance typically provides coverage for legal defense in criminal cases, which means that the policyholder will have financial assistance to hire a lawyer and cover legal fees associated with defending against criminal charges. This coverage can vary depending on the specific terms and conditions of the insurance policy, so it is important to review the policy documents to understand the extent of coverage provided for legal defense in criminal cases.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Table 4.1 [74] provides a summary of
the impactful multilingual transformer models available at the time of
publishing this book.
4.1.1 Basic Multilingual Transformer
Multilingual transformer models are typically based on the mBERT ar-
chitecture [72]. In this section, we describe the basic components of a
multilingual transformer architecture.
Input Layer A series of tokens is provided as input to the multilin-
gual transformer. The token input is derived from a one-shot represen-
tation of a limited vocabulary, often a subword vocabulary. Typically,
this vocabulary is learned by concatenating monolingual data from sev-
eral languages using algorithms such as BPE [223], WordPiece [273], or
SentencePiece [144]. To guarantee that diverse languages and scripts are
well represented in the vocabulary, data may be sampled using exponen-
tial weighted smoothing or distinct vocabularies for clusters of languages
can be learned [55] by dividing the vocabulary size.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Section 4.3 provides
someinsightsintotheinnerworkingsofmultilingualtransformermodels.
Finally,Section 4.4 provides a practical case study on zero-shot multi-
lingual sentiment classiﬁcation.
4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES
This section discusses the diﬀerent architectural designs for multilingual
Transformers. We divide multilingual transformers into (i) Natural Lan-
guage Understanding (NLU) architectures and (ii) Natural Language
Generation (NLG) architectures. Table 4.1 [74] provides a summary of
the impactful multilingual transformer models available at the time of
publishing this book.
4.1.1 Basic Multilingual Transformer
Multilingual transformer models are typically based on the mBERT ar-
chitecture [72]. In this section, we describe the basic components of a
multilingual transformer architecture.
Input Layer A series of tokens is provided as input to the multilin-
gual transformer.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Finally,Section 4.4 provides a practical case study on zero-shot multi-
lingual sentiment classiﬁcation.
4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES
This section discusses the diﬀerent architectural designs for multilingual
Transformers. We divide multilingual transformers into (i) Natural Lan-
guage Understanding (NLU) architectures and (ii) Natural Language
Generation (NLG) architectures. Table 4.1 [74] provides a summary of
the impactful multilingual transformer models available at the time of
publishing this book.
4.1.1 Basic Multilingual Transformer
Multilingual transformer models are typically based on the mBERT ar-
chitecture [72]. In this section, we describe the basic components of a
multilingual transformer architecture.
Input Layer A series of tokens is provided as input to the multilin-
gual transformer. The token input is derived from a one-shot represen-
tation of a limited vocabulary, often a subword vocabulary.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />10■Transformers for Machine Learning: A Deep Dive
•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-
ger and Jeremy Howard https://course.fast.ai/
•Jay Alammar’s visual explanation of transformers and related ar-
chitectures https://jalammar.github.io/
1.3.4 Case Studies and Details
At the end of Chapters 2–7, we include a case study that allows the
reader to see how one or more of the models and methods discussed in
the chapter can be applied, or how they stack up against one another
when applied to the same problem. The aim is for the case study to
provide a small starting point in working with transformer models from
which one can branch out further. Each case study has been chosen to
run within approximately one hour on GPUs at least as powerful as
the NVIDIA K80 (Google Colaboratory provides these for free).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Simon in the same global academic
citation impact studies.
Given the above background, I am thrilled to share with you the
most thorough and up-to-date compendium of research, practices, case
studies, and applications available today that can provide the best ROI
on the latest AI technological advances on transformers inspired by the
paper, “Attention is All You Need.” Since Google introduced transformer
architecture in 2017, transformers have provided exponential improve-
ments in context-focused realization toward meaning-aware AI as deep
(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch
as dot-product attention and multi-head attention. Resulting advances
in enhanced parallel processing of sequential data have made eﬃcient
context sensitive and hence more “meaningful” for ever-larger datasets
and much more feasible than earlier.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For layer-level knowledge localization, provided that the ﬁrst layer of
BERT gets representations in the form of a mix of token, segment, and
positionalembeddingsasinput.Itcomestoreasonthatthebottomlevels
contain the most linear information about word order. It was shown that
theknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-
base. This is followed by an enhanced understanding of the hierarchical
structure of sentences. Numerous studies have shown that the middle
BERT layers contain the most syntactic information and that the last
BERT levels include the most task-speciﬁc information. Additionally, it
was shown that, although the majority of syntactic information may be
localized in a few levels, semantic information is distributed across the
model, which explains why some non-trivial cases are initially handled
wrong then successfully at higher layers.
3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS
3.5.1 Goal
In this chapter, we looked at several applications of the Transformer
architecture.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />GPT-3 had over 1000 times as many parameters as
GPT-3 small. All models were trained on 300 billion tokens and used a
context window of 2048 tokens. Note that the position-wise feedforward
network had dff= 4dmodel. Larger models using larger batch sizes used
smaller learning rates.
Performance Several sizes of GPT-3 models were evaluated for zero-,
one-, and few-shot learning, which Brown et al. describe as diﬀerent
types of in-context learning. With the few-shot type of learning, GPT-3
givesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe
context window (10–100 examples). One-shot learning then provides one
example and a task description, while zero-shot provides no examples,
only a task description. There is no gradient updating or ﬁne-tuning.
The model is assessed on 12 NLP datasets.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />10■Transformers for Machine Learning: A Deep Dive
•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-
ger and Jeremy Howard https://course.fast.ai/
•Jay Alammar’s visual explanation of transformers and related ar-
chitectures https://jalammar.github.io/
1.3.4 Case Studies and Details
At the end of Chapters 2–7, we include a case study that allows the
reader to see how one or more of the models and methods discussed in
the chapter can be applied, or how they stack up against one another
when applied to the same problem. The aim is for the case study to
provide a small starting point in working with transformer models from
which one can branch out further. Each case study has been chosen to
run within approximately one hour on GPUs at least as powerful as
the NVIDIA K80 (Google Colaboratory provides these for free). Case
studies are also available in the Github repository which accompanies
this book: https://github.com/CRCTransformers/deepdive-book.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Larger models using larger batch sizes used
smaller learning rates.
Performance Several sizes of GPT-3 models were evaluated for zero-,
one-, and few-shot learning, which Brown et al. describe as diﬀerent
types of in-context learning. With the few-shot type of learning, GPT-3
givesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe
context window (10–100 examples). One-shot learning then provides one
example and a task description, while zero-shot provides no examples,
only a task description. There is no gradient updating or ﬁne-tuning.
The model is assessed on 12 NLP datasets.
Results showed that as the number of model parameters increases,
the model needs fewer demonstrations to learn how to perform a task
(and hence to reach a given accuracy target). When few-shot is used,
model performance increases more quickly with model size, suggesting
larger models are better at in-context learning.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Bidirectional Encoder Representations from Transformers (BERT) ■63
(’liquor’, 0.005587858949299897),
(’center city’, 0.005496678910160935),
(’beers’, 0.005368697666709216),
(’events’, 0.005089779403417317)]
Listing 3.12 Median topic: Nightlife in Charlotte, NC
3.6 CASE STUDY: FINE-TUNING BERT
3.6.1 Goal
The goals of this case study is to provide a step-by-step demo of ﬁne-
tuning a standard BERT model for any sentence classiﬁcation, we chose
sentiment classiﬁcation as a sample popular task.
3.6.2 Data, Tools, and Libraries
We chose the Google Play application review dataset.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295]. Black-box and white-box attacks
aretwomaintypesofadversarialattacksthatgenerateexamplesto
deceive the model [233, 86, 76]. Recent research shows that adding
imperceptible noise by perturbing inputs reveals model vulnerabil-
ities [178]. Adversarial defense is about making the model robust</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295]. Black-box and white-box attacks
aretwomaintypesofadversarialattacksthatgenerateexamplesto
deceive the model [233, 86, 76]. Recent research shows that adding
imperceptible noise by perturbing inputs reveals model vulnerabil-
ities [178].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>9. Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The exclusion for damages caused by transporting hazardous materials does not apply when the vehicle is transporting oils.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The exclusion for damages caused by transporting hazardous materials does not apply under the following circumstances:

- When the sums paid are solely for the reimbursement of travel expenses.
- When the vehicle insured was rented at the time of the damage.
- When the aid consists of a towing operation, the guarantee is only acquired if the towing is carried out in accordance with legal or regulatory provisions.
- For the reimbursement of cleaning and restoration costs of the insured vehicle's interior trim, the driver's clothing, and the passengers' clothing following the voluntary emergency transport of a road accident victim.

These are the exceptions to the exclusion clause for damages caused by transporting hazardous materials.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As spatial resolution increases and the videos get longer, divided
space-timeattentionscalesbetterthanthejointspace-timeattentionbe-
cause the spatial and temporal components are separated in the divided
case. Note that, for a single patch, the spatial and temporal attention
are both linear. Spatial is O(N)and temporal is O(F), so the divided
scales asO(N+F). The joint attention is O(N·F).
The paper ﬁnds that in many cases, spatial attention is more im-
portant than temporal attention. But, there are cases where the tem-
poral attention is very important. Another ﬁnding is that the divided
space-time attention is able to learn more than the full, joint space-time
attention because the divided case treats them as two separate attention
mechanisms, and thus it has twice the parameters and can learn more, in
principal. Because of this, the recommended attention method is divided
space-time attention.
6.6 GRAPH TRANSFORMERS
Can transformers be applied to graph datasets?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />TaBERT’s representations may then be ﬁne-tuned further
using the training data for that job.
TaBERT is trained via content snapshots, in which the model only
encodes the bits of a table that are most relevant to a query. Due to the
enormous amount of rows in certain database tables, encoding them is a
computationallyintensiveandineﬃcientprocedure.Byencodingjustthe
portion of material that is most relevant to the utterance, content snap-
shots enable TaBERT to handle with enormous tables. For instance, the
phrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted
from the WikiTableQuestions dataset) may be accompanied with a table
includinginformationontheyear,venue,position,andevent.Asnapshot
of content will take a subset of three rows. This subset will not include
all of the table’s data, but it is suﬃcient for the model to understand
that, for example, the venue column comprises cities.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Exclusions
LES DOMMAGES SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.
Exclusions
LES DOMMAGES CAUSÉS OU SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,
  le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />son contenu, son conducteur et ses 
ayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.
Exclusions
LES DOMMAGES CAUSÉS OU SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,
  le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ;</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.
Exclusions
LES DOMMAGES CAUSÉS OU SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,
  le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; toutefois la non-garantie ne saurait être invoquée du chef de transports d’huiles,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule assuré faisait l’ob- jet d’une location lors de la survenance du dom- mage.Exclusions
LES DOMMAGES SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.
Exclusions
LES DOMMAGES CAUSÉS OU SUBIS PAR :
     le véhicule, son contenu, son conducteur et ses 
ayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Toutefois, si 
l’aide consiste en une opération de remorquage, 
la garantie n’est acquise que si le remorquage 
est effectué en conformité avec les dispositions 
légales ou réglementaires  ;
 • le remboursement des frais de nettoyage et remise  
en état des garnitures intérieures du véhicule 
assuré, des vêtements du conducteur et des pas-
sagers à la suite du transport bénévole d’urgence 
d’un blessé de la route.
OUTRE LES EXCLUSIONS PRÉVUES  
AU CHAPITRE 5 « CE QUE LE CONTRAT  
NE GARANTIT JAMAIS  », NOTRE GARANTIE 
NE S’APPLIQUE PAS  :
 • lorsque vous êtes mis en cause pour des 
dommages dus à l’amiante ou à tout matériau 
contenant de l’amiante sous quelque forme  
et en quelque quantité que ce soit,
 • aux dommages résultants, directement 
ou indirectement, d’une contamination 
biologique ou chimique provoquée par un acte 
de terrorisme de quelque nature qu’il soit.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-
plaining and harnessing adversarial examples , arXiv preprint
arXiv:1412.6572, (2014).
[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />C H A P T E R 7
Interpretability and
Explainability
Techniques for
Transformers
Incritical applications in domains like healthcare, legislation, law en-
forcement, or ﬁnancial, in addition to the predictions, there is a need
to understand the models from an explainability standpoint. Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />C H A P T E R 7
Interpretability and
Explainability
Techniques for
Transformers
Incritical applications in domains like healthcare, legislation, law en-
forcement, or ﬁnancial, in addition to the predictions, there is a need
to understand the models from an explainability standpoint. Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>10. How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The refund calculation for the unused portion of the premium or contribution during the period when the risk did not occur can vary based on the terms and conditions of the insurance policy or agreement. Typically, it is calculated on a pro-rata basis, meaning the refund amount is proportional to the unused period of coverage. For instance, if the risk did not occur for half of the coverage period, then half of the premium or contribution may be refunded. However, some policies may have specific conditions or fees that could affect the refund amount. It's always recommended to review the policy documents or consult with the insurance provider for accurate information.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The refund for the portion of the premium or contribution not used during the period when the risk did not occur is typically calculated based on the insurance company's refund policy or the terms of the contribution agreement. 

Here are some common methods used to calculate refunds in such situations:

1. Pro-rata Refund: This method involves calculating the unused portion of the premium or contribution on a pro-rata basis. The refund amount is determined by dividing the total premium or contribution by the total coverage period and then multiplying that by the number of days the risk did not occur.

2. Short-Rate Cancellation: In some cases, insurance policies or contribution agreements may have a short-rate cancellation provision. This means that if the policyholder cancels before the end of the coverage period, the refund amount will be less than the pro-rata amount to account for administrative costs or fees.

3. Flat Fee Deduction: Some insurance policies or contribution agreements may have a flat fee deduction for cancellations or early terminations. In this case, the refund amount is reduced by a predetermined flat fee.

It's important to review the specific terms and conditions of the insurance policy or contribution agreement to understand how refunds are calculated in the event that the risk does not occur during the coverage period.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />(5.53)
When the kernel functions φ(x)are as deﬁned in [53], the attention
weightsAij= exp(
qikT
j)
can be approximated by
exp(qikT
j) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)
where Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-
dimensional standard normal distribution. If ωin (5.54) is replaced with√
dω
∥ω∥, thenωis any point on the surface of the dk-dimensional sphere
of radius√dk.
This kernel approximation of the softmax calculation reduces the
quadratic complexity to one that is nearly linear in sequence length, and
the approximation error can be decreased by periodically resampling ω.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />(5.53)
When the kernel functions φ(x)are as deﬁned in [53], the attention
weightsAij= exp(
qikT
j)
can be approximated by
exp(qikT
j) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)
where Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-
dimensional standard normal distribution. If ωin (5.54) is replaced with√
dω
∥ω∥, thenωis any point on the surface of the dk-dimensional sphere
of radius√dk.
This kernel approximation of the softmax calculation reduces the
quadratic complexity to one that is nearly linear in sequence length, and
the approximation error can be decreased by periodically resampling ω.
5.2.2.4 Big Bird
Big Bird is another eﬀort to give the Transformer a sparse attention
mechanism that allows for linear scaling.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />If ωin (5.54) is replaced with√
dω
∥ω∥, thenωis any point on the surface of the dk-dimensional sphere
of radius√dk.
This kernel approximation of the softmax calculation reduces the
quadratic complexity to one that is nearly linear in sequence length, and
the approximation error can be decreased by periodically resampling ω.
5.2.2.4 Big Bird
Big Bird is another eﬀort to give the Transformer a sparse attention
mechanism that allows for linear scaling. It is also proven to be Turing
complete and a universal sequence function approximator [292].
An Attention mechanism is a directed graph Big Bird describes
the attention mechanism as a directed graph. The vertices represent the
Lpositions in the input sequence. The directed edges represent the pre-
softmax attention weights. In other words, the directed edges are the
inner products between the query and key vectors, where the ithvertex</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />xviii■Foreword
focus on AI explainability, an increasingly important theme in practice
imposed by greater focus on issues such as ethical AI and trustable AI.
– Dr. Yogesh Malhotra
Founding Chairman and CEO
U.S. Venture Capital and Private Equity Firm
Global Risk Management Network LLC
scientist
www.yogeshmalhotra.com</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Finally, the ﬁne-tuned model is then used to perform inference
on the same task from multiple diﬀerent target languages. Transformers
has shown remarkable progress in cross-lingual transfer learning, espe-
cially when it comes to zero-shot learning. The details of how trans-
formers perform cross-lingual transfer learning is still an active research
area. This section discusses the active areas of research to understand
the main factors aﬀecting cross-lingual transfer learning [74]. The dis-
cussed factors include: (i) data related factors, (ii) model architecture
factors, and (iii) training tasks factors.
4.3.1.1 Data factors
Shared Vocabulary Prior to training a mLM, text from all languages
is tokenized using a WordPiece [220] or SentencePiece [144] models. The
main concept is to tokenize each word into its most often occurring sub-
words.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization. Pre-training tasks has
also been examined as a possible confounding factor, it was shown that
models trained with parallel corpora pre-training tasks such as XLM re-
tain language-agnostic representations within the higher encoder layers
in a stronger manner than models trained on monolingual corpora such
as mBERT and XLMR [52].
Parallel Corpora Representations Another way to approach the
language-agnostic representation hypothesis is to use parallel corpora in
diﬀerent languages and examine their parallel representations for align-
ment in the model embedding space.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Multilingual transformers
is certainly no exception but even more intriguing as they appear to use
and share knowledge across multiple languages; i.e., cross-lingual trans-
fer learning. This raises the question if multilingual transformers learn
language-agnostic cross-lingual representations. Several approaches have
been proposed to answer this question, each with a diﬀerent angle of at-
tack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora
Representations.
Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This raises the question if multilingual transformers learn
language-agnostic cross-lingual representations. Several approaches have
been proposed to answer this question, each with a diﬀerent angle of at-
tack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora
Representations.
Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This is
despite the fact that it does not use parallel corpora for pre-training.
XLM-R follows the architectural design of XLM without using the lan-
guage embeddings which allows for better handling of code-switching.
The main contributions of XLM-R are achieving state-of-the-art perfor-
mance with monolingual data and proving deep insights into the value
of cleaner and larger training data, the size of the vocabulary, as well as
the right level of hyperparameters tuning.
4.1.2.4 Alternating Language Model (ALM)
Alternating Language Model (ALM) ALM [282] is a pre-training
task for cross-lingual representations that works in the same way as the
TLMpre-trainingtaskexceptforthenatureoftheinputdata.TLMuses
parallel sentences for input, however ALM uses code-switched sentences
generated from parallel sentences. In code-switched sentences, some</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization. Pre-training tasks has
also been examined as a possible confounding factor, it was shown that
models trained with parallel corpora pre-training tasks such as XLM re-
tain language-agnostic representations within the higher encoder layers
in a stronger manner than models trained on monolingual corpora such
as mBERT and XLMR [52].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi. The importance of the feature xi
is measured using:
Diﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)
7.3.2 Model Distillation
Xie et al. refer to the model distillation category as a post-training
method where the encoded knowledge in the model is distilled into a rep-
resentation agreeable for an explanation by a user, as shown in Fig. 7.3.
Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice of
the reference input x′is domain speciﬁc. For example, in MNIST
classiﬁcation, input with all zeros representing the common back-
ground can be one of the choices for the reference input.
∆t=f(x)−f(x′)is the diﬀerence in neuron output between input
xand a reference input x′. The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice of
the reference input x′is domain speciﬁc. For example, in MNIST
classiﬁcation, input with all zeros representing the common back-
ground can be one of the choices for the reference input.
∆t=f(x)−f(x′)is the diﬀerence in neuron output between input
xand a reference input x′. The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■195
•Prediction Diﬀerence Analysis: Zintgraf et al. propose a
method based on Robnik-Sikonja and Kononenko research, to
measure the inﬂuence based on altering input information for
probabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi. The importance of the feature xi
is measured using:
Diﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)
7.3.2 Model Distillation
Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■195
•Prediction Diﬀerence Analysis: Zintgraf et al. propose a
method based on Robnik-Sikonja and Kononenko research, to
measure the inﬂuence based on altering input information for
probabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice for
the root point is such that for it to be insigniﬁcantly diﬀerent from
the input xfor whichfgenerates a diﬀerent prediction [187].
f(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)
f(x) =N
i∂f
∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)
whereϵsums up all second order and higher terms in the Taylor
expansion. The relevance score for the inputs can be derived from
the above equation as
ri=∂f
∂xi( ˆxi)·(xi−ˆxi) (7.3)
For a deep network with multiple layers, the Deep Taylor Decom-
position assumes decomposition of the relevance scores, starting
from the output to the input through the intermediate layers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The loss function Lprovides a metric of how close the explanation
model approximates the real model fin the locality of instance x.
•SHapley Additive exPlanations (SHAP): Shapley values
have foundations in game theory, particularly cooperative game
theory. One can view each feature as a player in a game, and the
goal is to distribute payouts fairly among players who form a coali-
tion based on their contribution to the total payout. The marginal
contribution of each feature averaged across the set of all possible
coalitions of features gives the Shapley values.
Let us represent the entire deep learning model through function
f, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj
for thej-th feature is given by
φj(f) =|S|!(d−|S|−1)!
d![f(S∪{xj})−f(S)](7.12)</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT is pre-trained with a 15% mask-out rate. This simple MLM task
has a downside where a mismatch is created between pre-training and
ﬁne-tuning tasks as the special [MASK]token does not appear during
ﬁne-tuning. To overcome this issue, every token in the 15% masked-out
tokens is subjected to the following heuristic:
•With a probability of 80%, the token is replaced with the special
[MASK]token.
•With a probability of 10%, the token is replaced with a random
token.
•With a probability of 10%, the token is left unchanged.
The occasional insertion of random tokens (i.e., noise) pushes BERT to
be less biased towards the masked token, especially when the masked
token is left unchanged, in its bidirectional context attention. The MLM
task uses cross-entropy loss only over the masked tokens and ignores the
prediction of all non-masked ones.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>11. Define "Préjudice d’affection" as described in the document.</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The document does not provide a specific definition for "Préjudice d’affection". However, it mentions that in the event of death, compensation for "Préjudice d’affection" is provided to the spouse and children of the deceased insured, or in their absence, to the deceased's father and/or mother. The maximum compensation is set at 50,000 €, with a limit of 10,000 € per beneficiary.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Le "préjudice d’affection" tel que décrit dans le document fait référence à une indemnisation accordée aux proches de l'assuré décédé en raison de la perte affective subie suite au décès de l'assuré. Cette indemnisation est destinée au conjoint, aux enfants, et en l'absence de ceux-ci, aux parents de l'assuré décédé. Il s'agit d'une compensation financière pour le préjudice émotionnel et affectif causé par la perte de l'être cher.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) 
 
(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pertes de gains 
professionnels 
futurs (article 27 -
2 E) Plafond  : 250  000 € 
EN CAS DE D ÉCÈS  
Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) 
 
(5) Le</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />- pour le préjudice d’affection des proches :  
› au conjoint  et aux enfants de l’assuré décédé,  
› à défaut au père et/ou à la mère de l’assuré décédé,  
- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  
› au conjoint  de l’assuré décédé,  
› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au 
jour de l’accident , 
› aux personnes dont l’assuré ou son conjoint a la tutelle ou la curatelle et qui sont économiquement à la charge  de l’assuré 
décédé au jour de l’accident .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />• en cas de décès :  
- pour la participation aux frais d’obsèques : à la personne ayant exposé les frais,  
- pour le préjudice d’affection des proches :  
› au conjoint  et aux enfants de l’assuré décédé,  
› à défaut au père et/ou à la mère de l’assuré décédé,  
- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  
› au conjoint  de l’assuré décédé,  
› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au 
jour de l’accident ,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />supérieur à 65 % (seuil de déclenchement)  
Pertes de gains 
professionnels 
futurs (article 27 -
2 E) Plafond  : 250  000 € 
EN CAS DE D ÉCÈS  
Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />(article 27 -
2 E) Plafond  : 250  000 € 
EN CAS DE D ÉCÈS  
Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) 
 
(5) Le  plafond est majoré de 50 % dans les situations visées au</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />: 250  000 € 
EN CAS DE D ÉCÈS  
Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) 
 
(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />),  
 le préjudice esthétique permanent  (P.E .P.), 
 le préjudice d’agrément  (P.A.). 
 les frais  de logement  adapté s, 
 les frais de  véhicu le adaptés  
 
En cas de décès :  
 
 les pertes de revenus des ayants droit  consécutives au décès  du conducteur, que ce décès su rvienne imm édiatement 
ou dans le délai d ’un a n des suites de l’accident garan ti (P.R.),  
 le préjudice d’affection  (P.A.F.), 
 les frais d’obsè ques (F. O.).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />taux d’incapacité permanente  supérieur à 65 % (seuil de déclenchement)  
Pertes de gains 
professionnels 
futurs (article 27 -
2 E) Plafond  : 250  000 € 
EN CAS DE D ÉCÈS  
Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Participation aux 
frais d’obsèques 
(article 27 -3 A) Plafond  : 5 000 € 
Préjudice 
d’affection  
(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  
Pertes de revenus 
des proches  
(article 27-3 C) Plafond  : 940 000 €  
Services à la 
personne (article 
27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) 
 
(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. 
Prestations 
d’accompagnement 
personnalisé  
(article 27 -3 E) Plafond  : 5 000 €, soit 250 unités de valeur </td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[146]G. Lample and A. Conneau ,Cross-lingual language model
pretraining , Advances in Neural Information Processing Systems
(NeurIPS), (2019).
[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />228■Bibliography
Tetreault, eds., Association for Computational Linguistics, 2020,
pp. 8440–8451.
[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,
and M. Baroni ,What you can cram into a single vector: Prob-
ing sentence embeddings for linguistic properties , arXiv preprint
arXiv:1805.01070, (2018).
[63]A. Conneau and G. Lample ,Cross-lingual language model
pretraining , in Advances in Neural Information Processing Sys-
tems32:AnnualConferenceonNeuralInformationProcessingSys-
tems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,
Canada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-
Buc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />8440–8451.
[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,
and M. Baroni ,What you can cram into a single vector: Prob-
ing sentence embeddings for linguistic properties , arXiv preprint
arXiv:1805.01070, (2018).
[63]A. Conneau and G. Lample ,Cross-lingual language model
pretraining , in Advances in Neural Information Processing Sys-
tems32:AnnualConferenceonNeuralInformationProcessingSys-
tems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,
Canada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-
Buc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.
[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.
Bowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating
cross-lingual sentence representations , in Proceedings of the 2018
Conference on Empirical Methods in Natural Language Process-
ing, Association for Computational Linguistics, 2018.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[154]G. Letarte, F. Paradis, P. Giguère, and F. Laviolette ,
Importance of self-attention for sentiment analysis , in Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-
terpreting Neural Networks for NLP, 2018, pp. 267–275.
[155]M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer ,
BART: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension , in Proceed-
ings of the 58th Annual Meeting of the Association for Compu-
tational Linguistics, Online, July 2020, Association for Computa-
tional Linguistics, pp. 7871–7880.
[156]P. S. H. Lewis, B. Oguz, R. Rinott, S. Riedel, and
H. Schwenk ,MLQA: evaluating cross-lingual extractive question
answering , in Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2020, Online, July
5–10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault,
eds., Association for Computational Linguistics, 2020, pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■37
(a) Example showing the En-
glish word “going” pays atten-
tion to “je” and “vais”.
(b) Example showing the En-
glishword“be”paysattentionto
“en” and “etre”.
(c) English word “library” pays
attention to multiple words.
(d) The words “two” and “deux”
are matched.
Figure 2.17 Attention examples and plots.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />337–346.
[5]D. Amodei, C. Olah, J. Steinhardt, P. Christiano,
J. Schulman, and D. Mané ,Concrete problems in ai safety ,
arXiv preprint arXiv:1606.06565, (2016).
[6]A. Andoni, P. Indyk, T. Laarhoven, I. P. Razenshteyn,
and L. Schmidt ,Practical and optimal lsh for angular distance ,
in NIPS, 2015.
[7]D. Araci ,Finbert: Financial sentiment analysis with pre-trained
language models , ArXiv, abs/1908.10063 (2019).
[8]E. Arkhangelskaia and S. Dutta ,Whatcha lookin’at?
deeplifting bert’s attention in question answering , arXiv preprint
arXiv:1910.06431, (2019).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization. Pre-training tasks has
also been examined as a possible confounding factor, it was shown that
models trained with parallel corpora pre-training tasks such as XLM re-
tain language-agnostic representations within the higher encoder layers
in a stronger manner than models trained on monolingual corpora such
as mBERT and XLMR [52].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[75]W. B. Dolan and C. Brockett ,Automatically constructing a
corpus of sentential paraphrases , in IJCNLP, 2005.
[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,
Boosting adversarial attacks with momentum ,inProceedingsofthe
IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 9185–9193.
[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-
pretability of deep neural networks with semantic information , in
Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017, pp. 4306–4314.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>12. Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the circumstances under which insurance covers occasional and voluntary towing or assistance of a vehicle.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Insurance coverage for occasional and voluntary towing or assistance of a vehicle can vary depending on the specific insurance policy. In general, insurance policies may provide coverage for towing or assistance in the following circumstances:

1. **Roadside Assistance**: Many insurance policies offer roadside assistance coverage, which can include services like towing, jump-starts, tire changes, and fuel delivery in case of a breakdown or mechanical failure.

2. **Accidents**: If your vehicle is involved in an accident and needs to be towed to a repair shop, the towing costs may be covered by your insurance policy.

3. **Mechanical Breakdown**: If your vehicle experiences a mechanical breakdown and needs to be towed to a repair facility, the towing costs may be covered by your insurance.

4. **Stuck in a Ditch or Snow**: If your vehicle gets stuck in a ditch, snow, or mud and requires towing assistance to get back on the road, some insurance policies may cover the towing costs.

5. **Voluntary Towing**: Some insurance policies may also cover voluntary towing, which means you can request towing services even if your vehicle is not in an emergency situation. This could include towing to a repair shop, storage facility, or another location.

It's important to review your insurance policy or contact your insurance provider to understand the specific coverage and limitations related to towing and assistance services for your vehicle.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253]. The capacity of the BERT architecture is characterized by (i) the
DOI: 10.1201/9781003170082-3 43</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The bidirectional
attention of the transformer encoder forces the [MASK]prediction task to
usethecontextprovidedbytheothernon-maskedtokensinthesequence.
BERT is pre-trained with a 15% mask-out rate. This simple MLM task
has a downside where a mismatch is created between pre-training and
ﬁne-tuning tasks as the special [MASK]token does not appear during
ﬁne-tuning. To overcome this issue, every token in the 15% masked-out
tokens is subjected to the following heuristic:
•With a probability of 80%, the token is replaced with the special
[MASK]token.
•With a probability of 10%, the token is replaced with a random
token.
•With a probability of 10%, the token is left unchanged.
The occasional insertion of random tokens (i.e., noise) pushes BERT to
be less biased towards the masked token, especially when the masked
token is left unchanged, in its bidirectional context attention.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].
7.1 TRAITS OF EXPLAINABLE SYSTEMS
One of the decisive goals of an explainable system is that it allows the
system’s end-users to understand the relationship between the input and
the output.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT is pre-trained with a 15% mask-out rate. This simple MLM task
has a downside where a mismatch is created between pre-training and
ﬁne-tuning tasks as the special [MASK]token does not appear during
ﬁne-tuning. To overcome this issue, every token in the 15% masked-out
tokens is subjected to the following heuristic:
•With a probability of 80%, the token is replaced with the special
[MASK]token.
•With a probability of 10%, the token is replaced with a random
token.
•With a probability of 10%, the token is left unchanged.
The occasional insertion of random tokens (i.e., noise) pushes BERT to
be less biased towards the masked token, especially when the masked
token is left unchanged, in its bidirectional context attention. The MLM
task uses cross-entropy loss only over the masked tokens and ignores the
prediction of all non-masked ones.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT uses unlabeled text
to pre-train deep bidirectional contextual representations. This resulted
in rich pre-trained language models that can be ﬁne-tuned with a simple
additional output layer and a reasonably sized labeled dataset to pro-
duce state-of-the-art performance in a broad range of NLP tasks. These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />C H A P T E R 7
Interpretability and
Explainability
Techniques for
Transformers
Incritical applications in domains like healthcare, legislation, law en-
forcement, or ﬁnancial, in addition to the predictions, there is a need
to understand the models from an explainability standpoint. Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The “father of American psychology”, William James, created a two-
component framework to explain the visual attention mechanism [133].
In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The “father of American psychology”, William James, created a two-
component framework to explain the visual attention mechanism [133].
In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Therefore, the attention mechanism has allowed hu-
mans to focus on only a fraction of information of interest, thus enabling
optimum resource usage, leading to better survival and growth.
The “father of American psychology”, William James, created a two-
component framework to explain the visual attention mechanism [133].
In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig. 2.3, an attention mechanism can be considered as
a memory with keys and values and a layer which, when someone queries
it, generates an output from value whose keys map the input [17].
To formalize, let us consider the memory unit consisting of nkey-
value pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.
In the context of attention mechanisms in deep learning, volitional
cues map to queries, keys to nonvolitional cues, and sensory inputs to
value. Every sensory input (value) maps to the nonvolitional cue (key)
of that sensory input. Attention mechanisms can be thus considered as
a process of biasing selection over values (sensory inputs) via attention
pooling, using the queries (volitional cues) and keys (nonvolitional cues)
as shown in Fig. 2.3.
The attention mechanism is designed in a way to overcome the issues
described with RNN-based encoder-decoder architecture.
As shown in Fig. 2.3, an attention mechanism can be considered as
a memory with keys and values and a layer which, when someone queries
it, generates an output from value whose keys map the input [17].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !
Festivals .Fun .Beer .Lots ofbeer .Charlotte Center City
Partners (orFind Your Center )puts onalot ofthese types
offestivals Uptown and inSouth End .When you check out
their website ortheir weekly newsletter you ’llbeable to
seelots ofevents coming uplike Beer ,Blues and BBQ ,
Taste ofCharlotte ,Speedstreet and the like .\n\nMany of
these events and festivals usually have beer available ,
hence why I’mafan .And ,yeah ,Ialso really like
supporting the local organization that ’sresponsible for
Uptown ’sdevelopment .Ifonly there was aPBR festival ...!</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>13. What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Maintaining insurance coverage during test drives for a vehicle being prepared for sale is crucial to protect both the seller and potential buyers. Here are some common requirements:

1. **Existing Insurance Policy**: The vehicle should be covered under the seller's existing insurance policy. It's important to inform the insurance company that the vehicle is being used for test drives during the selling process.

2. **Liability Coverage**: The insurance policy should include liability coverage to protect against any accidents or damages that may occur during test drives. This coverage typically helps pay for the other party's medical expenses or property damage if the seller is found at fault.

3. **Comprehensive and Collision Coverage**: Comprehensive coverage protects against non-collision incidents like theft or vandalism, while collision coverage helps pay for damages to the vehicle in case of an accident during a test drive.

4. **Permission for Test Drives**: Some insurance policies may require explicit permission for test drives. Make sure to check with your insurance provider if this is a requirement.

5. **Driver Requirements**: Ensure that anyone test driving the vehicle meets the driver requirements specified in your insurance policy. This may include having a valid driver's license and meeting age restrictions.

6. **Notification of Changes**: Inform your insurance company about any changes in the vehicle's status, such as it being used for test drives. Failure to disclose this information could lead to coverage issues.

7. **Temporary Coverage**: If necessary, you may need to obtain temporary coverage or a special policy for test drives if your existing policy does not adequately cover this activity.

It's essential to review your insurance policy and discuss any specific requirements or concerns with your insurance provider to ensure that you have appropriate coverage during test drives for a vehicle being prepared for sale.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />(CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.
ISBN: 978-0-367-77165-2 (hbk)
ISBN: 978-0-367-76734-1 (pbk)
 
ISBN: 978-1-003-17008-2 (ebk)
DOI: 10.1201/9781003170082
Typeset in Latin Modern  font 
by KnowledgeWorks Global Ltd.
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data
Names: Kamath, Uday, author. 
Title: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.
ISBN: 978-0-367-77165-2 (hbk)
ISBN: 978-0-367-76734-1 (pbk)
 
ISBN: 978-1-003-17008-2 (ebk)
DOI: 10.1201/9781003170082
Typeset in Latin Modern  font 
by KnowledgeWorks Global Ltd.
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data
Names: Kamath, Uday, author.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Multilingual Transformer Architectures ■85
XLCO is sentence-based, HICTL provides both sentence and word-level
cross-lingual representations.
Sentence level representations are constructed in the same manner
as XCLO except for the negative sampling where instead of collecting
samples from{yj}N
j=1,j̸=ismoothed linear interpolation [31, 300] between
sentence representations is used to construct hard negative samples.
For world-level representations, the contrastive loss similarity score
is computed between a parallel sentence (x i,yi)[CLS]token represen-
tation and other words representations. For each parallel sentence pair
input (xi,yi), a bag of wordsWis maintained, where all words in Ware
considered positive samples and all other words in the vocabulary are
negative samples. For eﬃcient negative words sampling, HICTL does
not sample from the entire vocabulary but rather constructs a set of
negative words that very close to parallel sentence ( xi,yi)[CLS]token
representation.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />i.e. the resulting rule or anchor, such that A(x) = 1 ,f
represents the black-box model, Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way. Beam-search
can be used to improve upon this greedy search method by main-
taining a candidate set of rules during the iterative search process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Sentence level representations are constructed in the same manner
as XCLO except for the negative sampling where instead of collecting
samples from{yj}N
j=1,j̸=ismoothed linear interpolation [31, 300] between
sentence representations is used to construct hard negative samples.
For world-level representations, the contrastive loss similarity score
is computed between a parallel sentence (x i,yi)[CLS]token represen-
tation and other words representations. For each parallel sentence pair
input (xi,yi), a bag of wordsWis maintained, where all words in Ware
considered positive samples and all other words in the vocabulary are
negative samples. For eﬃcient negative words sampling, HICTL does
not sample from the entire vocabulary but rather constructs a set of
negative words that very close to parallel sentence ( xi,yi)[CLS]token
representation.
4.1.3 Dual-Encoder Multilingual NLU
4.1.3.1 Language-agnostic BERT Sentence Embedding (LaBSE)
Language-agnostic BERT Sentence Embedding (LaBSE) [88] is an ar-
chitecture for training cross-lingual sentence representations which com-
bines Masked Language Model (MLM) and Translation Language Model
(TLM) pre-training tasks from XLM [146] with a translation ranking
task using bi-directional dual-encoders with additive margin softmax
loss[283].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors lay the following three requirements for faithful expla-
nations for attention mechanisms.
1. Attention mechanism should be a NECESSARY component for
good model performance.
2. Attention distributions should be hard to manipulate, i.e., if any
trained model can vary the distribution of attention weights and
yet have similar predictions, they may not be suitable for the ex-
planation. This directly corresponds to the exclusivity requisite
for faithful explanations and will guide the search for adversarial
models.
3. Attention distributions should work well in uncontextualized set-
tings. Since the attention weights are typically learned on contex-
tualized hidden layer outputs, to see the impact on input tokens,
one needs to use uncontextualized settings to judge their useful-
ness.
7.4.2.1 Is attention necessary for all tasks?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors lay the following three requirements for faithful expla-
nations for attention mechanisms.
1. Attention mechanism should be a NECESSARY component for
good model performance.
2. Attention distributions should be hard to manipulate, i.e., if any
trained model can vary the distribution of attention weights and
yet have similar predictions, they may not be suitable for the ex-
planation. This directly corresponds to the exclusivity requisite
for faithful explanations and will guide the search for adversarial
models.
3. Attention distributions should work well in uncontextualized set-
tings. Since the attention weights are typically learned on contex-
tualized hidden layer outputs, to see the impact on input tokens,
one needs to use uncontextualized settings to judge their useful-
ness.
7.4.2.1 Is attention necessary for all tasks?
The authors use the same three sets of tasks and six classiﬁcation
datasets using the BiLSTM model from the Jain and Wallace setting
and create another model where the attention weights are uniform in
distribution compared to the learned weights.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>14. What is the special deductible for the "Formules Tiers Eco" and "Tous Risques Eco" in case a driver is not designated?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The special deductible for the "Formules Tiers Eco" and "Tous Risques Eco" when a driver has not been designated applies to the civil liability and all accident damage Eco guarantees. It is applied primarily to the all accident damage Eco guarantee when it has been subscribed. The exact amount of the deductible is indicated in the specific conditions of the insurance contract.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          In the "Formules Tiers Eco" and "Tous Risques Eco" insurance formulas, if a driver is not designated and multiple guarantees are involved in a single claim, the insured will bear the corresponding deductibles. Specifically, the special deductible applies to the following guarantees:

- Mechanical breakdown
- Damage to bicycles

This deductible affects the Liability and All Accidents Damage Eco guarantees. It is prioritized on the All Accidents Damage Eco guarantee if it was subscribed to.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Formules Tiers Eco et Tous Risques Eco : franchise 
spéciale lorsqu’un conducteur n’a pas été désigné,
  PANNE MÉCANIQUE, 
  DOMMAGES VÉLO*.
Lorsque plusieurs de ces garanties sont mises en jeu à 
l’occasion d’un même sinistre*, l’assuré* conserve à sa charge les franchises correspondantes.
Si le responsable des dommages n’est pas une personne 
couverte par la garantie Responsabilité civile, nous lui réclamerons le remboursement du montant de la franchise* proportionnellement à sa responsabilité.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Cette franchise affecte les garanties Responsabilité civile et 
Dommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.
    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +
Conduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement 
conduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.
Cette franchise affecte les garanties Responsabilité civile et 
Dommages tous accidents.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.
    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +
Conduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement 
conduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.
Cette franchise affecte les garanties Responsabilité civile et 
Dommages tous accidents. Elle s’applique en priorité sur la garantie Dommages tous accidents lorsqu’elle a été souscrite.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />15.4 Franchise
 Lorsque mention en est faite sur vos Conditions particulières l’assuré* conserve à sa charge une franchise* pour tout sinistre* mettant en jeu les garanties :
  DOMMAGES AU VÉHICULE ASSURÉ,
  BRIS DE PARE-BRISE,
  BRIS DE GLACES sauf pour la réparation, sans 
remplacement du pare-brise,
  INDEMNISA TION RENFORCÉE, pour les formules T ous 
Risques Confort et T ous Risques Confort +,
  RESPONSABILITÉ CIVILE
Formules Tiers Essentiel et Essentiel + et formules 
Tous Risques Confort et Confort + : franchise spéciale lorsqu’un conducteur novice n’a pas été désigné.
Formules Tiers Eco et Tous Risques Eco : franchise 
spéciale lorsqu’un conducteur n’a pas été désigné,
  PANNE MÉCANIQUE, 
  DOMMAGES VÉLO*.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3   T ABLEAU DES FORMULES DE GARANTIES
PagesTIERS
ECOTIERS
ESSENTIELTIERS
ESSENTIEL+TOUS 
RISQUES
ECOTOUS 
RISQUES
CONFORTTOUS 
RISQUES
CONFORT+
LES GARANTIES 
Responsabilité civile/ Défense 11
Recours 14
Assistance aux personnes 26
Dommages corporels du conducteur Eco 17
Dommages corporels du conducteur 19
Catastrophes naturelles 21
Événements climatiques 21
Bris de pare-brise 21
Bris de glaces 21
Incendie, Explosion,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />17.3  Dispositions particulières
    FORMULE TIERS Eco ET FORMULE TOUS RISQUES Eco
Prêt de votre véhicule (prêt de volant*) :  lorsque le véhicule 
assuré est conduit par une personne non désignée sur les Conditions particulières, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières ; sauf s’il s’agit d’un associé ou d’un préposé* du souscripteur agissant exclusivement dans le cadre de l’activité professionnelle.
Cette franchise affecte les garanties Responsabilité civile et 
Dommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To that end, BERT’s input is designed to
distinctly represent NLP downstream tasks involving a single or a pair
of sentences using the same input representation design. Most major
NLP downstream tasks can be covered using a single sentence (e.g., text
classiﬁcation, sequence tagging, summarization, etc.) or paired sentences
(e.g., question answering, natural language inference, etc.).
For any input sequence, BERT preﬁxes a special [CLS]token. The
hidden vector of this token in the last BERT layer will be used as an
aggregate representation for the entire input sequence and usually used
for classiﬁcation tasks. For NLP tasks with paired sentences, BERT con-
catenates the sentences into one sequence with a separator token [SEP]
in between, which serves as one way BERT uses to distinguish the two
sentences. BERT also uses a learned segment embedding to indicate
to which sentence a token belongs.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />QUI EST LE BÉNÉFICIAIRE DE L’INDEMNITÉ ? 
Le propriétaire du véhicule assuré.
 DISPOSITIONS PARTICULIÈRES 
     VALEUR D’ACHA T 12 MOIS POUR LES FORMULES TIERS ESSENTIEL ET ESSENTIEL + ET LES FORMULES TOUS RISQUES CONFORT ET CONFORT +
Si le sinistre* survient dans les 12 mois suivant la date de 
première mise en circulation du véhicule,
-  lorsque celui-ci est détruit (le montant des réparations 
dépasse la valeur de remplacement*),
-  ou lorsqu’il est volé et non retrouvé,l’indemnité est égale à la valeur d’acquisition* du véhicule, 
sur présentation de la facture d’achat. GARANTIES VOL Eco ET VOL
-  Si votre véhicule est retrouvé dans un délai de 20 jours à 
compter de la réception de votre déclaration, vous vous engagez à en reprendre possession et l’indemnité que nous verserons, sera égale à celle indiquée ci-dessus augmentée des frais que vous aurez engagés en accord avec nous pour la récupération du véhicule.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />FORMULES DE GARANTIES ET OPTIONS    
TIERS  TIERS - 
VOL - 
INCENDIE  TOUS RISQUES  TOUS RISQUES 
PLUS (1) 
GARANTIES MOBILITÉ  
Assistance au véhicule et aux 
personnes transportées  23 ● ● ● ● 
Assistance panne  0 kilomètre /  
rétention administrative du 
permis  de conduire  24  OPTION  OPTION   OPTION  ●  
Véhicule de 
remplacement
(5)(6) Niveau 1  
25 OPTION  OPTION  OPTION   
Niveau 2    OPTION  ● 
Panne mécanique(5)(7) 26   OPTION  ● 
 
(1) Formule de  garanties</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />44■Transformers for Machine Learning: A Deep Dive
number of transformer layers L, (ii) the size of hidden representations
H, and the number of bidirectional self-attention heads A.
Input and output representations Given the simplicity of the
BERT’s core architecture, just a stack of bidirectional transformer en-
coders, the ingenuity of BERT lies in the design of input and output
representations that can be used to train many downstream NLP tasks
with the same architecture. To that end, BERT’s input is designed to
distinctly represent NLP downstream tasks involving a single or a pair
of sentences using the same input representation design. Most major
NLP downstream tasks can be covered using a single sentence (e.g., text
classiﬁcation, sequence tagging, summarization, etc.) or paired sentences
(e.g., question answering, natural language inference, etc.).
For any input sequence, BERT preﬁxes a special [CLS]token.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■205
Jensen-Shannon Divergence is used to measure the diﬀerence between
the distributions. The optimization equation is given by:
maximizeα(1),···,α(k)f({α(i)}k
i=1) (7.20)
wheref({α(i)}k
i=1)is:
k
i=1JSD[α(i),ˆα] +1
k(k−1)k
i<jJSD[α(i),α(j)](7.21)
The ﬁrst part of the equation∑k
i=1JSD[α(i),ˆα]ﬁnds maximally
diﬀerent attention from the observed ˆαand the second part
1
k(k−1)∑k
i<jJSD[α(i),α(j)is maximally diﬀerent from each other.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice for
the root point is such that for it to be insigniﬁcantly diﬀerent from
the input xfor whichfgenerates a diﬀerent prediction [187].
f(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)
f(x) =N
i∂f
∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)
whereϵsums up all second order and higher terms in the Taylor
expansion. The relevance score for the inputs can be derived from
the above equation as
ri=∂f
∂xi( ˆxi)·(xi−ˆxi) (7.3)
For a deep network with multiple layers, the Deep Taylor Decom-
position assumes decomposition of the relevance scores, starting
from the output to the input through the intermediate layers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The superscript (0)denotes that this is the input
layer.
The Laplacian positional encodings λialso get embedded into a
d−dimensional space with an additional learnable embedding layer
C(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)
i:
λ(0)
i=C(0)λi+c(0)(6.18)
h(0)
i=˜h(0)
i+λ(0)
i (6.19)
Note that c(0)∈Rdis a bias term for the Laplacian positional embed-
ding, h(0)is the full node embedding, and Laplacian positional embed-
dings are only computed for the input layer and are not used inside
Transformer layers.
6.6.2.1 Graphs without edge attributes
There are two ways to structure a graph Transformer, depending on
whether the graph has edge attributes or not.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp. 599–604.
[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel ,Backpropagation
applied to handwritten zip code recognition , Neural Computation,
1 (1989), pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />128■Transformers for Machine Learning: A Deep Dive
whereSiis the set of key positions that query iattends to and Z(i,Si) =
∑
r∈Siexp(
qikT
r√dk(
is the softmax normalization term.
Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />j−logZ(i,S(r)
i)(
vj (5.39)
m(r)
i,j=⌋
}
∞ j /∈S(r)
i
105ifi=j
logNi,jotherwise(5.40)
Ni,j=⏐⏐⏐⏐{r′:j∈S(r′)
i}⏐⏐⏐⏐ (5.41)
Note that in the multi-round LSH case, Reformer modiﬁes the mask-
ing term to include (and downweight) the case where query at position
iattends to a key at position i. This case is added because, while the
standard transformer allows a position to attend to itself, this is unhelp-
ful when Q=K, as is the case in LSH attention.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In other words, the conditional probability P(sR
i|sI
i)
should be deﬁned as:
P(
sR
i|sI
i)
=eφ(sI
i,sR
i)
∑
sR
j∈SReφ(sR
i,sR
j)
φ(
sI
i,sR
j)
=gI(
sI
i)⊤gR(
sR
j)(4.17)
wheregIis the input sentence encoding function and gRis response
sentence encoding function, gIandgRmake up the dual-encoder. Both
gIandgRare implemented using transformers and are trained with the
log-likelihood loss function,˜P(sR
i|sI
i), for each task.
As illustrated in Fig. 4.7, mUSE uses a single shared encoder sup-
porting multiple downstream tasks. The training tasks include: QA pre-
diction task, NLI task, and translation ranking.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■179
Figure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two
time steps are shown. ER,ES, andEArepresent the combined embed-
ding layer positional encoding, and layer norm for the returns-to-go,
states, and actions, respectively. The layer after GPT-2 contains three
linear submodules for predicting the next element of the sequence. Next
state, action, and return-to-go are predicted using diﬀerent submodules.
These ﬁxed trajectories are used for autoregressive training. The pre-
viousKstepsarepassedtotheDecisionTransformer.Eachstepcontains
three “tokens”: the state, action, and return-to-go. Each type of token
has its own linear embedding layer, which is followed up by a Layer-
Norm.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>15. What is the guarantee included in all four formulas of guarantees?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the guarantee included in all four formulas of guarantees.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The guarantee included in all four formulas of guarantees is the assurance of trustworthiness in the transformer-based models. This trustworthiness is assessed through criteria such as satisfactory testing, experience, safety, and ethics, ensuring that the models can perform reliably, consistently, and ethically in various applications without compromising societal well-being.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />148■Transformers for Machine Learning: A Deep Dive
largest component of 5.75: p(x) = (p1,...,pN), call itpj, and is used to
computetheupdatedtokenrepresentation.Thus,thoughtheparameters
have increased four times because of four feed-forward networks, hard
routing guarantees that the computational cost remains the same. Thus
the switching feed-forward network enables the scaling of experts to any
number (bound by memory capacity) without increasing computational
cost and not needing any transfer helps in the sharding process.
Model parallelism shards the model across the devices (cores/ma-
chines) while consuming the same data in the batch, resulting in a large
modelwithslowerprocessingduetosequentialﬂow;thebottleneckintro-
duced by communication. On the other hand, data parallelism keeps the
model weights constant and shards the data across, allowing improved
computational speed but a lower capacity model due to size.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key
kjso that kj=qj
||qj||. To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key
kjso that kj=qj
||qj||. To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries
m=2L
Number of buckets(5.34)
Within each block, each position is allowed to attend to the others in
the block and to those in the preceding block.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Table 4.1 [74] provides a summary of
the impactful multilingual transformer models available at the time of
publishing this book.
4.1.1 Basic Multilingual Transformer
Multilingual transformer models are typically based on the mBERT ar-
chitecture [72]. In this section, we describe the basic components of a
multilingual transformer architecture.
Input Layer A series of tokens is provided as input to the multilin-
gual transformer. The token input is derived from a one-shot represen-
tation of a limited vocabulary, often a subword vocabulary. Typically,
this vocabulary is learned by concatenating monolingual data from sev-
eral languages using algorithms such as BPE [223], WordPiece [273], or
SentencePiece [144]. To guarantee that diverse languages and scripts are
well represented in the vocabulary, data may be sampled using exponen-
tial weighted smoothing or distinct vocabularies for clusters of languages
can be learned [55] by dividing the vocabulary size.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />188■Transformers for Machine Learning: A Deep Dive
that a user can evaluate and measure. As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />found that the model converged faster and general-
ized better when the language modeling objective from the unsupervised
phase, (6.1), was added to (6.2). So, the full objective was the weighted
sumL2(C) +λL1(C).
Formatting data for ﬁne-tuning Data for each of the four training tasks
is formatted diﬀerently:
•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-
eted with a start and an end token, so the input is formatted like
[⟨s⟩,text,⟨/s⟩].
•A natural language inference (NLI) instance has two parts, the
premise,p, and the hypothesis, h. Labels can be entailment, con-
tradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],
where $ is a delimiter token.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>16. What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The default coefficient used in the calculation of the premium for the insured, as mentioned in Article 1, is 1.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The default coefficient used in the calculation of the premium for the insured as mentioned in Article 1 is 1. This coefficient is applied to the prime de référence to determine the premium due by the insured at each annual renewal of the contract.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />5112 •  LE CADRE RÉGLEMENTAIRE  
DE VOTRE CONTRAT 
12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  
(BONUS-MALUS)
L’annexe de l’article A.121-1 du Code des assurances, dont le texte est 
reproduit ci-dessous concerne les modalités de calcul de votre coefficient de 
réduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.
Cette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure 
à 80 cm3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % 
de Bonus).
Si vous êtes entièrement responsable d’un sinistre* au cours 
de l’année N-1 :
au 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni 
Bonus ni Malus).
Si vous êtes entièrement responsable de 2 sinistres* au cours 
de l’année N-1:
au 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= 
1,25 (soit 25 % de Malus).
Les sinistres* pris en compte pour le calcul de votre coefficient 
au 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />5112 •  LE CADRE RÉGLEMENTAIRE  
DE VOTRE CONTRAT 
12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  
(BONUS-MALUS)
L’annexe de l’article A.121-1 du Code des assurances, dont le texte est 
reproduit ci-dessous concerne les modalités de calcul de votre coefficient de 
réduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.
Cette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure 
à 80 cm3.
Article 1 : réduction et majoration des primes
Lors de chaque échéance annuelle du contrat, la prime due par l’assuré  
est déterminée en multipliant le montant de la prime de référence, telle 
qu’elle est définie à l’article 2, par un coefficient dit « coefficient de 
réduction-majoration » fixé conformément aux articles 4 et 5 suivants.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />6819.  VOTRE COEFFICIENT BONUS/MALUS
 19.1  Signification du coefficient bonus/
malus
C’est l’instrument de mesure de votre Bonus ou de votre 
Malus.
      Lorsqu’il est inférieur à 1, il vous donne par différence 
votre taux de Bonus.
Ex. : coefficient 0,80 = 20 % de Bonus.
  Lorsqu’il est supérieur à 1, il vous donne par différence 
votre taux de Malus.
Ex. : coefficient 1,25 = 25 % de Malus.
Lors de la souscription de votre contrat il est mentionné 
sur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ex. : coefficient 1,25 = 25 % de Malus.
Lors de la souscription de votre contrat il est mentionné 
sur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.
Les conditions d’application et d’évolution de ce coefficient 
Bonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.
Par dérogation aux dispositions légales (Article A 121-1  
alinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).
 19.2    Comment évolue le coefficient bonus/
malus ?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % 
de Bonus).
Si vous êtes entièrement responsable d’un sinistre* au cours 
de l’année N-1 :
au 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni 
Bonus ni Malus).
Si vous êtes entièrement responsable de 2 sinistres* au cours 
de l’année N-1:
au 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= 
1,25 (soit 25 % de Malus).
Les sinistres* pris en compte pour le calcul de votre coefficient 
au 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1.
     si la responsabilité de l’assuré* n’est que partiellement 
engagée, la majoration est réduite de moitié : le coefficient est multiplié par 1,125.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Lorsqu’il est inférieur à 1, il vous donne par différence 
votre taux de Bonus.
Ex. : coefficient 0,80 = 20 % de Bonus.
  Lorsqu’il est supérieur à 1, il vous donne par différence 
votre taux de Malus.
Ex. : coefficient 1,25 = 25 % de Malus.
Lors de la souscription de votre contrat il est mentionné 
sur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.
Les conditions d’application et d’évolution de ce coefficient 
Bonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.
Par dérogation aux dispositions légales (Article A 121-1  
alinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.
Les conditions d’application et d’évolution de ce coefficient 
Bonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.
Par dérogation aux dispositions légales (Article A 121-1  
alinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).
 19.2    Comment évolue le coefficient bonus/
malus ?
   Dans ce présent paragraphe, N correspond à l’année en 
cours.
Le coefficient d’origine, c’est-à-dire celui affecté à un 
conducteur lors de sa première année d’assurance*, est égal à 1.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ex. : coefficient 1,25 = 25 % de Malus.
Lors de la souscription de votre contrat il est mentionné 
sur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.
Les conditions d’application et d’évolution de ce coefficient 
Bonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.
Par dérogation aux dispositions légales (Article A 121-1  
alinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).
 19.2    Comment évolue le coefficient bonus/
malus ?
   Dans ce présent paragraphe, N correspond à l’année en 
cours.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Lorsqu’il est inférieur à 1, il vous donne par différence 
votre taux de Bonus.
Ex. : coefficient 0,80 = 20 % de Bonus.
  Lorsqu’il est supérieur à 1, il vous donne par différence 
votre taux de Malus.
Ex. : coefficient 1,25 = 25 % de Malus.
Lors de la souscription de votre contrat il est mentionné 
sur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.
Les conditions d’application et d’évolution de ce coefficient 
Bonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Multilingual Transformer Architectures ■91
is obvious that the number of available articles in English is much higher
than other languages such as Persian or Urdu. Similarly, the amount of
parallel data available for pairs of languages diﬀer highly depending of
the popularity of such languages. To overcome these challenges, most
mLMs use exponentially smoothed weighting of the data while creat-
ing the pre-training data. This weighting prevents the low resource lan-
guagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal
pre-training data belongs to language i, then the probability of this lan-
guage ispi=k
100. Each probability is then modulated by an exponen-
tial factor α, then the resulting values are normalized to provide the
ﬁnal probability distribution over the languages. This probability dis-
tribution is used to the sample the pre-training data from the diﬀerent
languages.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />i.e. the resulting rule or anchor, such that A(x) = 1 ,f
represents the black-box model, Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi. The importance of the feature xi
is measured using:
Diﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)
7.3.2 Model Distillation
Xie et al. refer to the model distillation category as a post-training
method where the encoded knowledge in the model is distilled into a rep-
resentation agreeable for an explanation by a user, as shown in Fig. 7.3.
Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice of
the reference input x′is domain speciﬁc. For example, in MNIST
classiﬁcation, input with all zeros representing the common back-
ground can be one of the choices for the reference input.
∆t=f(x)−f(x′)is the diﬀerence in neuron output between input
xand a reference input x′. The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This weighting prevents the low resource lan-
guagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal
pre-training data belongs to language i, then the probability of this lan-
guage ispi=k
100. Each probability is then modulated by an exponen-
tial factor α, then the resulting values are normalized to provide the
ﬁnal probability distribution over the languages. This probability dis-
tribution is used to the sample the pre-training data from the diﬀerent
languages. Therefore, low-resource languages will be over sampled while
high-resource languages will be under sampled. Such process guaran-
tees the use of a reasonable set of vocabulary (while training the Word-
Piece [220] or SentencePiece model [144]) in the low resource languages.
Table 4.1 summarizes the number of languages supported by diﬀerent
mLMs and the total vocabulary used by them. Typically, mLMs which
support more languages have a larger vocabulary.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />(5.53)
When the kernel functions φ(x)are as deﬁned in [53], the attention
weightsAij= exp(
qikT
j)
can be approximated by
exp(qikT
j) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)
where Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-
dimensional standard normal distribution. If ωin (5.54) is replaced with√
dω
∥ω∥, thenωis any point on the surface of the dk-dimensional sphere
of radius√dk.
This kernel approximation of the softmax calculation reduces the
quadratic complexity to one that is nearly linear in sequence length, and
the approximation error can be decreased by periodically resampling ω.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>17. What are the different options proposed by the medical team for transport or medical repatriation?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the different options proposed by the medical team for transport or medical repatriation.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The medical team proposes two options for transport or medical repatriation:

1. **Assistance au véhicule et aux personnes transportées**: Matmut Assistance offers a range of services implemented by Inter Mutuelles Assistance GIE, located at 118 avenue de Paris, 79000 Niort. This service includes assistance for the vehicle and the individuals being transported.

2. **Garanties Mobilité**: The Mobility Guarantees are available when subscribed to and listed in the specific conditions of the contract, whether they are included or optional in the chosen coverage formula. The ceilings and indemnification modalities applicable to the Mobility Guarantees are detailed in articles 3-1 and 34 of the general conditions.

These options provide support and assistance for transport or medical repatriation needs as part of the insurance coverage.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19 BIS.1.4. Processus de souscription électronique
L’assuré peut souscrire son contrat électroniquement :
• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;
• soit dans le cadre d’une souscription en ligne sur Internet.
  ■Quelles sont les différentes étapes du processus de souscription 
électronique ?
• Renseignements des informations concernant l’assuré
Aux fins de nous permettre de proposer à l’assuré le contrat et les options 
les plus adaptées, l’assuré doit renseigner un certain nombre d’informations.
Si l’assuré souscrit son contrat par téléphone auprès de l’un de 
nos conseillers, ses réponses sont recueillies préalablement à toute 
souscription.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />• à vérifier régulièrement ses e -mails indésirables afin de s’ assurer que des 
e-mails liés à la gestion et à l’exécution de son contrat n’y figurent pas et 
s’ils y figurent à en prendre connaissance.
19 BIS.1.4. Processus de souscription électronique
L’assuré peut souscrire son contrat électroniquement :
• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;
• soit dans le cadre d’une souscription en ligne sur Internet.
  ■Quelles sont les différentes étapes du processus de souscription 
électronique ?
• Renseignements des informations concernant l’assuré
Aux fins de nous permettre de proposer à l’assuré le contrat et les options 
les plus adaptées, l’assuré doit renseigner un certain nombre d’informations.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />■Quelles sont les différentes étapes du processus de souscription 
électronique ?
• Renseignements des informations concernant l’assuré
Aux fins de nous permettre de proposer à l’assuré le contrat et les options 
les plus adaptées, l’assuré doit renseigner un certain nombre d’informations.
Si l’assuré souscrit son contrat par téléphone auprès de l’un de 
nos conseillers, ses réponses sont recueillies préalablement à toute 
souscription. Lorsqu’une date et heure sont indiquées en bas d’un 
document (telles les Conditions Personnelles), par la signature électronique 
de ce document l’assuré reconnaît que ce document a bien été établi à la 
date et heure indiquées.
Si l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner 
ces informations lui-même.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />30  CONDITIONS GÉNÉRALES   
Le symbole renvoie à un terme  
défini au lexi que (article 1 ) Section V  - GARANTIES MOBILITÉ    
Les garanties Mobilité vous sont acquises uniquement lorsque vous les avez souscrites et qu’elles figurent aux  conditions 
particulières  et ce, qu’elles soient en inclusion ou en option de la formule de garanties choisie.  
 
Les plafonds et modalités d’indemnisation applicables aux garanties Mobilité figurent aux articles 3 -1 et 34 des présentes conditions  
générales . 
 
ARTICLE  23    Assistance au véhicule et aux personnes transportées  
 
Matmut  Assistance  propose un ensemble de prestations mis en œuvre  par Inter Mutuelles Assistance GIE (118 avenue de Paris, 
79000 Niort).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19 BIS.1.4. Processus de souscription électronique
L’assuré peut souscrire son contrat électroniquement :
• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;
• soit dans le cadre d’une souscription en ligne sur Internet.
  ■Quelles sont les différentes étapes du processus de souscription 
électronique ?
• Renseignements des informations concernant l’assuré
Aux fins de nous permettre de proposer à l’assuré le contrat et les options 
les plus adaptées, l’assuré doit renseigner un certain nombre d’informations.
Si l’assuré souscrit son contrat par téléphone auprès de l’un de 
nos conseillers, ses réponses sont recueillies préalablement à toute 
souscription. Lorsqu’une date et heure sont indiquées en bas d’un 
document (telles les Conditions Personnelles), par la signature électronique 
de ce document l’assuré reconnaît que ce document a bien été établi à la 
date et heure indiquées.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />• Renseignements des informations concernant l’assuré
Aux fins de nous permettre de proposer à l’assuré le contrat et les options 
les plus adaptées, l’assuré doit renseigner un certain nombre d’informations.
Si l’assuré souscrit son contrat par téléphone auprès de l’un de 
nos conseillers, ses réponses sont recueillies préalablement à toute 
souscription. Lorsqu’une date et heure sont indiquées en bas d’un 
document (telles les Conditions Personnelles), par la signature électronique 
de ce document l’assuré reconnaît que ce document a bien été établi à la 
date et heure indiquées.
Si l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner 
ces informations lui-même. A tout moment, l’assuré peut retourner sur 
l’écran de consultation précédent afin de corriger une information inexacte.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />792 0.768 0.779
Medical Records 0.612 0.510 0.556
Internal Medicine 0.588 0.642 0.614
Other 0.587 0.670 0.626
accuracy 0.67
macro avg 0.645 0.648 0.644
weighted avg 0.673 0.670 0.670
TABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data
Class/Metrics Precision RecallF1
Surgery 0.796 0.788 0.792
Medical Records 0.605 0.676 0.639
Internal Medicine 0.690 0.633 0.660
Other 0.589 0.582 0.586
accuracy 0.694
macro avg 0.670 0.670 0.669
weighted avg 0.696 0.694 0.694
2.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />1.2.2 Pre-training Methods and Applications
A large body of work has focused on how a transformer can be
pre-trained. There are encoder-only models, such as BERT [71], decoder-
only models like the famed generative pre-trained transformer models
GPT-3 [32], and encoder-decoder models like T5 [205] and ByT5 [280].
BERT is discussed in detail in Chapter 3, T5 in Chapter 5, and ByT5
inChapter 6.
There have been many application and domain-speciﬁc transform-
ers made for speciﬁc data domains (e.g., ﬁnancial or medical text) and
speciﬁc kinds of data (e.g., images or video). We discuss several such
applications in Chapter 6.
1.3 RESOURCES
In this section, we will discuss some resources that can be useful for
researchers and practitioners.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■213
TABLE 7.1 Fine-tuned base BERT on Test Data
Class/Metrics Precision RecallF1
Surgery 0.792 0.768 0.779
Medical Records 0.612 0.510 0.556
Internal Medicine 0.588 0.642 0.614
Other 0.587 0.670 0.626
accuracy 0.67
macro avg 0.645 0.648 0.644
weighted avg 0.673 0.670 0.670
TABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data
Class/Metrics Precision RecallF1
Surgery 0.796 0.788 0.792
Medical Records 0.605 0.676 0.639
Internal Medicine 0.690 0.633 0.660
Other 0.589 0.582 0.586
accuracy 0.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />612 0.510 0.556
Internal Medicine 0.588 0.642 0.614
Other 0.587 0.670 0.626
accuracy 0.67
macro avg 0.645 0.648 0.644
weighted avg 0.673 0.670 0.670
TABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data
Class/Metrics Precision RecallF1
Surgery 0.796 0.788 0.792
Medical Records 0.605 0.676 0.639
Internal Medicine 0.690 0.633 0.660
Other 0.589 0.582 0.586
accuracy 0.694
macro avg 0.670 0.670 0.669
weighted avg 0.696 0.694 0.694
2.The BIO_ClinincalBERT performs worse than the base model on
39instances.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■211
7.6.2 Data, Tools, and Libraries
Healthcare data usage has lots of restrictions and constraints due to
HIPAA privacy regulations. However, MTSamples.com collects tran-
scribed medical reports with sample transcription reports for forty spe-
cialties and work types to overcome these limitations and help transcrip-
tions be more accessible. Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />However, MTSamples.com collects tran-
scribed medical reports with sample transcription reports for forty spe-
cialties and work types to overcome these limitations and help transcrip-
tions be more accessible. Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model. We employ the Huggingface implementation of BERT
and Bio_ClinicalBERT as our transformer implementations. Captum li-
brary is used to perform input attribution via the saliency method.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model. We employ the Huggingface implementation of BERT
and Bio_ClinicalBERT as our transformer implementations. Captum li-
brary is used to perform input attribution via the saliency method. For
understanding and visualizing how the BERT layers and heads work we
use the exBERT visualization package.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model. We employ the Huggingface implementation of BERT
and Bio_ClinicalBERT as our transformer implementations. Captum li-
brary is used to perform input attribution via the saliency method. For
understanding and visualizing how the BERT layers and heads work we
use the exBERT visualization package.
7.6.3 Experiments, Results, and Analysis
7.6.3.1 Exploratory data analysis
We perform some basic EDA to understand the data from distribution
and corpus wise.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The word
cloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show
bias towards terms such as diagnoses, injury,chronic, etc., which form
the base language for most medical transcriptions and records. The doc-
ument length distribution plot (Fig. 7.8) shows a long-tailed distribution
with greater than 25% documents falling beyond the maximum sequence
length for BERT. Finally, the conversion of 40classes to 4categories
(Fig. 7.10) shows almost a balanced distribution and becomes helpful in
classiﬁer comparisons as the impact of imbalance is now minimized.
7.6.3.2 Experiments
We use the 90–10% split of training and testing and further create a
validation set of 10% for hyperparameter and learning curves. We will
ﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned
on the training data and evaluated on test data.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />),possiblyindicatingthatthebaseBERTmodel
associates such terms with a class other than Surgery. The domain-
adapted Bio_Clinical BERT has positive and negative attributions for
bladderas well as positive explanations for the descriptions of the pro-
cedures (e.g., “placed in dorsal lithotomy position” and “prepped and
draped in the standard fashion”). This may give us some insight into
how the additional pre-training is helping Bio_Clinical BERT achieve a
relatively better performance.
In the example as shown in Figs. 7.14 and 7.12, we can see that
the BIO_ClinicalBERT correctly predicts Medical Records while
the base BERT predicts Internal Medicine. Highlights on context
words such as “past medical history”, “present”, “admitted”, etc. in the
BIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy
Medical Records is the predicted class for the BIO_ClinicalBERT.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Figs. 7.7 to 7.10 shows interesting outputs from word cloud, docu-
ment length distribution, top words, and class distribution. The word
cloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show
bias towards terms such as diagnoses, injury,chronic, etc., which form
the base language for most medical transcriptions and records. The doc-
ument length distribution plot (Fig. 7.8) shows a long-tailed distribution
with greater than 25% documents falling beyond the maximum sequence
length for BERT. Finally, the conversion of 40classes to 4categories
(Fig. 7.10) shows almost a balanced distribution and becomes helpful in
classiﬁer comparisons as the impact of imbalance is now minimized.
7.6.3.2 Experiments
We use the 90–10% split of training and testing and further create a
validation set of 10% for hyperparameter and learning curves.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This raises the question if multilingual transformers learn
language-agnostic cross-lingual representations. Several approaches have
been proposed to answer this question, each with a diﬀerent angle of at-
tack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora
Representations.
Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>18. How is the coverage amount determined based on the level of permanent disability?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on how the coverage amount is determined based on the level of permanent disability.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The coverage amount based on the level of permanent disability is determined through a process that involves assessing the extent of impairment or disability that an individual has sustained. This assessment is typically done by medical professionals or experts in the field of disability evaluation.

The determination of the coverage amount may involve the following steps:

1. **Medical Evaluation**: The individual's medical condition and functional limitations are assessed to determine the level of permanent disability.

2. **Disability Rating**: A disability rating is assigned based on the severity of the impairment. This rating is often expressed as a percentage, with higher percentages indicating more severe disabilities.

3. **Insurance Policy**: The coverage amount may be outlined in the individual's insurance policy or benefits plan. The policy will specify how much coverage is provided based on the disability rating.

4. **Coverage Calculation**: The coverage amount is calculated based on the disability rating and the terms of the insurance policy. This calculation may take into account factors such as the individual's income, occupation, and the type of disability insurance coverage they have.

5. **Legal Regulations**: In some cases, legal regulations or guidelines may dictate how the coverage amount is determined for individuals with permanent disabilities.

Overall, the coverage amount for individuals with permanent disabilities is determined through a combination of medical assessment, insurance policy terms, and legal regulations to ensure that the individual receives appropriate financial support based on their level of disability.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■195
•Prediction Diﬀerence Analysis: Zintgraf et al. propose a
method based on Robnik-Sikonja and Kononenko research, to
measure the inﬂuence based on altering input information for
probabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■195
•Prediction Diﬀerence Analysis: Zintgraf et al. propose a
method based on Robnik-Sikonja and Kononenko research, to
measure the inﬂuence based on altering input information for
probabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of
an input feature xiwith respect to class cby determining the dif-
ference between p(c|x−iandp(c|x )using the marginal probability
p(c|x−i) =
xip(xi|x−i)p(c|x−i,xi) (7.9)
wheremathbfx corresponds to all input features and x−icorre-
sponds to all features except xi. The importance of the feature xi
is measured using:
Diﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)
7.3.2 Model Distillation
Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way. Beam-search
can be used to improve upon this greedy search method by main-
taining a candidate set of rules during the iterative search process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Other studies discovered that
no one head contains the whole syntactic tree. Additionally, attention
weightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.
Additionally, it was shown that even when attention heads specialize in
monitoring semantic relations, they do not always help BERT perform
well on related tasks.
For layer-level knowledge localization, provided that the ﬁrst layer of
BERT gets representations in the form of a mix of token, segment, and
positionalembeddingsasinput.Itcomestoreasonthatthebottomlevels
contain the most linear information about word order. It was shown that
theknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-
base. This is followed by an enhanced understanding of the hierarchical
structure of sentences. Numerous studies have shown that the middle
BERT layers contain the most syntactic information and that the last
BERT levels include the most task-speciﬁc information.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Additionally, attention
weightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.
Additionally, it was shown that even when attention heads specialize in
monitoring semantic relations, they do not always help BERT perform
well on related tasks.
For layer-level knowledge localization, provided that the ﬁrst layer of
BERT gets representations in the form of a mix of token, segment, and
positionalembeddingsasinput.Itcomestoreasonthatthebottomlevels
contain the most linear information about word order. It was shown that
theknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-
base. This is followed by an enhanced understanding of the hierarchical
structure of sentences. Numerous studies have shown that the middle
BERT layers contain the most syntactic information and that the last
BERT levels include the most task-speciﬁc information. Additionally, it
was shown that, although the majority of syntactic information may be
localized in a few levels, semantic information is distributed across the
model, which explains why some non-trivial cases are initially handled
wrong then successfully at higher layers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For layer-level knowledge localization, provided that the ﬁrst layer of
BERT gets representations in the form of a mix of token, segment, and
positionalembeddingsasinput.Itcomestoreasonthatthebottomlevels
contain the most linear information about word order. It was shown that
theknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-
base. This is followed by an enhanced understanding of the hierarchical
structure of sentences. Numerous studies have shown that the middle
BERT layers contain the most syntactic information and that the last
BERT levels include the most task-speciﬁc information. Additionally, it
was shown that, although the majority of syntactic information may be
localized in a few levels, semantic information is distributed across the
model, which explains why some non-trivial cases are initially handled
wrong then successfully at higher layers.
3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS
3.5.1 Goal
In this chapter, we looked at several applications of the Transformer
architecture.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Vi-
sualization of the attention layers demonstrates that the model selects
informative words and sentences for classiﬁcation.
Jesse Vig introduced a tool “BertViz” for visualizing attention in the
Transformer at all levels, viz. at the whole model level, the attention-
head level, and the individual neuron level [255]. The model view pro-
vides a single high-level view of the entire model across all the layers
and heads for a given input. The model view helps in visualizing how
attention patterns evolve in the attention heads across all layers. The
attention-head view visualizes the attention patterns produced by one
or more attention heads in a given layer and can help detect biases such
as gender bias. The neuron view visualizes the individual neurons in the
query and key vectors and their interaction producing attention, thus
giving a detailed view of how the patterns are formed.
7.3.3.2 Joint training
In a joint training-based approach, the explanation task is associated
with the actual learning task so that the model learns the explanation
and the actual task jointly.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers apply the same amount of
computation to all the inputs leading to ineﬃciencies in many cases
where computations can be conditioned on the complexity.
Universaltransformers(UT)byDehghanietal.[69]isanextensionof
transformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts
get supplemented by the recurrent inductive bias of RNNs while being
computationally universal. Instead of a ﬁxed number of layers in the
transformers,UniversaltransformershaveaUniversaltransformerblock,
i.e., a self-attention mechanism followed by a recurrent transformation
thatprovidesarecurrentinductivebiasforeachinputsymbolinparallel.
As shown in Fig. 5.5, the Universal Transformer is a recurrent function
not in time but in depth that evolves the hidden states corresponding to
every input in parallel, based at each step on the sequence of previous
hidden states.
UT has many commonalities with the existing neural architectures,
such as the Neural GPU [136] and the Neural Turing Machine [98].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way. Beam-search
can be used to improve upon this greedy search method by main-
taining a candidate set of rules during the iterative search process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].
•Integrated Gradients (IG). Integrated gradients by Sundara-
jan et al. is an explanation technique by computing relevance for
a networkfbased on two axioms: sensitivity and implementation
variance [236].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].
•Integrated Gradients (IG). Integrated gradients by Sundara-
jan et al. is an explanation technique by computing relevance for
a networkfbased on two axioms: sensitivity and implementation
variance [236]. The sensitivity axiom: for an input xwhich diﬀers
from some baseline input x′along feature xiandf(x)̸=f(x′)
thenxishould have a non-zero relevance.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The word
cloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show
bias towards terms such as diagnoses, injury,chronic, etc., which form
the base language for most medical transcriptions and records. The doc-
ument length distribution plot (Fig. 7.8) shows a long-tailed distribution
with greater than 25% documents falling beyond the maximum sequence
length for BERT. Finally, the conversion of 40classes to 4categories
(Fig. 7.10) shows almost a balanced distribution and becomes helpful in
classiﬁer comparisons as the impact of imbalance is now minimized.
7.6.3.2 Experiments
We use the 90–10% split of training and testing and further create a
validation set of 10% for hyperparameter and learning curves. We will
ﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned
on the training data and evaluated on test data.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The choice of
the reference input x′is domain speciﬁc. For example, in MNIST
classiﬁcation, input with all zeros representing the common back-
ground can be one of the choices for the reference input.
∆t=f(x)−f(x′)is the diﬀerence in neuron output between input
xand a reference input x′. The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />i.e. the resulting rule or anchor, such that A(x) = 1 ,f
represents the black-box model, Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Cumulative scoring is an evaluation set-dependent metric esti-
mating how much higher a score one can obtain on a probing task with
the introduction of each layer.
Hewitt and Liang, in their work, devise a control task strategy as
a measure to address the probing confounder problem [110]. Probing
confounder problem can be deﬁned as—given the main neural architec-
ture that needs to be explained, the probing classiﬁer such as MLP or
logistic regression and the supervised auxiliary task, how do we assign
the credit for performance such as test accuracy to either of the three?
They devise control tasks that can be easily learned by a probing di-
agnostic classiﬁer but not encoded in the representations (actual neural
model or layers). Evaluating the performance (testing accuracy) diﬀer-
ence between the control tasks and auxiliary tasks with diﬀerent probing
classiﬁer choices as a selectivity measure can easily assign the credit of
learning to either the representation or the probing classiﬁer.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>19. How are the names and addresses of the members present or represented recorded during a general assembly?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on how the names and addresses of the members present or represented are recorded during a general assembly.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          During a general assembly, the names and addresses of the members present or represented are typically recorded in a sign-in sheet or attendance register. Each member is required to sign or mark their attendance along with their name and address to officially record their presence at the meeting. This record serves as a formal documentation of who participated in the general assembly and can be used for various purposes such as quorum verification, voting eligibility, and communication with the members.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree. In terms of how syntactic information is represented, it
seems that self-attention weights do not directly encode syntactic struc-
ture, but they may be changed to reﬂect it.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.4.2 BERTology
The great push BERT has provided to the NLP ﬁeld triggered much
research into understanding how it works and the type of knowledge it
extracts through massive pre-training. BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />However, the last layer is too close
to the target functions (i.e., masked language model and next sentence
prediction) during pre-training, therefore may be biased to those targets.
3.4.2 BERTology
The great push BERT has provided to the NLP ﬁeld triggered much
research into understanding how it works and the type of knowledge it
extracts through massive pre-training. BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />52■Transformers for Machine Learning: A Deep Dive
BERT sentence representation can also be obtained by pooling the
individual token representations. However, the last layer is too close
to the target functions (i.e., masked language model and next sentence
prediction) during pre-training, therefore may be biased to those targets.
3.4.2 BERTology
The great push BERT has provided to the NLP ﬁeld triggered much
research into understanding how it works and the type of knowledge it
extracts through massive pre-training. BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />),possiblyindicatingthatthebaseBERTmodel
associates such terms with a class other than Surgery. The domain-
adapted Bio_Clinical BERT has positive and negative attributions for
bladderas well as positive explanations for the descriptions of the pro-
cedures (e.g., “placed in dorsal lithotomy position” and “prepped and
draped in the standard fashion”). This may give us some insight into
how the additional pre-training is helping Bio_Clinical BERT achieve a
relatively better performance.
In the example as shown in Figs. 7.14 and 7.12, we can see that
the BIO_ClinicalBERT correctly predicts Medical Records while
the base BERT predicts Internal Medicine. Highlights on context
words such as “past medical history”, “present”, “admitted”, etc. in the
BIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy
Medical Records is the predicted class for the BIO_ClinicalBERT.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />At a glance, we can see that the most signiﬁcant
negativeattributionsforthebaseBERTmodelcomefromkidney-related
terms(kidney,renal,etc.),possiblyindicatingthatthebaseBERTmodel
associates such terms with a class other than Surgery. The domain-
adapted Bio_Clinical BERT has positive and negative attributions for
bladderas well as positive explanations for the descriptions of the pro-
cedures (e.g., “placed in dorsal lithotomy position” and “prepped and
draped in the standard fashion”). This may give us some insight into
how the additional pre-training is helping Bio_Clinical BERT achieve a
relatively better performance.
In the example as shown in Figs. 7.14 and 7.12, we can see that
the BIO_ClinicalBERT correctly predicts Medical Records while
the base BERT predicts Internal Medicine. Highlights on context
words such as “past medical history”, “present”, “admitted”, etc.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The domain-
adapted Bio_Clinical BERT has positive and negative attributions for
bladderas well as positive explanations for the descriptions of the pro-
cedures (e.g., “placed in dorsal lithotomy position” and “prepped and
draped in the standard fashion”). This may give us some insight into
how the additional pre-training is helping Bio_Clinical BERT achieve a
relatively better performance.
In the example as shown in Figs. 7.14 and 7.12, we can see that
the BIO_ClinicalBERT correctly predicts Medical Records while
the base BERT predicts Internal Medicine. Highlights on context
words such as “past medical history”, “present”, “admitted”, etc. in the
BIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy
Medical Records is the predicted class for the BIO_ClinicalBERT.
We can also visualize the self-attention weights to gain another view</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques. The Xie et al. taxonomy, based on foundational explainability
methods, restricted only to generic or speciﬁc transformer-based models,
is discussed in this section and as shown in Fig. 7.1.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques. The Xie et al. taxonomy, based on foundational explainability
methods, restricted only to generic or speciﬁc transformer-based models,
is discussed in this section and as shown in Fig. 7.1.
7.3.1 Visualization Methods
Visualization methods reveal an explanation by highlighting the in-
ﬂuence between the inputs and the outputs in a black-box model as
shown in Fig.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Sutskever’s research, which proposed a Hessian-free optimizer to train
RNNs eﬃciently on long-term dependencies, was a breakthrough in re-
viving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-
troduced sequence-to-sequence learning as a generic neural framework
comprised of an encoder neural network processing inputs as a sequence
and a decoder neural network predicting the outputs based on the in-
put sequence states and the current output states [238]. As a result, the
sequence-to-sequence framework became the core architecture for a wide
range of NLP tasks such as constituency parsing, named entity recogni-
tion (NER), machine translation, question-answering, and summariza-
tion, to name a few. Furthermore, even Google started replacing its</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key
kjso that kj=qj
||qj||. To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries
m=2L
Number of buckets(5.34)
Within each block, each position is allowed to attend to the others in
the block and to those in the preceding block.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For instance, the
phrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted
from the WikiTableQuestions dataset) may be accompanied with a table
includinginformationontheyear,venue,position,andevent.Asnapshot
of content will take a subset of three rows. This subset will not include
all of the table’s data, but it is suﬃcient for the model to understand
that, for example, the venue column comprises cities. TaBERT describes
table structure using a mix of traditional horizontal self-attention, which
captures the dependence between cells in speciﬁc rows, and vertical self-
attention, which catches the information ﬂow between cells in diﬀerent
rows. The ultimate outputs of layers of such horizontal and vertical self-
attention are distributed representations of utterance tokens and table
columns,whichmaybeutilizedtocalculatethedatabasequerybydown-
stream semantic parsers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />,
in INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,
eds., ISCA, 2010, pp. 1045–1048.
[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean ,Distributed representations of words and phrases and
their compositionality , in Advances in Neural Information Process-
ingSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-
mani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,
pp. 3111–3119.
[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to
computational geometry , MIT press, 2017.
[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,
and K.-R.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries
m=2L
Number of buckets(5.34)
Within each block, each position is allowed to attend to the others in
the block and to those in the preceding block. These two changes deﬁne
a new set of key positions that query ican attend to:
˜Si={
j:⌊si
m⌋
−1≤⌊sj
m⌋
≤⌊si
m⌋}
, (5.35)
wheresiis the position in the sorted matrix that position iwas moved
to. (5.35) can be used in (5.31) to compute the attention under the LSH
scheme described above.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Bibliography ■249
tional Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26–30, 2020, OpenReview.net, 2020.
[264]A. Warstadt, A. Singh, and S. R. Bowman ,Neural network
acceptability judgments , arXiv preprint arXiv:1805.12471, (2018).
[265]D. Watts and S. Strogatz ,Collective dynamics of ‘small-
world’ networks , Nature, 393 (1998), pp. 440–442.
[266]X. Wei, R. Weng, Y. Hu, L. Xing, H. Yu, and W. Luo ,
On learning universal representations across languages , in 9th In-
ternational Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3–7, 2021, OpenReview.net, 2021.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Without a loss in generality, we can rewrite (5.30) as a sum over an
expanded set of key positions, ˜Si⊇Si, that can include positions that
qidoes not attend to:
ai=
j∈˜Siexp(
qikT
j√dk−m(i,Si)−logZ(i,Si)(
vj(5.31)
m(i,Si) ={∞, j /∈Si
0,otherwise(5.32)
The termm(i,Si)in (5.31) is a masking term that ensures that key
positions that qidoesnotattend to does not contribute to the sum.
As mentioned above, the set Siis the set of key positions that query i
attends to. Under the LSH scheme deﬁned above, Sishould only contain
key positions that hash into the same bucket as the query, or in other
words
Si={j:h(qi) =h(kj)} (5.33)
A priori, there are no guarantees that a query will have any keys to
attend to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformer Modiﬁcations ■133
will only have directed edges with the vertices that correspond to the
key positions that it can attend to, as described by the set Siin (5.27).
Adjacency matrix The graph representation casts the problem of
reducing the complexity of the attention mechanism as a graph spar-
siﬁcation problem, which can be tackled with graph theory. Big Bird
describes the attention graph with its L×Ladjacency matrix, A:
A(i,j ) ={1, j∈Si
0,otherwise(5.55)
When the adjacency matrix is all ones, A(i,j ) = 1,∀(i,j), then we
have a fully connected graph where each vertex is connected to every
other vertex and an attention mechanism with quadratic complexity.
WheneverA(i,j ) = 0, it means that edge (i,j)does not exist and thus
query position i(qi) cannot attend to key position j(kj).</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>20. What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens, is typically defined in insurance policies as the "policy limit" or "coverage limit." This limit represents the maximum amount that the insurance company will pay out for covered claims or expenses under the policy. Once the policy limit is reached, the policyholder is generally not responsible for any additional charges, except for specific circumstances such as increases in fiscal burdens or similar conditions as outlined in the policy terms and conditions.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This behavior is expected since the sequence length was held ﬁxed.
To test performance, Linformer was trained in the same way as
RoBERTa [170] and compared using validation perplexities and perfor-
mance on downstream tasks. Based on perplexities, Linformer perfor-
mance increases as k increases and Linformer’s quality nearly matches
that of the standard transformer for L= 512,k= 128andL=
1024,k = 256. Based on downstream task performance, Linformer per-
formance is comparable to RoBERTa when L= 512,k = 128, and out-
performs RoBERTa at k= 256. Performance for L= 1024,k = 256and
L= 512,k= 256are similar, suggesting that performance for Linformer
is controlled more by kthan byL/k. There are two key results:
1. Layerwise parameter sharing performs the best.
2. Performance when using layerwise sharing is nearly identical to
the performance of using no parameter sharing.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The Linformer speed remained largely constant and was
signiﬁcantly faster for long sequences. Wang et al. kept the sequence
length ﬁxed and varied the batch size to process the text sequences.
This behavior is expected since the sequence length was held ﬁxed.
To test performance, Linformer was trained in the same way as
RoBERTa [170] and compared using validation perplexities and perfor-
mance on downstream tasks. Based on perplexities, Linformer perfor-
mance increases as k increases and Linformer’s quality nearly matches
that of the standard transformer for L= 512,k= 128andL=
1024,k = 256. Based on downstream task performance, Linformer per-
formance is comparable to RoBERTa when L= 512,k = 128, and out-
performs RoBERTa at k= 256. Performance for L= 1024,k = 256and
L= 512,k= 256are similar, suggesting that performance for Linformer
is controlled more by kthan byL/k.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Due to the complexity reduction aﬀorded by the pooling between
layers, it is possible to add additional encoder layers or make existing
layers wider without increasing the computational load in any signiﬁcant
manner.
Performance Three sizes of the standard transformer were compared
to several conﬁgurations of Funnel-Transformer, each with fewer or sim-
ilar numbers of expected ﬂoating-point operations to the transformer to
which it was being compared:
•large: 24 layers, d= 1024
•base: 12 layers, d= 768
•small: 6 layers, d= 768
Quality comparisons are made for GLUE and when Funnel-
Transformer decreases the sequence length and adds more layers, it per-
forms better than the standard transformer on text classiﬁcation and
all GLUE datasets except for STS-B. When the sequence length is de-
creases but the depth is not increased, performance decreases on GLUE
text classiﬁcation datasets.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The expansion phase of the DeLighT block is shown in Fig. 5.3.
Mehta et al. proposed that by increasing the depth of the DeLighT
block and the width of its intermediate GLT layers, the transformer
will have an increased representational capacity and that one can then
replace multi-head attention with single-head attention. Similarly, they
propose that the DeLighT block’s wide linear layers let one decrease the
size of the feedforward layer by up to 16x.
Performance DeLighT performed as well as or better than the stan-
dard transformer on machine translation, despite having considerably
fewer parameters. But, when DeLighT is given more parameters, it out-
performs the standard transformer. It also obtains similar or better qual-
ity to SOTA models on machine translation. Also, performance increases
with the number of network parameters in the DeLighT block.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Regarding aspects of semantic knowledge, BERT has shown some
knowledge for semantic roles. Also BERT encodes information about
entity types, relations, semantic roles, and proto-roles. However, it was
found that BERT struggles with representations of numbers. Research
probing into BERT’s world knowledge capabilities showed that, for some
relation types, vanilla BERT is competitive with methods relying on
knowledge bases. However, BERT cannot reason based on its world
knowledge. Essentially BERT can “guess” the aﬀordances and properties
of many objects, but does not have the information about their interac-
tions (e.g., it “knows” that people can walk into houses, and that houses
are big, but it cannot infer that houses are bigger than people.)
Additionally, BERTology is concerned with the localization of
linguistic information inside the BERT architecture, either at the</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This acts as a
residual connection. They then add a few transformer layers on top of g
for it to learn how to best combine the features.
Scaling ThetimecomplexityofFunnel-Transformeris O(d·T2+T·d2).
SinceTdecreases by half at successive encoder layers, the complexity
decreases by a factor of four for each layer. Since O(T·d2)has the large
constantd2, it tends to dominate, providing a linear speedup, instead of
a quadratic one.
Due to the complexity reduction aﬀorded by the pooling between
layers, it is possible to add additional encoder layers or make existing
layers wider without increasing the computational load in any signiﬁcant
manner.
Performance Three sizes of the standard transformer were compared
to several conﬁgurations of Funnel-Transformer, each with fewer or sim-
ilar numbers of expected ﬂoating-point operations to the transformer to
which it was being compared:
•large: 24 layers, d= 1024
•base: 12 layers, d= 768
•small: 6 layers, d= 768
Quality comparisons are made for GLUE and when Funnel-
Transformer decreases the sequence length and adds more layers, it per-
forms better than the standard transformer on text classiﬁcation and
all GLUE datasets except for STS-B.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Performance Talking-Heads Attention (THA) was evaluated by
training a T5 model using THA and the same hyperparameters as the
T5 paper (except for omitting dropout during pre-training) and simi-
larly training an ALBERT model. The authors found that THA consis-
tently performed better than multi-head attention [227]. Just projecting
the attention logits or just the attention weights is only slightlybetter
than using pure multi-head attention. Signiﬁcant performance increases
came from using both projections. Using talking-heads attention on the
encoder block’s self-attention layers has a larger eﬀect on model perfor-
mance than using talking-heads attention on the decoder block’s atten-
tion layers.
Multi-headattentionalreadycomesatacostandtheprojectionsthat
THA adds, WαandWA, increase that cost.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The dovector is what is then pro-
jected in the query, key, and value spaces. DeLighT blocks closer to the
output of the model are made wider and deeper than those closer to the
model input. DeLightT used do=d/2and also shuﬄes the features be-
tween the groups in the GLT, using a mixer connection [182] to combine
the shuﬄed features with the input, similar to a residual connection.
The expansion phase of the DeLighT block is shown in Fig. 5.3.
Mehta et al. proposed that by increasing the depth of the DeLighT
block and the width of its intermediate GLT layers, the transformer
will have an increased representational capacity and that one can then
replace multi-head attention with single-head attention. Similarly, they
propose that the DeLighT block’s wide linear layers let one decrease the
size of the feedforward layer by up to 16x.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />i.e. the resulting rule or anchor, such that A(x) = 1 ,f
represents the black-box model, Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■205
Jensen-Shannon Divergence is used to measure the diﬀerence between
the distributions. The optimization equation is given by:
maximizeα(1),···,α(k)f({α(i)}k
i=1) (7.20)
wheref({α(i)}k
i=1)is:
k
i=1JSD[α(i),ˆα] +1
k(k−1)k
i<jJSD[α(i),α(j)](7.21)
The ﬁrst part of the equation∑k
i=1JSD[α(i),ˆα]ﬁnds maximally
diﬀerent attention from the observed ˆαand the second part
1
k(k−1)∑k
i<jJSD[α(i),α(j)is maximally diﬀerent from each other.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />204■Transformers for Machine Learning: A Deep Dive
The experiments show a consistently low correlation between the atten-
tion weights and feature importance scores across all the datasets, espe-
cially for contextualized encoders .
7.4.1.2 Counterfactual experiments
To validate the second hypothesis, the authors put forth two empirical
questions
1. How much does the output change if the attention scores are ran-
domly permutated?
2. Can we ﬁnd maximally diﬀerent attention that does not change
the output more than a predeﬁned threshold epsilon?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The word
cloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show
bias towards terms such as diagnoses, injury,chronic, etc., which form
the base language for most medical transcriptions and records. The doc-
ument length distribution plot (Fig. 7.8) shows a long-tailed distribution
with greater than 25% documents falling beyond the maximum sequence
length for BERT. Finally, the conversion of 40classes to 4categories
(Fig. 7.10) shows almost a balanced distribution and becomes helpful in
classiﬁer comparisons as the impact of imbalance is now minimized.
7.6.3.2 Experiments
We use the 90–10% split of training and testing and further create a
validation set of 10% for hyperparameter and learning curves. We will
ﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned
on the training data and evaluated on test data.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Drepresent a known distribution
of perturbations, zis a sample drawn from the conditional distri-
bution when rule AappliesD(z|A), andτis the given precision
threshold, then the anchor deﬁnition is given by:
ED(z|A)
⊮f(x)=f (z)
≥τ, A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />the edges as the attention weights; the maximum ﬂow algorithm can
compute the maximum attention ﬂow from any node of any layer to the
input nodes(tokens).
Fig. 7.6 shows attention ﬂow computation between an input token
(source) and the output using the capacity (minimum weights).
The research shows that the two methods give complementary view-
points, and compared to draw attention, both yield higher correlations
with importance scores of input tokens using gradient-based and the
ablation method.
7.6 CASE STUDY: TEXT CLASSIFICATION WITH EXPLAIN-
ABILITY
7.6.1 Goal
In the healthcare domain, the transparency and interpretability of ma-
chine learning models are imperative for their adoption. This section will
go through a use case using explainability techniques on state-of-the-
art transformers to illustrate the usefulness of explainability techniques.
The goal is to compare a simple traditional interpretable machine learn-
ing algorithm like logistic regression to the state-of-the-art BERT and
Clinical BERT to see the performance gain and application of post-hoc
techniques to explain the black-box models.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>21. What is the role of the board of directors in determining the remuneration of the director general?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the role of the board of directors in determining the remuneration of the director general.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The board of directors plays a crucial role in determining the remuneration of the director general. Their responsibilities typically include:

1. **Setting Compensation Policies:** The board establishes the overall compensation philosophy and policies for the organization, including guidelines for executive compensation.

2. **Approving Compensation Packages:** The board reviews and approves the compensation package of the director general, which may include salary, bonuses, stock options, benefits, and other forms of remuneration.

3. **Ensuring Fairness and Alignment:** The board ensures that the director general's compensation is fair, competitive, and aligned with the organization's goals, performance, and industry standards.

4. **Performance Evaluation:** The board evaluates the performance of the director general against predetermined goals and benchmarks to determine if the compensation is justified.

5. **Engaging Compensation Committees:** In some cases, boards may delegate compensation-related tasks to a specialized committee, such as the compensation committee, which makes recommendations to the full board.

6. **Transparency and Disclosure:** Boards are responsible for ensuring transparency in executive compensation practices and disclosing relevant information to stakeholders, including shareholders.

Overall, the board of directors plays a critical role in ensuring that the director general's remuneration is appropriate, competitive, and in line with the organization's objectives and values.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />154■Transformers for Machine Learning: A Deep Dive
has weights WO, giving the mechanism the following output:
yi=
L
j=1Aijv(xj)(
(WO (5.76)
=L
j=1Aijf(xj) (5.77)
whereAijare the attention weights from (5.27), v(xj)is a value
vector from 5.25, and f(xj) =v(xj)WO.f(xj)is the output of the
attention mechanism corresponding to the jthinput token xj.
In [143], Kobayashi et al. speciﬁcally suggest using ||Aijf(xj)||as
it’s the degree to which token iattends to token j. They also show that
||f(xj)||andAijare meaningful quantities and sometimes play opposing
roles in determining model behavior.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Each input
vector, xi, generates three diﬀerent vectors: the query, key, and value,
(qi,ki,vi,). The query, key, and value vectors are obtained by projecting
the input vector, xi, at timeion the learnable weight matrices Wq,Wk,
andWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight
matrices are randomly initialized and the weights are jointly learned
from the training process. For the ﬁrst attention layer of the encoder
and decoder, the inputs are the summation of the word embeddings and
positional encodings.
Similar to the attention discussion in section 2.3 where we discussed
the query, key, and values, and how they impact the ﬁnal attention
scores, the self-attention has all three vectors generated for every input
and the following are their key roles:
1. The role of the query vector of token i,qi, is to combine with every
other key vectors∑l
j=0qikjTto inﬂuence the weights for its own
output, zi.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For the ﬁrst attention layer of the encoder
and decoder, the inputs are the summation of the word embeddings and
positional encodings.
Similar to the attention discussion in section 2.3 where we discussed
the query, key, and values, and how they impact the ﬁnal attention
scores, the self-attention has all three vectors generated for every input
and the following are their key roles:
1. The role of the query vector of token i,qi, is to combine with every
other key vectors∑l
j=0qikjTto inﬂuence the weights for its own
output, zi.
2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery
other query vectors to get similarity with query and to inﬂuence
the output through query-key product scoring.
3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For the ﬁrst attention layer of the encoder
and decoder, the inputs are the summation of the word embeddings and
positional encodings.
Similar to the attention discussion in section 2.3 where we discussed
the query, key, and values, and how they impact the ﬁnal attention
scores, the self-attention has all three vectors generated for every input
and the following are their key roles:
1. The role of the query vector of token i,qi, is to combine with every
other key vectors∑l
j=0qikjTto inﬂuence the weights for its own
output, zi.
2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery
other query vectors to get similarity with query and to inﬂuence
the output through query-key product scoring.
3. The role of the value vector of token i,vi, is extracting information
by combining with the output of the query-key scores to get the
output vector zi.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />self-attention. Fig. 2.10 shows how the input vectors, xi, are converted
to the output vectors, zi, through the self-attention layer. Each input
vector, xi, generates three diﬀerent vectors: the query, key, and value,
(qi,ki,vi,). The query, key, and value vectors are obtained by projecting
the input vector, xi, at timeion the learnable weight matrices Wq,Wk,
andWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight
matrices are randomly initialized and the weights are jointly learned
from the training process. For the ﬁrst attention layer of the encoder
and decoder, the inputs are the summation of the word embeddings and
positional encodings.
Similar to the attention discussion in section 2.3 where we discussed
the query, key, and values, and how they impact the ﬁnal attention
scores, the self-attention has all three vectors generated for every input
and the following are their key roles:
1.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Performance Clustered attention outperforms standard transformer
and Reformer on automatic speech recognition with WSJ and Switch-
board audio datasets. It also approximates pre-trained RoBERTa on
GLUE and SQuAD, with minute performance loss. It performs better
on GLUE that RoBERTa, but not SQuAD, where it is slightly worse.
As number of clusters increases, the approximation becomes more
accurate.Itconvergesuptotwiceasfastasthestandardtransformer,for
longsequencelengthsand,forshortsequencelengths,clusteredattention
isnotfaster than the standard transformer.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree. In terms of how syntactic information is represented, it
seems that self-attention weights do not directly encode syntactic struc-
ture, but they may be changed to reﬂect it. When executing the cloze
task, BERT takes subject-predicate agreement into consideration.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree. In terms of how syntactic information is represented, it
seems that self-attention weights do not directly encode syntactic struc-
ture, but they may be changed to reﬂect it.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree. In terms of how syntactic information is represented, it
seems that self-attention weights do not directly encode syntactic struc-
ture, but they may be changed to reﬂect it. When executing the cloze
task, BERT takes subject-predicate agreement into consideration. Addi-
tionally, it was shown that BERT does not "understand" negation and
is indiﬀerent to input that is faulty.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.4.2 BERTology
The great push BERT has provided to the NLP ﬁeld triggered much
research into understanding how it works and the type of knowledge it
extracts through massive pre-training. BERTology aims to answer some
of the questions about why BERT performs well on so many NLP tasks.
Some of the topics addressed by BERTology include the type of knowl-
edge learned by BERT and where it is represented. In general there
three types of knowledge BERT acquires: Syntactic knowledge, Seman-
tic knowledge, World knowledge.
BERT representations of syntactic knowledge are hierarchical rather
than linear, i.e., they include a syntactic tree structure in addition to
the word order information. Additionally, BERT embeddings store infor-
mation about speech segments, grammatical chunks, and roles. BERT’s
understanding of syntax, on the other hand, is incomplete, since probing
classiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe
syntactic tree.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Foreword
Renowned AI pioneer and Nobel laureate Herbert Simon underscored
“attention” as the most valuable resource of the information econ-
omy, as necessary to allocate attention eﬃciently among the over-
abundance of information resources. Having written the foundational
paper on meaning-aware AI and recently having served as MIT-
Princeton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-
lishing by invitation in the same journal’s special issue of ASQ, and of
being the Malcolm Baldrige National Quality Award administrator, as
well as being ranked along with Dr. Simon in the same global academic
citation impact studies.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].
•Integrated Gradients (IG). Integrated gradients by Sundara-
jan et al. is an explanation technique by computing relevance for
a networkfbased on two axioms: sensitivity and implementation
variance [236].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />He held
many senior roles: chief analytics oﬃcer for Digital Reasoning, advisor
for Falkonry, and chief data scientist for BAE Systems Applied Intelli-
gence. Dr. Kamath has many patents and has built commercial products
using AI in domains such as compliance, cybersecurity, ﬁnancial crime,
and bioinformatics. He currently works as the chief analytics oﬃcer for
Smarsh. He is responsible for data science, research of analytical prod-
ucts employing deep learning, transformers, explainable AI, and modern
techniques in speech and text for the ﬁnancial domain and healthcare.
Kenneth L. Graham has two decades experience solving quantita-
tive problems in multiple domains, including Monte Carlo simulation,
NLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the
past ten years, he has focused on building scalable solutions in NLP for
government and industry, including entity coreference resolution, text
classiﬁcation, active learning, automatic speech recognition, and tempo-
ral normalization.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].
•Integrated Gradients (IG). Integrated gradients by Sundara-
jan et al. is an explanation technique by computing relevance for
a networkfbased on two axioms: sensitivity and implementation
variance [236]. The sensitivity axiom: for an input xwhich diﬀers
from some baseline input x′along feature xiandf(x)̸=f(x′)
thenxishould have a non-zero relevance.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors lay the following three requirements for faithful expla-
nations for attention mechanisms.
1. Attention mechanism should be a NECESSARY component for
good model performance.
2. Attention distributions should be hard to manipulate, i.e., if any
trained model can vary the distribution of attention weights and
yet have similar predictions, they may not be suitable for the ex-
planation. This directly corresponds to the exclusivity requisite
for faithful explanations and will guide the search for adversarial
models.
3. Attention distributions should work well in uncontextualized set-
tings. Since the attention weights are typically learned on contex-
tualized hidden layer outputs, to see the impact on input tokens,
one needs to use uncontextualized settings to judge their useful-
ness.
7.4.2.1 Is attention necessary for all tasks?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The relevance score R∆xi∆tassigned
for the input feature xiwithNneurons necessary to compute tis
given by:
∆t=N
i=1R∆xi∆t (7.5)
Shrikumar et al. provide diﬀerent ways to calculate the weighting
between the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale
rule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich
measures the relevance of ∆xwith respect to ∆t, averaged by ∆x
as:
m∆x∆t =R∆x∆t
∆x(7.6)
Layer by layer computation of relevance scores using chain rule is
adopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply
DeepLIFT on the BERT model predictions and test the outcomes
to monitor shifts in the attention values for input for the question-
answering application [8].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors lay the following three requirements for faithful expla-
nations for attention mechanisms.
1. Attention mechanism should be a NECESSARY component for
good model performance.
2. Attention distributions should be hard to manipulate, i.e., if any
trained model can vary the distribution of attention weights and
yet have similar predictions, they may not be suitable for the ex-
planation. This directly corresponds to the exclusivity requisite
for faithful explanations and will guide the search for adversarial
models.
3. Attention distributions should work well in uncontextualized set-
tings. Since the attention weights are typically learned on contex-
tualized hidden layer outputs, to see the impact on input tokens,
one needs to use uncontextualized settings to judge their useful-
ness.
7.4.2.1 Is attention necessary for all tasks?
The authors use the same three sets of tasks and six classiﬁcation
datasets using the BiLSTM model from the Jain and Wallace setting
and create another model where the attention weights are uniform in
distribution compared to the learned weights.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />He currently works as the chief analytics oﬃcer for
Smarsh. He is responsible for data science, research of analytical prod-
ucts employing deep learning, transformers, explainable AI, and modern
techniques in speech and text for the ﬁnancial domain and healthcare.
Kenneth L. Graham has two decades experience solving quantita-
tive problems in multiple domains, including Monte Carlo simulation,
NLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the
past ten years, he has focused on building scalable solutions in NLP for
government and industry, including entity coreference resolution, text
classiﬁcation, active learning, automatic speech recognition, and tempo-
ral normalization. He currently works at AppFolio as a senior machine
learning engineer. Dr. Graham has ﬁve patents for his work in natural
language processing, seven research publications, and a PhD in con-
densed matter physics.
Wael Emara has two decades of experience in academia and indus-
try.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Foreword
Renowned AI pioneer and Nobel laureate Herbert Simon underscored
“attention” as the most valuable resource of the information econ-
omy, as necessary to allocate attention eﬃciently among the over-
abundance of information resources. Having written the foundational
paper on meaning-aware AI and recently having served as MIT-
Princeton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-
lishing by invitation in the same journal’s special issue of ASQ, and of
being the Malcolm Baldrige National Quality Award administrator, as
well as being ranked along with Dr. Simon in the same global academic
citation impact studies.
Given the above background, I am thrilled to share with you the
most thorough and up-to-date compendium of research, practices, case
studies, and applications available today that can provide the best ROI
on the latest AI technological advances on transformers inspired by the
paper, “Attention is All You Need.” Since Google introduced transformer
architecture in 2017, transformers have provided exponential improve-
ments in context-focused realization toward meaning-aware AI as deep
(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch
as dot-product attention and multi-head attention.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>22. What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The specific expenses covered can vary depending on the insurance policy. However, typically, these may include accommodation costs, meals, and sometimes even local transportation. It's important to check the specific terms and conditions of the insurance policy for detailed information.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Expenses that may be covered for a person waiting for the repatriation of a non-transportable injured individual could include:

1. Accommodation costs: Expenses related to staying in a hotel or temporary accommodation while waiting for repatriation.

2. Meal expenses: Costs for meals and food during the waiting period.

3. Transportation costs: Any necessary transportation expenses incurred during the waiting period.

4. Medical expenses: Costs related to medical care or treatment needed while waiting for repatriation.

5. Communication expenses: Costs for phone calls, internet access, or other communication needs during the waiting period.

It's important to check the specific terms and conditions of the insurance policy or assistance program to understand the exact coverage provided in such situations.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />6/60
 Intérieur du véhicule
Habitacle, dessous du capot et coffre.
 Marchandise
Tout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, 
à l’exception des biens véhiculés dans le cadre du transport public de marchandises.
 Matériel professionnel
Tout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.
 Mécontentement
Incompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec 
manifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte 
ne sont pas considérées contractuellement comme l’expression d’un mécontentement.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Marchandise
Tout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, 
à l’exception des biens véhiculés dans le cadre du transport public de marchandises.
 Matériel professionnel
Tout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.
 Mécontentement
Incompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec 
manifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte 
ne sont pas considérées contractuellement comme l’expression d’un mécontentement.
 Nullité
Annulation pure et simple du contrat qui est censé alors n’avoir jamais existé.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 
III – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 
IV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 
V - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 
VI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253]. The capacity of the BERT architecture is characterized by (i) the
DOI: 10.1201/9781003170082-3 43</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Matériel professionnel
Tout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.
 Mécontentement
Incompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec 
manifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte 
ne sont pas considérées contractuellement comme l’expression d’un mécontentement.
 Nullité
Annulation pure et simple du contrat qui est censé alors n’avoir jamais existé.
 Préposé
Toute personne travaillant pour le compte du souscripteur.
 Réclamation
Déclaration actant, par téléphone, par courrier, par courriel ou en face à face, le mécontentement d’un client 
envers l’assureur.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />121 -1 du Code des assurances,  
la clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  
garantissant les quadricycles à moteur.  
II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 
III – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 
IV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 
V - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 
VI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques. The Xie et al. taxonomy, based on foundational explainability
methods, restricted only to generic or speciﬁc transformer-based models,
is discussed in this section and as shown in Fig. 7.1.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].
7.1 TRAITS OF EXPLAINABLE SYSTEMS
One of the decisive goals of an explainable system is that it allows the
system’s end-users to understand the relationship between the input and
the output.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques. The Xie et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />sommaire
Protection Auto - Conditions générales EUR059H-0124
31Véhicule
Le véhicule de tourisme terrestre à moteur, d’un 
poids total autorisé en charge inférieur à 3,5 T (au -
tomobile ou moto > 125 cm3), immatriculé en France 
métropolitaine et désigné aux Conditions person -
nelles du contrat d’assurance Automobile, ainsi 
que la caravane ou remorque inscrite sur le contrat 
d’assurance si elle y est attelée au moment de 
l’évènement.
Sont exclus
les véhicules utilisés pour le transport  
de passagers à titre onéreux tels que taxis, 
ambulances, auto-écoles, corbillards, voitures 
de location.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !
Festivals .Fun .Beer .Lots ofbeer .Charlotte Center City
Partners (orFind Your Center )puts onalot ofthese types
offestivals Uptown and inSouth End .When you check out
their website ortheir weekly newsletter you ’llbeable to
seelots ofevents coming uplike Beer ,Blues and BBQ ,
Taste ofCharlotte ,Speedstreet and the like .\n\nMany of
these events and festivals usually have beer available ,
hence why I’mafan .And ,yeah ,Ialso really like
supporting the local organization that ’sresponsible for
Uptown ’sdevelopment .Ifonly there was aPBR festival ...!</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.
Loubes,Obtaining fairness using optimal transport theory , in
International Conference on Machine Learning, PMLR, 2019,
pp. 2357–2365.
[97]A.Graves ,Generating sequences with recurrent neural networks. ,
CoRR, abs/1308.0850 (2013).
[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-
chines, CoRR, abs/1410.5401 (2014).
[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.
Inf. Theory, 44 (1998), pp. 2325–2383.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To all the researchers and frontline COVID workers
for their extraordinary service.
– Uday Kamath, Kenneth L. Graham,
and Wael Emara
To my parents Krishna and Bharathi, my wife
Pratibha, the kids Aaroh and Brandy, my family and
friends for their support.
–Uday Kamath
To my wife Alyson, to my mother, my in-laws, my
family and friends, thank you for the support and your
willingness to sacriﬁce your time with me.
–Kenneth L. Graham
To my wife Noha, my parents Ali and Zainab, my
sister Wesam, my extended family and friends, thank
you all for being there for me all the time.
–Wael Emara</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■211
7.6.2 Data, Tools, and Libraries
Healthcare data usage has lots of restrictions and constraints due to
HIPAA privacy regulations. However, MTSamples.com collects tran-
scribed medical reports with sample transcription reports for forty spe-
cialties and work types to overcome these limitations and help transcrip-
tions be more accessible. Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Cross-lingual Word Recovery (CLWR) CLWR [125] uses par-
allel corpora to learn word alignments between two languages. The
pre-training task starts with a trainable attention matrix [17] that learns
to represent the source language token embeddings in terms of the tar-
get language token embeddings. The transformation learned in the at-
tention matrix is then provided to the cross-lingual model to learn to
reconstruct the source token embedding. Formally, given a parallel sen-
tence pair (x,y), wherex= (x1,x2,xm)is a sentence with mwords
from a source language s, andy= (y 1,y2,yn)is a sentence with n
words from a target language t. CLWR starts by representing each xias
xt
i∈Rhby all word embeddings of y[125]:
xt
i=n
j=1softmax (A ij)yt
j (4.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />4.1.2.8 ERNIE-M
Many techniques have shown the value of using parallel and monolin-
gual corpora to get better aligned cross-lingual representations. How-
ever, parallel corpora are always limited in size relative to monolingual
data, limiting the possible progress unless we collect sizable parallel cor-
pora. ERNIE-M [194] uses large monolingual corpora and limited par-
allel corpora to produce cross-lingual representations. To enhance pro-
ducedrepresentations,ERNIE-Mgeneratespseudo-parallelsentencesus-
ing the large available monolingual corpora and then uses them along
with the parallel data to improve the alignment of cross-lingual repre-
sentations. ERNIE-M proposes two pre-training tasks, Cross-Attention
masked Language Modeling (CAMLM) and Back-Translation Masked
Language Modeling (BTMLM).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />A(x) = 1 (7.13)
In general, ﬁnding anchors is infeasible in a continuous space, and
by introducing a parameter δ, the deﬁnition can be changed in
terms of probability to meet this precision threshold for some ar-
bitrarily small δas given by:
Pr [ρ(A)≥τ]≥1−δ (7.14)
Since there may be several anchors that meet this criterion, pref-
erence is given to the anchors with the highest coverage cov(A),
deﬁned as
cov(A) =ED(z)[A(z)] (7.15)
IfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration
then becomes a combinatorial optimization problem given by:
max
A∈A′cov(A) (7.16)
In practice, various heuristics-based approaches are used. For ex-
ample, a greedy search technique such as a bottom-up approach
where you start with an empty rule set and generate a set of can-
didate rules by iteratively extending an anchor by one additional
feature predicate creates this rule set in a greedy way.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance. Taking this
into account, Wang et al. [263] provide an explicit strategy for align-
ing the representations of matched word pairs across languages during
mBERT training. This is accomplished by including a loss function that
minimizes the Euclidean distance between aligned words’ embeddings.
Zhao et al. [298] also report comparable results when the representations
of word pairs are explicitly aligned and the vector spaces are further nor-
malized.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>23. How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on how the insurer ensures compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          To ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract, the insurer typically implements the following measures:

1. **Data Encryption**: The insurer encrypts personal information to protect it from unauthorized access or breaches.

2. **Access Control**: Access to personal information is restricted to authorized personnel only, and strict access control measures are in place.

3. **Data Minimization**: The insurer only collects and processes personal information that is necessary for providing assistance services, minimizing the data collected.

4. **Anonymization and Pseudonymization**: Personal data is anonymized or pseudonymized whenever possible to reduce the risk of identifying individuals.

5. **Consent Management**: The insurer obtains explicit consent from beneficiaries before collecting and processing their personal information for the assistance contract.

6. **Data Retention Policies**: The insurer establishes clear data retention policies to ensure that personal information is not kept longer than necessary for the purpose for which it was collected.

7. **Regular Audits and Monitoring**: Regular audits are conducted to ensure compliance with data protection regulations, and monitoring systems are in place to detect any unauthorized access or breaches.

8. **Employee Training**: Employees handling personal information receive training on data protection regulations and best practices to ensure proper handling of personal data.

9. **Data Processing Agreements**: The insurer may enter into data processing agreements with third-party service providers to ensure that personal information is handled in compliance with data protection regulations.

By implementing these measures, the insurer can ensure compliance with data protection regulations and safeguard the personal information of beneficiaries for the assistance contract.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />TaBERT is trained via content snapshots, in which the model only
encodes the bits of a table that are most relevant to a query. Due to the
enormous amount of rows in certain database tables, encoding them is a
computationallyintensiveandineﬃcientprocedure.Byencodingjustthe
portion of material that is most relevant to the utterance, content snap-
shots enable TaBERT to handle with enormous tables. For instance, the
phrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted
from the WikiTableQuestions dataset) may be accompanied with a table
includinginformationontheyear,venue,position,andevent.Asnapshot
of content will take a subset of three rows. This subset will not include
all of the table’s data, but it is suﬃcient for the model to understand
that, for example, the venue column comprises cities. TaBERT describes
table structure using a mix of traditional horizontal self-attention, which
captures the dependence between cells in speciﬁc rows, and vertical self-
attention, which catches the information ﬂow between cells in diﬀerent
rows.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 
III – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 
IV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 
V - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 
VI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This is followed by an enhanced understanding of the hierarchical
structure of sentences. Numerous studies have shown that the middle
BERT layers contain the most syntactic information and that the last
BERT levels include the most task-speciﬁc information. Additionally, it
was shown that, although the majority of syntactic information may be
localized in a few levels, semantic information is distributed across the
model, which explains why some non-trivial cases are initially handled
wrong then successfully at higher layers.
3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS
3.5.1 Goal
In this chapter, we looked at several applications of the Transformer
architecture. In this case study, we see how to use pre-trained (or ﬁne-
tuned) Transformer models to do topic modeling. If one is exploring a
newdataset,thismethodcouldbeusedduringexploratorydataanalysis.
3.5.2 Data, Tools, and Libraries
pip install -U datasets bertopic
Listing 3.1 Python environment setup</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Authors
Uday Kamath has spent more than two decades developing analyt-
ics products and combines this experience with learning in statistics,
optimization, machine learning, bioinformatics, and evolutionary com-
puting. He has contributed to many journals, conferences, and books, is
the author of XAI: An Introduction to Interpretable XAI, Deep Learn-
ing for NLP and Speech Recognition, Mastering Java Machine Learning,
and Machine Learning: End-to-End Guide for Java Developers . He held
many senior roles: chief analytics oﬃcer for Digital Reasoning, advisor
for Falkonry, and chief data scientist for BAE Systems Applied Intelli-
gence. Dr. Kamath has many patents and has built commercial products
using AI in domains such as compliance, cybersecurity, ﬁnancial crime,
and bioinformatics. He currently works as the chief analytics oﬃcer for
Smarsh.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■211
7.6.2 Data, Tools, and Libraries
Healthcare data usage has lots of restrictions and constraints due to
HIPAA privacy regulations. However, MTSamples.com collects tran-
scribed medical reports with sample transcription reports for forty spe-
cialties and work types to overcome these limitations and help transcrip-
tions be more accessible. Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Then we tokenize the sentences, convert the words to token IDs, and
append <bos> and <eos> IDs to the beginning and end of the token
ID sequences. Padding the variable-length sequences to the maximum
observed length in the batch using the <pad> token ensures a ﬁxed-
size tensor for training and evaluation.
The total of 135,842language pairs after ﬁltering reduce to 131,951
and we further split it into 80% training, 10% validation and 10% test
data, i.e., 105,460, 13,308, and 13,183respectively.
Figs. 2.13 and 2.14 show the distribution plots as histograms for En-
glish/French and joint distribution. Most of the sentences in the parallel
corpus are between 4and8tokens/words length.
Figure 2.13 SentencelengthdistributionforEnglishandFrenchsentences.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />All these functional units
get detailed treatment from a theoretical and practical perspec-
tives for the readers to get a complete handle on the topic. Finally,
a real-world case study using transformers for machine translation
tasks showing the operative aspects concludes the chapter.
3. The advent of BERT has revolutionized the ﬁeld of natural lan-
guage processing (NLP) and helped to get close to human-level
performance in many conventionally challenging tasks. Chapter
3 introduces the details of the BERT architecture and how it is
pre-trained and ﬁne-tuned for classical NLP tasks such as sin-
gle/pair text classiﬁcation, token tagging, and question answer-
ing. The chapter also discusses the ﬁeld of BERTology, which is
research related to the inner workings of BERT and how it pro-
cesses and analyzes text and information. Finally, the chapter in-
troduces some deep learning architectures that modify BERT for
more eﬃciency (e.g., RoBERTa) and other types of NLP appli-
cations (e.g., NLP for tabular data—TaBERT).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.6.2 Data, Tools, and Libraries
We chose the Google Play application review dataset. The dataset in-
cludes 15,746 samples in three categories; namely negative, neutral, and
positive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-
tuning task as well as standard Python data science stack for all other
data handling and visualization [249] (Figs. 3.9–3.11).
We use BERT BASEas the starting pre-trained BERT mode. Listing
3.6.3 shows how to load the pre-trained BERT model from the Trans-
(a)
 (b)
Figure 3.9 Exploratory data analysis of the sentiment classiﬁcation
dataset.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Dr. Kamath has many patents and has built commercial products
using AI in domains such as compliance, cybersecurity, ﬁnancial crime,
and bioinformatics. He currently works as the chief analytics oﬃcer for
Smarsh. He is responsible for data science, research of analytical prod-
ucts employing deep learning, transformers, explainable AI, and modern
techniques in speech and text for the ﬁnancial domain and healthcare.
Kenneth L. Graham has two decades experience solving quantita-
tive problems in multiple domains, including Monte Carlo simulation,
NLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the
past ten years, he has focused on building scalable solutions in NLP for
government and industry, including entity coreference resolution, text
classiﬁcation, active learning, automatic speech recognition, and tempo-
ral normalization. He currently works at AppFolio as a senior machine
learning engineer. Dr.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />121 -1 du Code des assurances,  
la clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  
garantissant les quadricycles à moteur.  
II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 
III – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 
IV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 
V - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 
VI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[20]S. Barocas and D. Boyd ,Engaging the ethics of data science
in practice , Communications of the ACM, 60 (2017), pp. 23–25.
[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-
box models via model extraction , arXiv preprint arXiv:1705.08504,
(2017).
[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci
trustworthiness and how to address them , in Proceedings of the 1st
Workshop on Explainable Computational Intelligence (XCI 2017),
2017.
[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and
alternatives , arXiv preprint arXiv:2102.12452, (2021).
[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-
guage model for scientiﬁc text , in EMNLP, 2019.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Interpretability and Explainability Techniques for Transformers ■211
7.6.2 Data, Tools, and Libraries
Healthcare data usage has lots of restrictions and constraints due to
HIPAA privacy regulations. However, MTSamples.com collects tran-
scribed medical reports with sample transcription reports for forty spe-
cialties and work types to overcome these limitations and help transcrip-
tions be more accessible. Kaggle’s medical transcriptions dataset and the
classiﬁcation task are based on this dataset. The medical-nlp project fur-
ther transforms the data into four labels or specialities (Surgery, Medical
Records, Internal Medicine, and Other) using the medical taxonomy. We
willusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.
We use pandas for basic text processing and exploratory data anal-
ysis. Sklearn library is used for traditional NLP pipeline and logistic
regression model.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[26]E. M. Bender, D. Hovy, and A. Schofield ,Integrating
ethics into the nlp curriculum , in Proceedings of the 58th Annual
MeetingoftheAssociationforComputationalLinguistics:Tutorial
Abstracts, 2020, pp. 6–9.
[27]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle ,
Greedy layer-wise training of deep networks , in Proceedings of the
19th International Conference on Neural Information Processing
Systems, NIPS’06, MIT Press, 2006, pp. 153–160.
[28]Y. Bengio and Y. LeCun ,Scaling learning algorithms towards
AI, MIT Press, 2007.
[29]G. Bertasius, H. Wang, and L. Torresani ,Is space-time
attention all you need for video understanding? , in ICML, 2021.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].
3.Trust. Dependable models are the ones that do not need valida-
tion. It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Foreword
Renowned AI pioneer and Nobel laureate Herbert Simon underscored
“attention” as the most valuable resource of the information econ-
omy, as necessary to allocate attention eﬃciently among the over-
abundance of information resources. Having written the foundational
paper on meaning-aware AI and recently having served as MIT-
Princeton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-
lishing by invitation in the same journal’s special issue of ASQ, and of
being the Malcolm Baldrige National Quality Award administrator, as
well as being ranked along with Dr. Simon in the same global academic
citation impact studies.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>24. Are damages resulting from climatic events covered under the "Garantie Dommages tous accidents"?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on whether damages resulting from climatic events are covered under the "Garantie Dommages tous accidents".<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The "Garantie Dommages tous accidents" typically covers damages resulting from various accidents, such as fire, water damage, vandalism, and theft. However, coverage for damages resulting from climatic events may vary depending on the specific terms and conditions of the insurance policy. 

It is essential to carefully review your insurance policy or consult with your insurance provider to determine if damages resulting from climatic events, such as storms, floods, or other natural disasters, are covered under the "Garantie Dommages tous accidents" policy. In some cases, additional coverage or specific insurance policies may be required to protect against damages caused by climatic events.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />BERT uses unlabeled text
to pre-train deep bidirectional contextual representations. This resulted
in rich pre-trained language models that can be ﬁne-tuned with a simple
additional output layer and a reasonably sized labeled dataset to pro-
duce state-of-the-art performance in a broad range of NLP tasks. These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Resulting advances
in enhanced parallel processing of sequential data have made eﬃcient
context sensitive and hence more “meaningful” for ever-larger datasets
and much more feasible than earlier.
Covering the latest advances in neural network architectures related
to transformers spanning applications such as Natural Language Pro-
cessing (NLP), speech recognition, time series analysis, and computer
vision and domain-speciﬁc models spanning science, medicine, and ﬁ-
nance, the book aims to meet the theoretical, research, application, and
practical needs across academia and industry for multiple audiences in-
cluding postgraduate students and researchers, undergraduate students,
industry practitioners, and professionals. The book rounds oﬀ its theory-
driven applied and practical coverage with hands-on case studies with
xvii</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Since BERT, many ﬂavors of pre-trained models have been made
available across diﬀerent domains, providing models that can be
ﬁne-tuned to domain-speciﬁc data across science, medicine, and
ﬁnance. In addition, language-speciﬁc pre-trained models oﬀer in-
creasingly competitive results on downstream language speciﬁc
tasks. In Chapter 6, we discuss the pre-trained models that are
available, showing their beneﬁts and applications to speciﬁc do-
mains such as computer vision, speech, time series, and text. This
chapter includes a case study that compares the performance of
three transformer-based automatic speech recognition models.
7. There is a need to understand the models from an explainability
standpoint in many critical applications and given the black-box
nature of transformers-based models. In Chapter 7, we will cover
the traits of the models that address explainability, related areas
that impact explainability, the taxonomy of explainable methods
applied tothe transformer-based and attention-based systems, and
ﬁnally,adetailedcasestudyintheelectronichealthrecordsystems
using transformers with diﬀerent explainable techniques to become
more practical.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />These
developments lowered the barrier of entry for the wide adoption of these
powerful pre-trained models. It is now common practice in the artiﬁcial
intelligence ﬁeld to share pre-trained models and ﬁne-tune them with
minimal cost as opposed to the old paradigm of designing task-speciﬁc
architectures. In this chapter, we cover BERT’s basic design concepts,
developments, and applications.
3.1 BERT
3.1.1 Architecture
Core layers One of the major contributions of BERT is the simplicity
of design and the diversity of covered downstream tasks. BERT’s archi-
tecture consists of multi-layers of the bidirectional transformer encoder
[253]. The capacity of the BERT architecture is characterized by (i) the
DOI: 10.1201/9781003170082-3 43</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].
7.1 TRAITS OF EXPLAINABLE SYSTEMS
One of the decisive goals of an explainable system is that it allows the
system’s end-users to understand the relationship between the input and
the output.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />C H A P T E R 7
Interpretability and
Explainability
Techniques for
Transformers
Incritical applications in domains like healthcare, legislation, law en-
forcement, or ﬁnancial, in addition to the predictions, there is a need
to understand the models from an explainability standpoint. Unfortu-
nately, one can categorize most state-of-the-art transformer models and
techniques covered in this book as a “black box” that can signiﬁcantly
impede adoption. Therefore, there is a paramount need for building ex-
plainability around these complex state-of-the-art models from both an
understanding and diagnosis perspective. As proposed by Xie et al., we
will cover the traits of the models that address explainability, related
areas that impact explainability, the taxonomy of explainable meth-
ods applied to transformer-based and attention-based systems, and ﬁ-
nally, a detailed case study in the electronic health record systems using
transformers with diﬀerent explainable techniques to get practical in-
sights [275].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Simon in the same global academic
citation impact studies.
Given the above background, I am thrilled to share with you the
most thorough and up-to-date compendium of research, practices, case
studies, and applications available today that can provide the best ROI
on the latest AI technological advances on transformers inspired by the
paper, “Attention is All You Need.” Since Google introduced transformer
architecture in 2017, transformers have provided exponential improve-
ments in context-focused realization toward meaning-aware AI as deep
(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch
as dot-product attention and multi-head attention. Resulting advances
in enhanced parallel processing of sequential data have made eﬃcient
context sensitive and hence more “meaningful” for ever-larger datasets
and much more feasible than earlier.
Covering the latest advances in neural network architectures related
to transformers spanning applications such as Natural Language Pro-
cessing (NLP), speech recognition, time series analysis, and computer
vision and domain-speciﬁc models spanning science, medicine, and ﬁ-
nance, the book aims to meet the theoretical, research, application, and
practical needs across academia and industry for multiple audiences in-
cluding postgraduate students and researchers, undergraduate students,
industry practitioners, and professionals.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />6008–6018.
[161]J. Libovick `y, R. Rosa, and A. Fraser ,How language-neutral
is multilingual bert? , arXiv preprint arXiv:1911.03310, (2019).
[162]T. Limisiewicz, D. Marecek, and R. Rosa ,Universal de-
pendencies according to BERT: both more speciﬁc and more gen-
eral, in Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: Findings, EMNLP 2020, Online
Event, 16–20 November 2020, T. Cohn, Y. He, and Y. Liu, eds.,
vol. EMNLP 2020 of Findings of ACL, Association for Computa-
tional Linguistics, 2020, pp. 2710–2722.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Bidirectional Encoder Representations from Transformers (BERT) ■63
(’liquor’, 0.005587858949299897),
(’center city’, 0.005496678910160935),
(’beers’, 0.005368697666709216),
(’events’, 0.005089779403417317)]
Listing 3.12 Median topic: Nightlife in Charlotte, NC
3.6 CASE STUDY: FINE-TUNING BERT
3.6.1 Goal
The goals of this case study is to provide a step-by-step demo of ﬁne-
tuning a standard BERT model for any sentence classiﬁcation, we chose
sentiment classiﬁcation as a sample popular task.
3.6.2 Data, Tools, and Libraries
We chose the Google Play application review dataset.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />We show a partial
list of modiﬁed transformers in Table 1.1.
1.2.1.2 Transformer sublayer changes
In Chapter 2, we’ll take a detailed look at the structure of a trans-
former block, covering its four components so we can later discuss ways
in which researchers have modiﬁed them. In general, there are four parts
to a transformer block [254]: positional encodings, multi-head attention,
residual connections with layer normalization [13], and a position-wise
feedforward network. Changes to transformer sublayers have focused on</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and
P. Frossard ,Universal adversarial perturbations , in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017, pp. 1765–1773.
[190]N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and
J. F. Allen ,LSDSem 2017 shared task: The story cloze test , in
LSDSem@EACL, 2017.
[191]A. Nguyen, J. Yosinski, and J. Clune ,Deep neural networks
are easily fooled: High conﬁdence predictions for unrecognizable im-
ages, in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2015, pp. 427–436.
[192]J. Nivre, M. Abrams, š. Agić, L. Ahrenberg, L. Anton-
sen, K. Aplonova, M. J. Aranzabe, G. Arutie, M. Asa-
hara, L. Ateyah, M. Attia, and et.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />, in Proceedings of the 57th Conference of the
Association for Computational Linguistics, ACL 2019, Florence,
Italy, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-
nen, D. R. Traum, and L. Màrquez, eds., Association for Compu-
tational Linguistics, 2019, pp. 4996–5001.
[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,
and A. Korhonen ,XCOPA: A multilingual dataset for causal
commonsense reasoning , in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
2020.
[201]O. Press, N. A. Smith, and O. Levy ,Improving transformer
models by reordering their sublayers , in ACL, 2020.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />557–575.
[199]T. Pires, E. Schlinger, and D. Garrette ,How multilingual
is multilingual bert? , in Proceedings of the 57th Conference of the
Association for Computational Linguistics, ACL 2019, Florence,
Italy, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-
nen, D. R. Traum, and L. Màrquez, eds., Association for Compu-
tational Linguistics, 2019, pp. 4996–5001.
[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,
and A. Korhonen ,XCOPA: A multilingual dataset for causal
commonsense reasoning , in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
2020.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Learn-
ing Representations by Back-propagating Errors, pp. 696–699.
[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-
GAN: Protecting classiﬁers against adversarial attacks using gen-
erative models , arXiv preprint arXiv:1805.06605, (2018).
[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,
wav2vec: Unsupervised pre-training for speech recognition , in IN-
TERSPEECH, 2019.
[220]M. Schuster and K. Nakajima ,Japanese and Korean voice
search, 2012 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), (2012), pp. 5149–5152.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Ablation Studies Several ablation studies have been proposed to ex-
amine several hypotheses about the validity of language-agnostic repre-
sentations in multilingual transformers. First hypothesis tested in [199]
is that the joint script between high resource language is a confounding
factor for good multilingual transformers performance. However this was
shown to be true as multilingual transfer occur between languages that
donotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-
vanagari script [199]. Other work [229] examined the input tokenization
as a confounding factor and found that using subword tokenization bi-
asesthemodelstowardslearninglanguage-agnosticrepresentationsmore
than word-level or character-level tokenization. Pre-training tasks has
also been examined as a possible confounding factor, it was shown that
models trained with parallel corpora pre-training tasks such as XLM re-
tain language-agnostic representations within the higher encoder layers
in a stronger manner than models trained on monolingual corpora such
as mBERT and XLMR [52].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Learn-
ing Representations by Back-propagating Errors, pp. 696–699.
[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-
GAN: Protecting classiﬁers against adversarial attacks using gen-
erative models , arXiv preprint arXiv:1805.06605, (2018).
[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,
wav2vec: Unsupervised pre-training for speech recognition , in IN-
TERSPEECH, 2019.
[220]M. Schuster and K. Nakajima ,Japanese and Korean voice
search, 2012 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), (2012), pp. 5149–5152.
[221]H. Schwenk and X. Li ,A corpus for multilingual document
classiﬁcation in eight languages , in Proceedings of the Eleventh
International Conference on Language Resources and Evaluation
(LREC 2018), N. C. C. chair), K. Choukri, C. Cieri, T. De-
clerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani,
H. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga, eds.,
Paris, France, may 2018, European Language Resources Associa-
tion (ELRA).</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>25. What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (assistance pump) are the internal parts of these components. However, conduits and durits (hoses and pipes) are excluded from the warranty.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (power steering pump) in the document include:

- Crémaillère (rack and pinion)
- Vérins de direction (steering cylinders)
- Pompe d’assistance (power steering pump)

These components are part of the steering system and are covered under the warranty according to the document.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />électr ovanne pomp e injection,  moteur
pas à pas, potentiomètre accélérateur, boîtier
papillon,  régulateur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduits et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />électr ovanne pomp e injection,  moteur
pas à pas, potentiomètre accélérateur, boîtier
papillon,  régulat eur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduit s et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />potentiomètre accélérateur, boîtier
papillon,  régulateur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduits et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />boîtier
papillon,  régulat eur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduit s et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />régulateur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduits et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,
ﬂexibles et tuyaux)
Exclu
La main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses
garanties selon le barème du constructeur .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />régulat eur de débit
Alternateur, régulateur de tension, démarreur, balais
lanceurs, moteur essuie-glaces, moteur lève-vitres,
moteur toit ouvrant, moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduit s et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,
ﬂexibles et tuyaux)
Exclu
La main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses
garanties selon le barème du constructeur .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduits et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,
ﬂexibles et tuyaux)
Exclu
La main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses
garanties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49
48A - Etendue des garanties (suite)
G Garantie méc anique initiale
Les organes concernés :
GMoteur (pièc es lubriﬁées en mouv ement)
GTurbo compr esseur 
GBoîte de vitesses (pièces lubriﬁées en mouvement)
GBoîte automatique (pièces lubriﬁées en mouvement)
GPont (pièc es lubriﬁées à l’intérieur du pont)
GTransmission
GAutr es organesListe exhaustiv e des pièc es garanties
dans les organes :
Chemises,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />moteur électromagnétique des
portes, ventilateur de chauﬀage et résistance,
bobine d’allumage, allumeur, calculateur injection
Arbre longitudinal (sauf ﬂectors), transmissions, souﬄets
Cylindres hydrauliques de suspension (sauf sphères
et amor tisseurs,  joints,  conduit s et durits,
électrovanne et régulateurs).
Crémaillère, vérins de direction, pompe d’assistance
(sauf conduits et durits).
Déshydrateur et compresseur (sauf recharges,
ﬂexibles et tuyaux)
Exclu
La main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses
garanties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49
48A - Etendue des garanties (suite)
G Garantie méc anique initiale
Les organes concernés :
GMoteur (pièc es lubriﬁées en mouv ement)
GTurbo compr esseur 
GBoîte de vitesses (pièces lubriﬁées en mouvement)
GBoîte automatique (pièces lubriﬁées en mouvement)
GPont (pièc es lubriﬁées à l’intérieur du pont)
GTransmission
GAutr es organesListe exhaustiv e des pièc es garanties
dans les organes :
Chemises,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />La direction : crémaillère, colonne de direction, système 
de commande électrique et hydraulique, à  l’exception du 
contrôle et réglage du train avant.
    Les freins : groupe hydraulique ABS, câble de frein à main, 
cylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.
    La suspension :  axes de pivots et bagues, axes, barres 
stabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs .
  L’alimentation :  pompe à carburant, pompe d’injection, 
turbocompresseur, distributeur, débitmètre d’air, injecteurs.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />La transmission :  tous les organes et pièces internes, y 
compris les arbres, boîte de transfert et du réducteur 4x4, les roulements de roue/moyeux et les joints spi.
    La direction : crémaillère, colonne de direction, système 
de commande électrique et hydraulique, à  l’exception du 
contrôle et réglage du train avant.
    Les freins : groupe hydraulique ABS, câble de frein à main, 
cylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.
    La suspension :  axes de pivots et bagues, axes, barres 
stabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■29
Figure 2.14 Joint distribution for English and French sentences based on
length.
The top 20words for English and French are shown in Fig. 2.15. The
frequency and distribution shows some common words as expected such
as “the-le”, “is-est”, etc. It also highlights certain contractions such as
“don” for “don’t” and how the underlying tokenizer (spaCy) performs
preprocessing. Iteratively improving and analyzing the data helps in the
overall model results.
Bahdanau attention-based sequence-to-sequence and transformer-
based will be used to train/validate on the data and the best model
from both will be evaluated on the test set.
2.5.3.2 Attention
The encoder model class BahdanauEncoder as given in the Listing
2.1 uses a bidirectional gated recurrent unit (GRU) to encode sentences
in the source language.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■37
(a) Example showing the En-
glish word “going” pays atten-
tion to “je” and “vais”.
(b) Example showing the En-
glishword“be”paysattentionto
“en” and “etre”.
(c) English word “library” pays
attention to multiple words.
(d) The words “two” and “deux”
are matched.
Figure 2.17 Attention examples and plots.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />8440–8451.
[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,
and M. Baroni ,What you can cram into a single vector: Prob-
ing sentence embeddings for linguistic properties , arXiv preprint
arXiv:1805.01070, (2018).
[63]A. Conneau and G. Lample ,Cross-lingual language model
pretraining , in Advances in Neural Information Processing Sys-
tems32:AnnualConferenceonNeuralInformationProcessingSys-
tems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,
Canada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-
Buc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.
[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.
Bowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating
cross-lingual sentence representations , in Proceedings of the 2018
Conference on Empirical Methods in Natural Language Process-
ing, Association for Computational Linguistics, 2018.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />228■Bibliography
Tetreault, eds., Association for Computational Linguistics, 2020,
pp. 8440–8451.
[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,
and M. Baroni ,What you can cram into a single vector: Prob-
ing sentence embeddings for linguistic properties , arXiv preprint
arXiv:1805.01070, (2018).
[63]A. Conneau and G. Lample ,Cross-lingual language model
pretraining , in Advances in Neural Information Processing Sys-
tems32:AnnualConferenceonNeuralInformationProcessingSys-
tems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,
Canada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-
Buc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp. 599–604.
[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel ,Backpropagation
applied to handwritten zip code recognition , Neural Computation,
1 (1989), pp.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Transformers: Basics and Introduction ■41
Figure 2.21 Explaining translations for—It is snowing at my house.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,
and W. Samek ,Analyzing classiﬁers: Fisher vectors and deep
neural networks , in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 2912–2920.
[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,
From zero to hero: On the limitations of zero-shot cross-lingual
transfer with multilingual transformers , CoRR, abs/2005.00633
(2020).
[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil
asymmetrique (a learning scheme for asymmetric threshold net-
works), in Proceedings of Cognitiva 85, 1985, pp. 599–604.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />40■Transformers for Machine Learning: A Deep Dive
Figure 2.20 Explaining translations for—How long you will be here?</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>26. What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The insurer can take several actions if the policyholder fails to fulfill their obligations under the contract. These can include:

1. Cancellation of the policy: The insurer can choose to cancel the policy, effectively ending the coverage. This is usually done after a certain period of non-compliance by the policyholder.

2. Denial of claim: If the policyholder fails to fulfill their obligations and then makes a claim, the insurer can deny this claim. This is often the case if the policyholder has not paid their premiums or has violated terms of the contract.

3. Legal action: In some cases, the insurer may choose to take legal action against the policyholder. This is usually a last resort and is often used in cases of fraud or misrepresentation.

4. Increase in premiums: The insurer may also choose to increase the policyholder's premiums as a result of their failure to fulfill their obligations.

5. Modification of the contract: In some cases, the insurer may choose to modify the terms of the contract to account for the policyholder's failure to fulfill their obligations.

It's important to note that the specific actions taken can vary depending on the terms of the insurance contract and the laws of the jurisdiction in which the contract was issued.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          If the policyholder fails to fulfill their obligations under the contract, the insurer can take the following actions:

1. **Nullity of the Contract**: The insurer may declare the contract null and void if the policyholder has made a false declaration of risk with the intention to deceive the insurer. This nullity can occur even if the false declaration did not impact any claims (Article L.113-8 of the Insurance Code).

2. **Vices of Consent**: If there are vices of consent such as error, fraud, or violence that are significant enough to influence the decision to enter into the contract, the insurer may consider the contract as if it never existed (Articles 1130 to 1144 of the Civil Code).

3. **Franchise**: The insurer may apply a deductible amount (franchise) that the policyholder is responsible for paying before receiving any indemnity after a claim. The policyholder is prohibited from insuring the portion of the risk covered by the deductible.

4. **Obligation to Declare**: The policyholder is obligated to report any claim that may trigger coverage to the insurer or their local representative as soon as they become aware of it and within a specified timeframe.

5. **Multiple Insurances**: If the policyholder has multiple insurance policies that could cover the same damages, they must declare the existence of these policies to the insurers involved in case of a claim.

Failure to fulfill these obligations by the policyholder may lead to consequences such as denial of coverage, reduction of benefits, or legal actions by the insurer.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />CONDITIONS GÉNÉRALES  7 
Nullité du contrat  
Mesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  
 fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. 
Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de 
la fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  
Exemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de 
la fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  
Exemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  
 vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, 
l’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  
Le contrat est considéré comme n’ayant jamais existé.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.
For oﬄine RL, the trajectories in the dataset are of the form
(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)
where ˆRt=∑T
t′=trt′isthe“return-to-go”,whichistheamountofreward
that needs to be generated to get to the end of the trajectory from t.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de 
la fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  
Exemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  
 vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, 
l’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  
Le contrat est considéré comme n’ayant jamais existé.  
 
Outillage  
Tous out ils ou matériels nécessaires à l’exercice de l’activité professionnelle de l’assuré.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />178■Transformers for Machine Learning: A Deep Dive
ronment can take. The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />CONDITIONS GÉNÉRALES  7 
Nullité du contrat  
Mesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  
 fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. 
Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de 
la fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  
Exemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  
 vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, 
l’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Franchise
Nonobstant toute disposition contraire, l’assuré conserve à sa charge une partie de l’indemnité due après sinistre. Il 
s’interdit de contracter une assurance pour la portion du risque constituée par la franchise.
Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque 
véhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise 
prévue par le contrat, si celle-ci est supérieure.
Obligation de l’assuré
L’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il 
en a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de 
catastrophe naturelle.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Franchise
Nonobstant toute disposition contraire, l’assuré conserve à sa charge 
une partie de l’indemnité due après sinistre. Il s’interdit de contracter une 
assurance pour la portion du risque constituée par la franchise.Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant 
de la franchise est de 380 € pour chaque véhicule endommagé. Toutefois, 
pour les véhicules terrestres à moteur à usage professionnel, tel que prévu 
à l’article 9 sera appliquée la franchise prévue par le contrat, si celle-ci est 
supérieure.
Obligation de l’assuré
L’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre 
susceptible de faire jouer la garantie dès qu’il en a connaissance et au 
plus tard dans les dix jours suivant la publication de l’arrêté interministériel 
constatant l’état de catastrophe naturelle.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Il 
s’interdit de contracter une assurance pour la portion du risque constituée par la franchise.
Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque 
véhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise 
prévue par le contrat, si celle-ci est supérieure.
Obligation de l’assuré
L’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il 
en a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de 
catastrophe naturelle.
Quand plusieurs assurances contractées par l’assuré peuvent permettre la réparation des dommages matériels directs 
non assurables résultant de l’intensité anormale d’un agent naturel, l’assuré doit, en cas de sinistre et dans le délai 
mentionné au précédent alinéa, déclarer l’existence de ces assurances aux assureurs intéressés.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />178■Transformers for Machine Learning: A Deep Dive
ronment can take. The agent and the environment interact at a sequence
of time steps t= 0,...,T . At eacht, the agent gets the environment’s
statest∈Sand then chooses an action at∈A(s). At time t+ 1, the
agent receives a reward from the environment rt+1∈Rand the environ-
ment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.
Note that generally speaking, the reward will be a function of the state
and action, rt=R(st,at). As time progresses, a sequence of states, ac-
tions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.
There is one important diﬀerence between the ﬁnite Markov decision
process outlined above and the process used for Decision Transformer.
Decision Transformer is applied to a type of RL called oﬄine RL. In
oﬄine RL, instead of having the agent interact with an environment and
be given a state update after taking an action and getting the reward,
there is a ﬁxed dataset that contains trajectories drawn from arbitrary
policies. This is a harder way for the agent to learn.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />This
sequence can be called a trajectory.
In practice, rtandstare random variables with probability distribu-
tions. The probability of moving to state s′and earning reward r, given
that action awas taken in state sis given by
p(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)
s′∈S
r∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)
Thetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-
ronment and the agent does not know the set of rewards or the transition
probabilities.
The task of RL is to learn the probability that the agent chooses
actionawhile in state s. This probability distribution is called a policy.
6.7.1 Decision Transformer
Decision Transformer [41] is the result of an attempt to replace rein-
forcement learning (RL) methods with a transformer that models the
sequence of states, actions, and rewards that is used to train an au-
tonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks
to which most transformers are applied.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The loss function Lprovides a metric of how close the explanation
model approximates the real model fin the locality of instance x.
•SHapley Additive exPlanations (SHAP): Shapley values
have foundations in game theory, particularly cooperative game
theory. One can view each feature as a player in a game, and the
goal is to distribute payouts fairly among players who form a coali-
tion based on their contribution to the total payout. The marginal
contribution of each feature averaged across the set of all possible
coalitions of features gives the Shapley values.
Let us represent the entire deep learning model through function
f, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj
for thej-th feature is given by
φj(f) =|S|!(d−|S|−1)!
d![f(S∪{xj})−f(S)](7.12)</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />\n\nThere ’splenty ofseating onboth the first and second
floors and they have arespectable ,ifpredictable ,Scotch
selection that helps mekeep them inmind when I’m
downtown ,soI’msure the rest ofthe liquor options are
pretty good .Coming from Illinois where you can buy booze
atgrocery stores ,it’sdepressing towalk into abar in
Charlotte and only have one ortwo choices .Cosmos expands
thehorizons and Iappreciate that .
Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>27. What are the consequences of not respecting the deadlines for reporting a claim?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on the consequences of not respecting the deadlines for reporting a claim.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          Not respecting the deadlines for reporting a claim can have several consequences, which may vary depending on the specific insurance policy and the circumstances surrounding the claim. Here are some common consequences:

1. **Loss of Coverage:** Failing to report a claim within the specified deadline may result in the insurance company denying coverage for that claim. This means you may have to bear the financial burden of the loss or damage on your own.

2. **Delayed Processing:** Late reporting can lead to delays in the processing of your claim. This can prolong the time it takes to receive compensation or assistance from the insurance company.

3. **Evidence Collection Challenges:** The longer you wait to report a claim, the more difficult it may be to gather evidence to support your claim. This can weaken your case and may result in a lower settlement amount.

4. **Legal Issues:** In some cases, failing to report a claim on time could lead to legal complications. It may breach the terms of your insurance contract, and the insurance company may take legal action or refuse to renew your policy in the future.

5. **Loss of Rights:** Insurance policies often have specific clauses regarding the reporting of claims. If you fail to adhere to these clauses, you may lose certain rights or benefits that you would have otherwise been entitled to.

6. **Increased Premiums:** Late reporting of claims can also impact your future insurance premiums. Insurance companies may view delayed reporting as a sign of increased risk, leading to higher premiums when you renew your policy.

It is essential to familiarize yourself with the reporting requirements outlined in your insurance policy and ensure timely reporting of any claims to avoid these potential consequences.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance. Taking this
into account, Wang et al. [263] provide an explicit strategy for align-
ing the representations of matched word pairs across languages during
mBERT training. This is accomplished by including a loss function that
minimizes the Euclidean distance between aligned words’ embeddings.
Zhao et al. [298] also report comparable results when the representations
of word pairs are explicitly aligned and the vector spaces are further nor-
malized.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Similartothepermutationattentionweightsexperiments,theresults
for the adversarial attention experiments also varied based on the data.
The attention distribution does not uniquely characterize why a par-
ticular prediction was made by the model as you can ﬁnd alternative
attention heatmaps that give the same predictions .
The overall conclusion drawn from the research was—attention does
not provide a consistent interpretation of why a model made a particular
prediction.
7.4.2 Attention is Not Not an Explanation
Wiegreﬀe and Pinter contest the claims and assumptions made by Jain
and Wallace in the research above and show through experiments the
usefulness of attention mechanisms for explainability [269]. Wiegreﬀe
and Pinter’s main contention from Jain and Wallace’s research is that
their “explanation” study is ambiguous, correlation experiments are in-
suﬃcient, and the adversarial weight experiments were vague and im-
practical.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They demonstrate how knowledge distillation from a monolingual
model may enhance a mLM’s cross-lingual performance.
4.3.1.3 Model tasks factors
Fine-tuning Strategies In [171] it is discussed that ﬁne-tuning a
mLM changes its parameters, impairing its cross-lingual capacity by
erasing part of the alignments learned during pre-training. They demon-
strate this by showing that when a mLM is ﬁne-tuned for POS tagging,
its cross-lingual retrieval performance decreases signiﬁcantly. To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained. They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They demon-
strate this by showing that when a mLM is ﬁne-tuned for POS tagging,
its cross-lingual retrieval performance decreases signiﬁcantly. To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained. They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance. Taking this
into account, Wang et al. [263] provide an explicit strategy for align-
ing the representations of matched word pairs across languages during
mBERT training.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained. They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance. Taking this
into account, Wang et al. [263] provide an explicit strategy for align-
ing the representations of matched word pairs across languages during
mBERT training. This is accomplished by including a loss function that
minimizes the Euclidean distance between aligned words’ embeddings.
Zhao et al.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />They demonstrate how knowledge distillation from a monolingual
model may enhance a mLM’s cross-lingual performance.
4.3.1.3 Model tasks factors
Fine-tuning Strategies In [171] it is discussed that ﬁne-tuning a
mLM changes its parameters, impairing its cross-lingual capacity by
erasing part of the alignments learned during pre-training. They demon-
strate this by showing that when a mLM is ﬁne-tuned for POS tagging,
its cross-lingual retrieval performance decreases signiﬁcantly. To over-
come this issue, they recommend utilizing a continual learning frame-
work for ﬁne-tuning the model so that it does not forget the original
task (MLM) on which it was trained. They claim improved results in
cross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-
tuning method.
Representations Alignment In [261, 168] the performance of zero-
shot cross-lingual transfer is examined using implicitly aligned represen-
tations learned by mLMs and representations from monolingual models
thataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote
that the explicit alignment provide improved performance.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />2 DIRECT ASSURANCE AUTOGUIDE 
DE VOS 
GARANTIES
Sans valeur contractuelle
Pour le détail de vos garanties vous devez  
vous reporter aux conditions générales page 13</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Le contr at est susp endu
automatiquement à partir du lendemain à 0 heur e du
jour de la cession.
que l’héritier nous avise dans les dix jours du transfer t de
propriété du véhicule à son nom.  L’assur ance est alors
transférée automatiquement,  s’il peut devenir sociétair e*
de la Macif et à charge pour lui de respecter les
obligations du contr at.
Si un sinistr e* surv enait après le transfer t de propriété et
avant que nous en ayions connaissanc e, l’indemnité due
pourr ait être réduite dans la proportion de la cotisation
versée par rapport à celle due.M M
Vous pouvez vous reporter, pour les cas envisagés ci-dessus,  aux disp ositions relativ es aux
possibilités de résiliation ﬁgur ant au chapitr e 3 « Vie du contr at », au paragraphe «Fin du
contr at».M1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page58</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Need aplace inCharlotte toget black out drunk without fear
ofwaking upmissing vital organs ?Than the Westin
Charlotte isthe place for you .Was witness toanamateur
drunk who passed out .Rather than doing the typical bounce
andtoss the staff gave genuine care and concern tothe
help him through the this moment ofself reflection .One
employee went sofar astoice his neck inhope tohelp him
sober up,ifnothing else atleast remeber his name .Even
when the EMTs arrived and drunk punches were thrown the
staff stood stoically ready toshow their support for their
customer .So,itbecame adinner and ashow .Five stars .
NERD RAGE !</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />It has been shown that models with high conﬁdence in pre-
diction do not guarantee trustworthiness [95, 191,189,291]. Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4]. Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Foreword
Renowned AI pioneer and Nobel laureate Herbert Simon underscored
“attention” as the most valuable resource of the information econ-
omy, as necessary to allocate attention eﬃciently among the over-
abundance of information resources. Having written the foundational
paper on meaning-aware AI and recently having served as MIT-
Princeton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-
lishing by invitation in the same journal’s special issue of ASQ, and of
being the Malcolm Baldrige National Quality Award administrator, as
well as being ranked along with Dr. Simon in the same global academic
citation impact studies.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To test the hypothesized ro-
bustness, they authors add six kinds of noise to the data and see how it
aﬀects performance:
1. Make deletions by giving each character has a 10%chance of being
deleted.
2. Give each character a 10%chance of added, deleted, or mutated
(with equal likelihood).
3. Give each character a 20%chance of being duplicated 1-3 times.
4. Capitalize each character and pad it with spaces.
5. Make each character uppercase, when the language uses case.
6. Set the case of each character randomly when the language uses
case.
The noise is injected in one of two ways: into the ﬁne-tuning and
evaluation data or just into the evaluation data. For the question an-
swering task, noise is added to the context but not to the question or
answer. For the sentence entailment training task, noise is added to the
premise and hypothesis.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />[20]S. Barocas and D. Boyd ,Engaging the ethics of data science
in practice , Communications of the ACM, 60 (2017), pp. 23–25.
[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-
box models via model extraction , arXiv preprint arXiv:1705.08504,
(2017).
[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci
trustworthiness and how to address them , in Proceedings of the 1st
Workshop on Explainable Computational Intelligence (XCI 2017),
2017.
[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and
alternatives , arXiv preprint arXiv:2102.12452, (2021).
[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-
guage model for scientiﬁc text , in EMNLP, 2019.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>28. How is the prescription period interrupted by a conservatory measure or forced execution act?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The prescription period is interrupted by a conservatory measure or forced execution act in accordance with the article L114-2 of the Insurance Code. This interruption can occur through any act of forced execution or any conservatory measure taken in accordance with the code of civil execution procedures. This means that any legal action taken to enforce a judgement or any precautionary measure taken to preserve the rights of a party can interrupt the prescription period. This interruption effectively resets the prescription period, allowing for additional time to pursue a claim or action.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The prescription period is interrupted by a conservatory measure or forced execution act in the following ways:

1. **Recognition of Right to Guarantee**: If the insurer recognizes the insured's right to benefit from the contested guarantee, the prescription period is interrupted.

2. **Forced Execution**: Any act of forced execution, such as a payment order or seizure, can interrupt the prescription period.

3. **Conservatory Measure**: If a conservatory measure is taken in accordance with the civil enforcement procedures code, like a conservatory seizure, the prescription period is interrupted.

4. **Legal Action**: Initiating legal action, including summary proceedings, even if brought before an incompetent court or if the filing is annulled due to a procedural defect, can interrupt the prescription period.

In all these cases, the interruption of the prescription period lasts until the conclusion of the legal proceedings, unless there is a lack of action by the parties for two years, withdrawal, or definitive rejection of the claim by the party taking legal action.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Quand l’action de l’assuré contre l’assureur a pour cause le recours d’un 
tiers, le délai de prescription ne court que du jour où ce tiers a exercé une 
action en justice contre l’assuré ou a été indemnisé par ce dernier.
La prescription est portée à dix ans dans les contrats d’assurance contre 
les accidents atteignant les personnes, lorsque les bénéficiaires sont les 
ayants-droit de l’assuré décédé.
Conformément à l’article L114-2 du Code des assurances, la prescription est 
interrompue par une des causes ordinaires d’interruption de la prescription 
visée ci-après :
• toute demande en justice, même en référé, ou même portée devant une 
juridiction incompétente ;
• tout acte d’exécution forcée, ou toute mesure conservatoire prise en 
application du code des procédures civiles d’exécution ;
• toute reconnaissance par l’assureur du droit à garantie de l’assuré, ou 
toute reconnaissance de dette de l’assuré envers l’assureur ; 
• tout recours à la médiation ou à la conciliation lorsque la partie est dans 
l’impossibilité d’agir par suite d’un empêchement résultant de la loi, de la 
convention d’assistance ou de la Force Majeure.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />65Causes ordinaires d'interruption de la prescription
Article 2240 du code civil
La reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.
Article 2241 du code civil
La demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.
Article 2242 du code civil
L’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Article 2241 du code civil
La demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.
Article 2242 du code civil
L’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.
Article 2243 du code civil
L’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Passé ce délai, il y a prescription : toute action dérivant 
du contrat d’assurance est éteinte.
Le délai de prescription est interrompu :
-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier 
domicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé 
par vos soins à MMA en ce qui concerne le règlement d’un sinistre,
- soit par désignation de l’expert à la suite d’un sinistre,
- soit par une des causes ordinaires d’interruption de la prescription :
* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,
* un acte d’exécution forcée (exemples : commandement de payer, saisie),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Le délai de prescription est interrompu :
-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier 
domicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé 
par vos soins à MMA en ce qui concerne le règlement d’un sinistre,
- soit par désignation de l’expert à la suite d’un sinistre,
- soit par une des causes ordinaires d’interruption de la prescription :
* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,
* un acte d’exécution forcée (exemples : commandement de payer, saisie),
*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : 
saisie conservatoire),</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />ou par l’envoi d’un recommandé adressé 
par vos soins à MMA en ce qui concerne le règlement d’un sinistre,
- soit par désignation de l’expert à la suite d’un sinistre,
- soit par une des causes ordinaires d’interruption de la prescription :
* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,
* un acte d’exécution forcée (exemples : commandement de payer, saisie),
*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : 
saisie conservatoire),
*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas 
d’annulation de l’acte de saisine pour vice de procédure,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Toutefois, ce délai ne court :  
▪ en cas de réticence, omission, déclaration fausse ou inexacte sur le risque couru, que du jour où l’Assureur  en a eu 
connaissance,  
▪ en cas de Sinistre , que du jour où les intéressés en ont eu connaissance, s’ils prouvent qu’ils l’ont ignoré jusque -là. 
 
Quand l’action de l ’Assuré  contre l’Assureur  a pour cause le recours d’un Tiers , le délai de la prescript ion ne court que du jour 
où ce t iers a exercé une action en justice contre l’ Assuré  ou a été indemnisé par ce dernier.  
 
Conformément à l’article L.114 -2 du Code des assurances, la prescription est interrompue par une des causes ordinaires 
d’interruption de la prescription constituées par : toute demande en justice, même en référé, ou même portée devant une 
juridiction incompétente ; tout acte d’exécution forcée, ou toute mesure conservatoire prise en application du code des 
procédures civil es d’exécution ;  toute reconnaissance par l ’Assureur  du droit à garantie de l’ Assuré , ou toute reconnaissance 
de dette de l’ Assuré  envers l’ Assureur .</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />- soit par désignation de l’expert à la suite d’un sinistre,
- soit par une des causes ordinaires d’interruption de la prescription :
* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,
* un acte d’exécution forcée (exemples : commandement de payer, saisie),
*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : 
saisie conservatoire),
*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas 
d’annulation de l’acte de saisine pour vice de procédure,
*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans,</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />65Causes ordinaires d'interruption de la prescription
Article 2240 du code civil
La reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.
Article 2241 du code civil
La demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.
Article 2242 du code civil
L’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.
Article 2243 du code civil
L’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />- soit par une des causes ordinaires d’interruption de la prescription :
* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,
* un acte d’exécution forcée (exemples : commandement de payer, saisie),
*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : 
saisie conservatoire),
*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas 
d’annulation de l’acte de saisine pour vice de procédure,
*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, 
désistement ou rejet définitif de la demande de celui qui agissait en justice.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries
m=2L
Number of buckets(5.34)
Within each block, each position is allowed to attend to the others in
the block and to those in the preceding block. These two changes deﬁne
a new set of key positions that query ican attend to:
˜Si={
j:⌊si
m⌋
−1≤⌊sj
m⌋
≤⌊si
m⌋}
, (5.35)
wheresiis the position in the sorted matrix that position iwas moved
to. (5.35) can be used in (5.31) to compute the attention under the LSH
scheme described above.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />204■Transformers for Machine Learning: A Deep Dive
The experiments show a consistently low correlation between the atten-
tion weights and feature importance scores across all the datasets, espe-
cially for contextualized encoders .
7.4.1.2 Counterfactual experiments
To validate the second hypothesis, the authors put forth two empirical
questions
1. How much does the output change if the attention scores are ran-
domly permutated?
2. Can we ﬁnd maximally diﬀerent attention that does not change
the output more than a predeﬁned threshold epsilon?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Therefore, the attention mechanism has allowed hu-
mans to focus on only a fraction of information of interest, thus enabling
optimum resource usage, leading to better survival and growth.
The “father of American psychology”, William James, created a two-
component framework to explain the visual attention mechanism [133].
In this framework, the spotlight of attention uses both nonvolitional
(involuntary) and volitional (voluntary) cues to bias the sensory input.
Thenonvolitional cue is involuntary and is based on the saliency and
noticeability of targets in the environment. In contrast, the volitional
cue is based on the subject’s voluntary eﬀort to focus on the target de-
liberately. For example, drawing attention to speciﬁc objects by coloring
them diﬀerently or attending to a crying baby are nonvolitional cues.
In contrast, attending to speciﬁc text for answering question or solving
speciﬁc problems are volitional cues.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To prepare for training, the phrase
pairs are created such that ﬁfty percent of the occurrences are consec-
utive and the remaining ﬁfty percent are not consecutive. Pre-training
the model at the sentence level is beneﬁcial in downstream tasks like
question answering (QA) , natural language inference (NLI), and se-
mantic text similarity (STS), which need sentence pairs as input. Let
l∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP
loss is deﬁned as follows:
L(x,y)
NSP =−logP (l|x,y) (4.2)
4.1.2.2 Cross-Lingual Language Model (XLM)
Cross-Lingual Language Model (XLM) [146] is an improvement over
the mBERT architecture by learning from monolingual and parallel cor-
pora. For learning from monolingual data XLM uses the standard MLM
pre-training task used by mBERT.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />First edition published 2022
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key
kjso that kj=qj
||qj||. To make the computation more eﬃcient, Reformer
does two simple things
1. Queries are sorted so those in the same hash bucket are adjacent.
Within the hash bucket, original sequence order is preserved
2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries
m=2L
Number of buckets(5.34)
Within each block, each position is allowed to attend to the others in
the block and to those in the preceding block. These two changes deﬁne
a new set of key positions that query ican attend to:
˜Si={
j:⌊si
m⌋
−1≤⌊sj
m⌋
≤⌊si
m⌋}
, (5.35)
wheresiis the position in the sorted matrix that position iwas moved
to.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The teacher forcing helps in ad-
dressing the slow convergence and instability problems when training
RNNs.
2.2.4 Issues with RNN-Based Encoder-Decoder
Asdescribedintheabovesection,completeinformationaboutthesource
sentence is compressed and encoded in one context variable used by the
decoder component. As the input size increases, there will be a loss of
information while compressing the input. The words in a sentence can
also have complex structure and long-distance associations based on the
language.Capturingthisinacompressedwayinasinglevectoralsoleads
to ineﬃciencies. On the other hand, each time step’s hidden variables
on the encoder side are available and carry information to be used by
the decoder network. Each time step in the decoder can be inﬂuenced
diﬀerently by the hidden variables in the encoder. RNNs also have issues
with vanishing and explosive gradients [115].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.
com  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
01923, 978-750-8400.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Hewitt and Liang, in their work, devise a control task strategy as
a measure to address the probing confounder problem [110]. Probing
confounder problem can be deﬁned as—given the main neural architec-
ture that needs to be explained, the probing classiﬁer such as MLP or
logistic regression and the supervised auxiliary task, how do we assign
the credit for performance such as test accuracy to either of the three?
They devise control tasks that can be easily learned by a probing di-
agnostic classiﬁer but not encoded in the representations (actual neural
model or layers). Evaluating the performance (testing accuracy) diﬀer-
ence between the control tasks and auxiliary tasks with diﬀerent probing
classiﬁer choices as a selectivity measure can easily assign the credit of
learning to either the representation or the probing classiﬁer. The work
also answers questions such as “how does the probe design aﬀect probing
task performance?” and “can the probes pick spurious signals?”.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>29. How does the insurer handle the analysis of a dispute and decision-making process?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          The text does not provide information on how an insurer handles the analysis of a dispute and decision-making process.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The insurer typically follows a structured process when analyzing a dispute and making decisions. Here is an overview of how insurers handle this:

1. **Receipt of Dispute**: The insurer first receives the dispute or claim from the policyholder or a third party.

2. **Documentation Review**: The insurer reviews all relevant documentation related to the dispute, including the insurance policy, claim details, evidence, and any communication between the parties.

3. **Investigation**: An investigation is conducted to gather additional information, which may involve interviewing involved parties, witnesses, or experts, and conducting site visits if necessary.

4. **Assessment of Liability**: Based on the information gathered, the insurer assesses the liability in the dispute. They determine if the claim is covered under the policy and if the policyholder is at fault.

5. **Evaluation of Damages**: The insurer evaluates the extent of damages or losses claimed by the policyholder and determines the amount that may be payable under the policy.

6. **Negotiation**: In some cases, the insurer may enter into negotiations with the policyholder or third parties to reach a settlement agreement.

7. **Decision-Making**: Based on the investigation, assessment of liability, evaluation of damages, and any negotiations, the insurer makes a decision on how to resolve the dispute. This decision can include accepting the claim, offering a settlement, or denying the claim.

8. **Communication**: The insurer communicates the decision to the policyholder or third party, providing reasons for the decision and any next steps.

9. **Appeals Process**: If the policyholder disagrees with the decision, there may be an appeals process within the insurer's organization or through external dispute resolution mechanisms.

10. **Resolution**: The insurer works towards resolving the dispute in a fair and timely manner, ensuring compliance with legal and regulatory requirements.

Overall, the insurer aims to handle disputes efficiently, fairly, and in accordance with the terms of the insurance policy and relevant laws and regulations.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />188■Transformers for Machine Learning: A Deep Dive
that a user can evaluate and measure. As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As proposed by Xie et al., the
four necessary traits are:
1.Conﬁdence. When the end-user (decision-maker) can align the
transformer-based model’s processing with their thought process
based on the input and the output, the conﬁdence in the system
increases. Saliency maps of attention on image or text highlights
the parts of the input that are important from the model per-
spective for decision-making (classiﬁcation, recognition, question-
answering, etc.), with the output mimicking how trained hu-
mans associate a focus-based mechanism as a form of explana-
tion [195, 127, 254,106,154].
2.Safety. When deployed in applications that directly or indi-
rectly impact human life, the transformer-based models should be
deemed safe. One can qualify safety in terms of the (i) consistent
and deterministic behavior, i.e., given the same input, the out-
put remains the same every time, (ii) robust and reliable under
standard and exceptional conditions, and (iii) the ability to guard
against choices that negatively impact society in general [191].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.6.2 Data, Tools, and Libraries
We chose the Google Play application review dataset. The dataset in-
cludes 15,746 samples in three categories; namely negative, neutral, and
positive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-
tuning task as well as standard Python data science stack for all other
data handling and visualization [249] (Figs. 3.9–3.11).
We use BERT BASEas the starting pre-trained BERT mode. Listing
3.6.3 shows how to load the pre-trained BERT model from the Trans-
(a)
 (b)
Figure 3.9 Exploratory data analysis of the sentiment classiﬁcation
dataset.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />5.2.1 Spatiotemporal self-attention 171
6.5.2.2 Spatiotemporal attention blocks 171
6.6 GRAPH TRANSFORMERS 172
6.6.1 Positional Encodings in a Graph 173
6.6.1.1 Laplacian positional encodings 173
6.6.2 Graph Transformer Input 173
6.6.2.1 Graphs without edge attributes 174
6.6.2.2 Graphs with edge attributes 175
6.7 REINFORCEMENT LEARNING 177
6.7.1 Decision Transformer 178
6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180
6.8.1 Goal 180
6.8.2 Data, Tools, and Libraries 180
6.8.3 Experiments, Results, and Analysis 180
6.8.3.1 Preprocessing speech data 180
6.8.3.2 Evaluation 181</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />5.1 Patch Embeddings 169
6.5.2 Self-Attention 170
6.5.2.1 Spatiotemporal self-attention 171
6.5.2.2 Spatiotemporal attention blocks 171
6.6 GRAPH TRANSFORMERS 172
6.6.1 Positional Encodings in a Graph 173
6.6.1.1 Laplacian positional encodings 173
6.6.2 Graph Transformer Input 173
6.6.2.1 Graphs without edge attributes 174
6.6.2.2 Graphs with edge attributes 175
6.7 REINFORCEMENT LEARNING 177
6.7.1 Decision Transformer 178
6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180
6.8.1 Goal 180
6.8.2 Data, Tools, and Libraries 180
6.8.3 Experiments, Results, and Analysis 180
6.8.3.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4]. Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />190■Transformers for Machine Learning: A Deep Dive
against adversarial examples. Two common methods of adversarial
defense are (i) adversarial training, in which the training dataset is
augmented with adversarial examples to introduce robustness, and
(ii) perturbation removal, in which the model identiﬁes adversarial
examples and rejects them [95, 218].
•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Fairness and Bias: One of the key goals for many models de-
ployed in critical areas impacting humans is to be “fair” and un-
biased in its decision-making process. Though an evolving ﬁeld,
fairness implies (i) group fairness, also called demographic parity
or statistical parity, focusing on race, gender, sexual orientation,
etc. [34], and (ii) individual fairness, focusing on individuals with
similar features and characteristics generating similar model out-
put [82]. Diﬀerent techniques to address fairness can be classiﬁed
as (i) pre-processing methods, removing sensitive features from the
data, (ii) in-process methods, where fairness constraints are added,
and (iii) post-processing methods to adjust the model predictions
after training [36, 139,96,30].
7.3 EXPLAINABLE METHODS TAXONOMY
There are many surveys on explainable AI where diﬀerent strategies
have generated many taxonomies for categorizing explainability tech-
niques.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Learning Mechanism : The ability to provide insights into the
model training process helps to increase the conﬁdence and trust
in the model. The process that leads to the learned state of the
model can be further explored by: understanding the evolution of
diﬀerent layers from learning concepts or statistical patterns from
the perspective of ﬁlters, weights, and activation [301, 294]. Study
of convergence of diﬀerent layers based on inputs and weights,
and generalization properties of networks from memorization and
statistical mechanics viewpoint [206].
•Model Debugging : Akin to software debugging, model debug-
ging corresponds to the inspection of the model architecture, data
processing through the networks, and the errors introduced dur-
ing the training and runtime process [140]. A common research
approach is to build auxiliary models that act as a diagnostic and
inspection probe [2]. The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />•Model Debugging : Akin to software debugging, model debug-
ging corresponds to the inspection of the model architecture, data
processing through the networks, and the errors introduced dur-
ing the training and runtime process [140]. A common research
approach is to build auxiliary models that act as a diagnostic and
inspection probe [2]. The proposal of ModelTracker by Amershi et
al.allowsvisualinteractionsfrommislabeleddata,missingfeatures
identiﬁcation, insights into insuﬃciency of training data for label
learning, the impact of outliers, feature space visualization, model
summary from performance, etc., a model-agnostic approach in
debugging [4]. Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Neural stethoscopes by Fuchs et al. are another
general-purpose framework that analyzes the learning process by
quantifying the importance of inﬂuential factors by promoting and
suppressing information [90].
•AdversarialAttacksandDefense :Adversarialexamplesarear-
tiﬁcially engineered inputs to feed into the models and judge their
abilitytodiscriminate.Constructingadversarialexamplesneedsan
understanding of input spaces and the boundaries between classes
in classiﬁcation problems. Adversarial attacks and defense are two
diﬀerent ways to explore the models from an explainability and di-
agnostic perspective [291, 295]. Black-box and white-box attacks
aretwomaintypesofadversarialattacksthatgenerateexamplesto
deceive the model [233, 86, 76]. Recent research shows that adding
imperceptible noise by perturbing inputs reveals model vulnerabil-
ities [178].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process. Developing frameworks and guidelines for ethics-based AI
is growing as a ﬁeld in and of itself [268, 59,26,20].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3–
19.
[110]J.HewittandP.Liang ,Designing and interpreting probes with
control tasks , arXiv preprint arXiv:1909.03368, (2019).
[111]M. Hind, D. Wei, M. Campbell, N. C. Codella, A. Dhu-
randhar, A. Mojsilović, K. Natesan Ramamurthy, and
K. R. Varshney ,Ted: Teaching ai to explain its decisions , in
Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,
and Society, 2019, pp. 123–129.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />19–36.
[208]M. T. Ribeiro, S. Singh, and C. Guestrin ," why should
i trust you?" explaining the predictions of any classiﬁer , in Pro-
ceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.
[209] ,Anchors: High-precision model-agnostic explanations , in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, 2018.
[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-
cations for individual instances , IEEE Transactions on Knowledge
and Data Engineering, 20 (2008), pp. 589–600.
[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,
T. Kočisk `y, and P. Blunsom ,Reasoning about entailment with
neural attention , arXiv preprint arXiv:1509.06664, (2015).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Xie
et al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-
tory testing and (ii) experience. Satisfactory testing is the ability
of the model to approximate performance similar to its training.
Quantitatively, when model performance metrics such as accuracy
or precision are similar on training and unseen test data, the trust
increases. Experience is a more qualitative assessment where one
judges the system to be suﬃcient to perform the task without in-
spection. Data and concept drift in real-world deployments can
further decrease the trust in the system. There have been many
studies in recent times that explore safety from a model prediction
viewpoint [134, 22,252,5].
4.Ethics. If the model does not violate any ethical principles set by
the end-user, it can be considered ethical in its decision-making
process.</td>
            </tr>
          
        </table>
      
    </div>
  
    <!-- Question -->
    <button type="button" class="collapsible">
      <h2>30. What is the file size of the document?</h2>
    </button>
    <div class="content">
      <!-- Facts -->
      
        <h3>Facts</h3>
        
      
      <!-- Answers -->
      
        <h3>Answers</h3>
        
          <span class="model_name">gpt-4-0613</span> &nbsp;
          As an AI model, I don't have the ability to access file information or properties such as file size.<br /><br />
          <!-- Evals -->
          
        
          <span class="model_name">gpt-3.5-turbo-0125</span> &nbsp;
          The file size of the document is not explicitly mentioned in the provided text excerpts. The content mainly discusses various topics related to transformers, pre-training tasks, data preprocessing, evaluation metrics, and experiments. If you need information on a specific section or topic within the document, please let me know so I can provide more details.<br /><br />
          <!-- Evals -->
          
        
      
      <!-- Chunks -->
      
        <h3>Chunks</h3>
        <table>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■181
defprocess_data(batch):
batch[ "speech "], batch[ "sampling_rate "] =
librosa.load(batch[ "file "], sr=16000)
return batch
timit = timit.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Bidirectional Encoder Representations from Transformers (BERT) ■51
the token size, the documents are preprocessed before extracting the
embeddings. Preprocessing divides the document to smaller paragraphs
or sentences that are smaller than the token size for the transformer
model. Then, clustering is performed on the document embeddings to
cluster all the documents with similar topics together. It is worth not-
ing that dimensionality reduction for the embeddings is recommended
before performing the clustering. Dimensionality reduction algorithms
such as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-
form Manifold Approximation and Projection (UMAP) can be used to
lowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-
umentsusingthenovelclass-basedmetricintroducedbyGrootendorstin
[101].</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As discussed earlier,
BERT uses two pre-training tasks, Masked Language Model (MLM) and
Next Sentence Prediction (NSP). For the MLM task, BERT randomly
masks token during the data pre-processing stage. Therefore, the masks
stay static throughout the entire model training process. RoBERTa on
the other hand follows a dynamic masking strategy where masked to-
kens are randomly chosen for each training epoch. RoBERTa also drops
the NSP pre-training task and only uses the dynamic MLM task. An-
other contribution for RoBERTa is the size of input sequences, where
full sentences of at most 512 tokens, are sampled contiguously from one
or more documents. If the end of a document is reached before getting
the required input sequence size, sentence sampling continues with the
next document after adding an extra separator token.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Bidirectional Encoder Representations from Transformers (BERT) ■51
the token size, the documents are preprocessed before extracting the
embeddings. Preprocessing divides the document to smaller paragraphs
or sentences that are smaller than the token size for the transformer
model. Then, clustering is performed on the document embeddings to
cluster all the documents with similar topics together. It is worth not-
ing that dimensionality reduction for the embeddings is recommended
before performing the clustering. Dimensionality reduction algorithms
such as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-
form Manifold Approximation and Projection (UMAP) can be used to
lowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-
umentsusingthenovelclass-basedmetricintroducedbyGrootendorstin
[101]. Class-Based Term Frequency-Inverse Document Frequency (c-TF-
IDF) is modiﬁcation of the classic Term Frequency-Inverse Document
Frequency (TF-IDF) in which all documents in a certain category is
considered as a single document then compute TF-IDF.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Pre-trained and Application-Speciﬁc Transformers ■181
defprocess_data(batch):
batch[ "speech "], batch[ "sampling_rate "] =
librosa.load(batch[ "file "], sr=16000)
return batch
timit = timit. map(process_data,
remove_columns=[ ’file ’,’audio ’,
’phonetic_detail ’,’word_detail ’,
’dialect_region ’,’sentence_type ’,
’speaker_id ’,’id’])
Listing 6.3 Preprocess speech
6.8.3.2 Evaluation
There are three types of errors we can make during transcription:
•substitution: swapping a correct word or character for an incorrect
one
•deletion: removing a correct word or character
•insertion: inserting a word or character
We can compute the word error rate and character error rate metrics
by summing the three kinds of errors and dividing by the number of
words (or characters):
WER =S+D+I
N(6.39)
CER =S+D+I
N(6.40)
whereSis the number of substitutions, Dis the number of deletions, I
is the number of insertions, and Nis the number of words (or characters,
for CER) in the transcript.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As discussed earlier,
BERT uses two pre-training tasks, Masked Language Model (MLM) and
Next Sentence Prediction (NSP). For the MLM task, BERT randomly
masks token during the data pre-processing stage. Therefore, the masks
stay static throughout the entire model training process. RoBERTa on
the other hand follows a dynamic masking strategy where masked to-
kens are randomly chosen for each training epoch. RoBERTa also drops
the NSP pre-training task and only uses the dynamic MLM task. An-
other contribution for RoBERTa is the size of input sequences, where
full sentences of at most 512 tokens, are sampled contiguously from one
or more documents. If the end of a document is reached before getting
the required input sequence size, sentence sampling continues with the
next document after adding an extra separator token.
On the hyperparameters front, RoBERTa showed that using large
mini-batches with increased learning rates during pre-training improves
the perplexity of the dynamic MLM task as well as the downstream task
performance.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />For the MLM task, BERT randomly
masks token during the data pre-processing stage. Therefore, the masks
stay static throughout the entire model training process. RoBERTa on
the other hand follows a dynamic masking strategy where masked to-
kens are randomly chosen for each training epoch. RoBERTa also drops
the NSP pre-training task and only uses the dynamic MLM task. An-
other contribution for RoBERTa is the size of input sequences, where
full sentences of at most 512 tokens, are sampled contiguously from one
or more documents. If the end of a document is reached before getting
the required input sequence size, sentence sampling continues with the
next document after adding an extra separator token.
On the hyperparameters front, RoBERTa showed that using large
mini-batches with increased learning rates during pre-training improves
the perplexity of the dynamic MLM task as well as the downstream task
performance. RoBERTa is also pre-trained on an order of magnitude
more data than BERT, the pre-training data consists of 160 GB includ-
ing Books Corpus, English Wikipedia, and CommonCrawl News dataset</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.2 BERT VARIANTS
3.2.1 Robustly Optimized BERT Pre-training Approach (RoBERTa)
RoBERTa [170] is a Robustly Optimized BERT Pre-training Approach
thatimprovesontheoriginalBERTmodelintermsofpre-trainingtasks,
training hyperparameters, and pre-training data. As discussed earlier,
BERT uses two pre-training tasks, Masked Language Model (MLM) and
Next Sentence Prediction (NSP). For the MLM task, BERT randomly
masks token during the data pre-processing stage. Therefore, the masks
stay static throughout the entire model training process. RoBERTa on
the other hand follows a dynamic masking strategy where masked to-
kens are randomly chosen for each training epoch. RoBERTa also drops
the NSP pre-training task and only uses the dynamic MLM task. An-
other contribution for RoBERTa is the size of input sequences, where
full sentences of at most 512 tokens, are sampled contiguously from one
or more documents.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.5.3.2 Topic size distribution
topic_sizes = topic_model.get_topic_freq()
topic_sizes
Listing 3.5 Sampling the topics
Now that we have computed a topic distribution, we need to see
what kind of reviews are in each topic. Let’s start by looking at the
distribution of topic sizes.
The commands in Listing 3.5 will output a summary table of the
sizes of the topics. Note that the size of a topic is the number of reviews
that contain that topic. The table is shown in Fig. 3.6.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.5.3 Experiments, Results, and Analysis
3.5.3.1 Building topics
from bertopic import BERTopic
topic_model = BERTopic(n_gram_range=(1, 3))
topics, probs = topic_model.fit_transform(
dataset["text"],
np.array(dataset["embedding"]))
print(f"Number of topics: {len(topic_model.get_topics())}")
Listing 3.4 Compute topics
Now that we’ve computed the embeddings for the speciﬁed subset of
Yelp reviews, we can proceed with the topic modeling in Listing 3.4.
3.5.3.2 Topic size distribution
topic_sizes = topic_model.get_topic_freq()
topic_sizes
Listing 3.5 Sampling the topics
Now that we have computed a topic distribution, we need to see
what kind of reviews are in each topic. Let’s start by looking at the
distribution of topic sizes.
The commands in Listing 3.5 will output a summary table of the
sizes of the topics.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />3.5.3.2 Topic size distribution
topic_sizes = topic_model.get_topic_freq()
topic_sizes
Listing 3.5 Sampling the topics
Now that we have computed a topic distribution, we need to see
what kind of reviews are in each topic. Let’s start by looking at the
distribution of topic sizes.
The commands in Listing 3.5 will output a summary table of the
sizes of the topics. Note that the size of a topic is the number of reviews
that contain that topic. The table is shown in Fig. 3.6.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />230■Bibliography
[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-
senborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and
N. Houlsby ,An image is worth 16x16 words: Transformers for
image recognition at scale , ArXiv, abs/2010.11929 (2021).
[79]P. Dufter and H. Schütze ,Identifying elements essential for
BERT’s multilinguality , in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
Online, November 2020, Association for Computational Linguis-
tics, pp. 4423–4437.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />40■Transformers for Machine Learning: A Deep Dive
Figure 2.20 Explaining translations for—How long you will be here?</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />230■Bibliography
[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-
senborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and
N. Houlsby ,An image is worth 16x16 words: Transformers for
image recognition at scale , ArXiv, abs/2010.11929 (2021).
[79]P. Dufter and H. Schütze ,Identifying elements essential for
BERT’s multilinguality , in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
Online, November 2020, Association for Computational Linguis-
tics, pp. 4423–4437.
[80]V. P. Dwivedi and X. Bresson ,A generalization of trans-
former networks to graphs , ArXiv, abs/2012.09699 (2020).</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />Then we tokenize the sentences, convert the words to token IDs, and
append <bos> and <eos> IDs to the beginning and end of the token
ID sequences. Padding the variable-length sequences to the maximum
observed length in the batch using the <pad> token ensures a ﬁxed-
size tensor for training and evaluation.
The total of 135,842language pairs after ﬁltering reduce to 131,951
and we further split it into 80% training, 10% validation and 10% test
data, i.e., 105,460, 13,308, and 13,183respectively.
Figs. 2.13 and 2.14 show the distribution plots as histograms for En-
glish/French and joint distribution. Most of the sentences in the parallel
corpus are between 4and8tokens/words length.
Figure 2.13 SentencelengthdistributionforEnglishandFrenchsentences.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />As we can see, the content of the unassigned cluster contains words
that do not strongly belong to any topic. The largest, smallest, and
median topics are shown in Listings 3.10, 3.11, and 3.12, respectively.
>>> dump_topic_and_docs("Largest topic", 0)
Largest topic size: 349
**** Representative reviews ****
This place makes me cringe! I’ve dined here with large groups
of friends when we needed to have a big table and they all
wanted to be bursting full of cheap food and that is really
the only excuse to go to this place. \n\nOne reviewer
mentioned the 90’s music and the goofy food art on the
walls. I could not agree more that this isso funny. Whoa
and talk about noisy. This place isdeafening inside on a
Friday orSaturday night, worse than a cafeteria. I think
that everyone with a City-Pass crams inthere insearch of
the best two-for-one deal on a massive mound of macaroni
slathered indreadful red sauce and salty cheese.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />212■Transformers for Machine Learning: A Deep Dive
Figure 7.7 Word cloud.
Figure 7.8 Document length distri-
bution.
Figure 7.9 Top words.
 Figure 7.10 Class distribution.
Tables 7.1 and 7.2 show that the BIO_ClinicalBERT with its pre-
training on a large corpus of medical records has a slight advantage over
the base BERT for classiﬁcation.
7.6.3.3 Error analysis and explainability
Next, we will try to perform error analysis using explainable techniques
to get more insights. The error analysis on the 500sized test data shows
1. The BIO_ClinincalBERT performs better than the base model on
51instances.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br /></td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />242■Bibliography
of the North American Chapter of the Association for Computa-
tionalLinguistics:HumanLanguageTechnologies,Volume1(Long
Papers), New Orleans, Louisiana, June 2018, Association for Com-
putational Linguistics, pp. 2227–2237.</td>
            </tr>
          
            <tr>
              
              <td><i> (p. )</i><br /><br />The massive decrease in
vocabulary size that comes with using bytes means more parameters can
be used elsewhere in the model.
Since byte sequences are longer than token sequences and trans-
former computational complexity is quadratic in sequence length (ex-
cept for linearized attention), earlier work has tried to mitigate the in-
creased complexity that comes with using byte-level and character-level
sequences. ByT5 starts with the mT5 architecture, but diﬀers from mT5
in a few ways. Firstly, there is no SentencePiece tokenization. Raw UTF-
8 bytes are used as input and embeddings are learned for each of the
256 bytes. In addition, there are three special tokens added to the vo-
cabulary: a padding token, an EOS token, and an unknown token. The
pre-training task is changed so that the length of the masked portion of
input spans is longer than in is in T5. in T5, the encoder and decoder
modules have the same number of layers.</td>
            </tr>
          
        </table>
      
    </div>
  

  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }

    for (i = 0; i < coll.length; i++) {
      coll[i].nextElementSibling.style.display = "block"
    }
  </script>
</body>

</html>