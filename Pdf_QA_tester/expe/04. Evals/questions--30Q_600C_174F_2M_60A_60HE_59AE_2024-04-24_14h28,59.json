{
  "meta": {},
  "items": [
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the significance of the value of replacement of a vehicle in the context of estimating damages?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not explain the importance of a vehicle's replacement value.\n2. There is no information on how the replacement value of a vehicle is used in damage estimation.\n3. The document lacks details on the role of a vehicle's replacement value in assessing damages.\n4. The significance of a vehicle's replacement value in damage estimation is not discussed in the text.\n5. The text does not provide insight into the relevance of a vehicle's replacement cost when calculating damages.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the significance of the value of replacement of a vehicle in the context of estimating damages.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:21:48.086728",
          "duration": 10.037789,
          "cost": 0.009930000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not explain the importance of a vehicle's replacement value."
          },
          {
            "meta": {},
            "text": "2. There is no information on how the replacement value of a vehicle is used in damage estimation."
          },
          {
            "meta": {},
            "text": "3. The document lacks details on the role of a vehicle's replacement value in assessing damages."
          },
          {
            "meta": {},
            "text": "4. The significance of a vehicle's replacement value in damage estimation is not discussed in the text."
          },
          {
            "meta": {},
            "text": "5. The text does not provide insight into the relevance of a vehicle's replacement cost when calculating damages."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.61730141852371,
              "Node id": "cb3acf95-4654-4a09-b285-34347d7988a3"
            },
            "text": "It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69)"
          },
          {
            "meta": {
              "score": 11.183004709109658,
              "Node id": "7f9ba7a0-0741-427b-a314-d064f98906db"
            },
            "text": "In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3."
          },
          {
            "meta": {
              "score": 10.994157068743228,
              "Node id": "86c9fc46-94e3-4b0a-aa18-19894d3c5262"
            },
            "text": "For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig."
          },
          {
            "meta": {
              "score": 10.994157068743228,
              "Node id": "ac33419a-e0e5-43f0-a6be-314c1c7edf2f"
            },
            "text": "In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]."
          },
          {
            "meta": {
              "score": 10.994157068743228,
              "Node id": "81b4b2cd-f2b0-4205-9f9e-a490cdb7c971"
            },
            "text": "Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength."
          },
          {
            "meta": {
              "score": 10.93266392067584,
              "Node id": "0707183f-0274-4e7d-bb0c-b02ee1df8bb9"
            },
            "text": "Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig."
          },
          {
            "meta": {
              "score": 10.688121787030054,
              "Node id": "53a29409-2210-4f7b-8a35-f5fdff3344cc"
            },
            "text": "The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention."
          },
          {
            "meta": {
              "score": 10.578113940838774,
              "Node id": "cbe6d420-11d3-4be5-995b-2fb56787c4d8"
            },
            "text": "In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv."
          },
          {
            "meta": {
              "score": 10.537982439514462,
              "Node id": "9f5d9ddf-d872-4c28-868e-77927717e007"
            },
            "text": "BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones."
          },
          {
            "meta": {
              "score": 10.334203694069192,
              "Node id": "ec32cc4b-b187-46d5-bda4-f40f69202ba5"
            },
            "text": "5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3."
          },
          {
            "meta": {
              "score": 0.7727712777438898,
              "Node id": "d7d336e0-17b8-4036-93a3-f1217f9d34e0"
            },
            "text": "The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12)"
          },
          {
            "meta": {
              "score": 0.771513651813147,
              "Node id": "7785e9f2-8ac6-4258-be99-3f86faa1991b"
            },
            "text": "By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]. The authors propose three classes of perturbations to\ngenerate visual explanations for image classiﬁcation: (i) constant,\nreplacing a region in the image with a constant value, (ii) noise,\nadding small noise to the region, and (iii) blur, blurring the region."
          },
          {
            "meta": {
              "score": 0.7659206703056911,
              "Node id": "96cd5002-ae7d-46bd-b17e-8d633eea0af9"
            },
            "text": "•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]."
          },
          {
            "meta": {
              "score": 0.7635175242824196,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7623037190772034,
              "Node id": "c176b914-5ccc-4b00-bbb1-7df42383619d"
            },
            "text": "7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision."
          },
          {
            "meta": {
              "score": 0.7619234415771955,
              "Node id": "20010ae9-c968-4c58-8312-204c37d847a5"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 0.7595101227336638,
              "Node id": "ed8eec2f-6db3-4cd5-a389-b07c94c67054"
            },
            "text": "194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased."
          },
          {
            "meta": {
              "score": 0.75652267389047,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.756191550550548,
              "Node id": "0dca327f-4762-4975-82a4-0a9a1a50e6cf"
            },
            "text": "3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]."
          },
          {
            "meta": {
              "score": 0.755730816408706,
              "Node id": "0af40419-c4b5-49b7-bd7a-c4488ccd7dcd"
            },
            "text": "194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the significance of the value of replacement of a vehicle in the context of estimating damages.",
              "prompt": {
                "meta": {},
                "user": "What is the significance of the value of replacement of a vehicle in the context of estimating damages?",
                "system": "Contexte :  It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69) \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. \n\n In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n 5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]. The authors propose three classes of perturbations to\ngenerate visual explanations for image classiﬁcation: (i) constant,\nreplacing a region in the image with a constant value, (ii) noise,\nadding small noise to the region, and (iii) blur, blurring the region. \n\n •Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n 194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n 194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. \n\n \n La question est What is the significance of the value of replacement of a vehicle in the context of estimating damages?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:04:30.399947",
              "duration": 3.698582,
              "cost": 0.13299
            },
            "meta": {},
            "text": "The text does not provide information on the significance of the value of replacement of a vehicle in the context of estimating damages.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the significance of the value of replacement of a vehicle in the context of estimating damages.\n\n1. 1. The text does not explain the importance of a vehicle's replacement value.\n2. 2. There is no information on how the replacement value of a vehicle is used in damage estimation.\n3. 3. The document lacks details on the role of a vehicle's replacement value in assessing damages.\n4. 4. The significance of a vehicle's replacement value in damage estimation is not discussed in the text.\n5. 5. The text does not provide insight into the relevance of a vehicle's replacement cost when calculating damages.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:17.913786",
                "duration": 2.0808530000000003,
                "cost": 0.00789
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the context of estimating damages, the value of replacement of a vehicle is significant for several reasons:\n\n1. **Determining Total Loss**: The value of replacement helps in determining whether a vehicle is considered a total loss. If the cost of repairing the damages exceeds the value of replacing the vehicle, it may be deemed a total loss by insurance companies.\n\n2. **Insurance Claims**: When filing an insurance claim for damages to a vehicle, the value of replacement is used to assess the amount of compensation the policyholder is entitled to receive. It helps in determining the fair market value of the vehicle before the damage occurred.\n\n3. **Negotiations**: In cases where damages are being settled outside of insurance claims, such as in legal disputes or negotiations with a third party at fault, the value of replacement serves as a benchmark for determining the amount of compensation or damages owed.\n\n4. **Financial Impact**: Understanding the value of replacement provides insight into the financial impact of the damages. It helps in assessing the cost-effectiveness of repairing the vehicle versus replacing it.\n\n5. **Resale Value**: The value of replacement also factors into the resale value of the vehicle post-repair. A lower replacement value may affect the resale value of the vehicle in the future.\n\nOverall, the value of replacement of a vehicle plays a crucial role in the process of estimating damages, determining compensation, and making decisions regarding repairs or replacements after an incident.",
              "prompt": {
                "meta": {},
                "user": "What is the significance of the value of replacement of a vehicle in the context of estimating damages?",
                "system": "Contexte :  It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69) \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. \n\n In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n 5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]. The authors propose three classes of perturbations to\ngenerate visual explanations for image classiﬁcation: (i) constant,\nreplacing a region in the image with a constant value, (ii) noise,\nadding small noise to the region, and (iii) blur, blurring the region. \n\n •Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision.\n•Meaningful Perturbation : Fong and Vedaldi propose a meta-\npredictor using the perturbation technique to give local explana-\ntions based on the sensitive areas in the input that predicted the\noutput [89]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. A more sophisticated method uses the reinforcement\nlearning model to ﬁnd the minimal set of words that must be\nerased to change the model’s decision. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n 194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. The erasure can\nbe performed on various levels of representation, such as the input\nword-vector dimensions, input words or group of words, and inter-\nmediatehiddenunits.Thestraightforwardtechniqueistocompute\nthe diﬀerence in log-likelihood on the labels when representations\nare erased. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n 194■Transformers for Machine Learning: A Deep Dive\nwhereαissmoothlydistributedintherange [0,1]andisassociated\nwith the path from x′tox. The above equation can be approx-\nimated using Riemann summation using appropriate number of\nsteps as described in the paper.\n7.3.1.2 Perturbation-based\nAltering the input and computing the feature relevance by comparing\nthe diﬀerence in the output between the original and altered input is the\ncore of all perturbation-based methods.\n•Representation Erasure: Li et al. propose a generic technique\nfor an explanation by examining the eﬀect of erasing the input rep-\nresentation to see how such changes aﬀect the output [157]. By an-\nalyzing the impact, one can identify essential representations that\nsigniﬁcantly contribute to the models’ decisions. \n\n \n La question est What is the significance of the value of replacement of a vehicle in the context of estimating damages?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:04:37.838699",
              "duration": 7.432641,
              "cost": 0.00262
            },
            "meta": {},
            "text": "In the context of estimating damages, the value of replacement of a vehicle is significant for several reasons:\n\n1. **Determining Total Loss**: The value of replacement helps in determining whether a vehicle is considered a total loss. If the cost of repairing the damages exceeds the value of replacing the vehicle, it may be deemed a total loss by insurance companies.\n\n2. **Insurance Claims**: When filing an insurance claim for damages to a vehicle, the value of replacement is used to assess the amount of compensation the policyholder is entitled to receive. It helps in determining the fair market value of the vehicle before the damage occurred.\n\n3. **Negotiations**: In cases where damages are being settled outside of insurance claims, such as in legal disputes or negotiations with a third party at fault, the value of replacement serves as a benchmark for determining the amount of compensation or damages owed.\n\n4. **Financial Impact**: Understanding the value of replacement provides insight into the financial impact of the damages. It helps in assessing the cost-effectiveness of repairing the vehicle versus replacing it.\n\n5. **Resale Value**: The value of replacement also factors into the resale value of the vehicle post-repair. A lower replacement value may affect the resale value of the vehicle in the future.\n\nOverall, the value of replacement of a vehicle plays a crucial role in the process of estimating damages, determining compensation, and making decisions regarding repairs or replacements after an incident.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the context of estimating damages, the value of replacement of a vehicle is significant for several reasons:\n\n1. **Determining Total Loss**: The value of replacement helps in determining whether a vehicle is considered a total loss. If the cost of repairing the damages exceeds the value of replacing the vehicle, it may be deemed a total loss by insurance companies.\n\n2. **Insurance Claims**: When filing an insurance claim for damages to a vehicle, the value of replacement is used to assess the amount of compensation the policyholder is entitled to receive. It helps in determining the fair market value of the vehicle before the damage occurred.\n\n3. **Negotiations**: In cases where damages are being settled outside of insurance claims, such as in legal disputes or negotiations with a third party at fault, the value of replacement serves as a benchmark for determining the amount of compensation or damages owed.\n\n4. **Financial Impact**: Understanding the value of replacement provides insight into the financial impact of the damages. It helps in assessing the cost-effectiveness of repairing the vehicle versus replacing it.\n\n5. **Resale Value**: The value of replacement also factors into the resale value of the vehicle post-repair. A lower replacement value may affect the resale value of the vehicle in the future.\n\nOverall, the value of replacement of a vehicle plays a crucial role in the process of estimating damages, determining compensation, and making decisions regarding repairs or replacements after an incident.\n\n1. 1. The text does not explain the importance of a vehicle's replacement value.\n2. 2. There is no information on how the replacement value of a vehicle is used in damage estimation.\n3. 3. The document lacks details on the role of a vehicle's replacement value in assessing damages.\n4. 4. The significance of a vehicle's replacement value in damage estimation is not discussed in the text.\n5. 5. The text does not provide insight into the relevance of a vehicle's replacement cost when calculating damages.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:19.703395",
                "duration": 1.787235,
                "cost": 0.01572
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. An insured individual's ability to choose their own defense lawyer can depend on the terms of their insurance policy and local laws.\n2. Some insurance policies may include a clause that allows the insured to select their own lawyer.\n3. Liability insurance policies often permit the insured to choose their own defense lawyer.\n4. If a conflict of interest arises, such as the insurance company's lawyer representing both the insured and the insurer, the insured may be allowed to select their own lawyer.\n5. In certain jurisdictions, the insured has the right to choose their own lawyer in criminal cases.\n6. This right to choose a lawyer in criminal cases may exist even if the insurance policy typically requires the use of a lawyer provided by the insurance company.\n7. A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss.\n8. If an insured individual has a recourse guarantee, they may be able to choose their own lawyer to pursue the recourse action.\n9. Even when the insured can choose their own lawyer, the insurance company may still need to approve the choice.\n10. The insurer will typically only cover legal fees up to a certain amount.\n11. It is recommended to refer to the specific insurance policy and consult with a legal professional for advice.",
          "prompt": {
            "meta": {},
            "user": "The specifics of when an insured individual can choose their own defense lawyer in legal actions related to penal defense and recourse guarantee can vary based on the terms of the insurance policy and local laws. However, generally, the insured may have the freedom to choose their defense lawyer in the following circumstances:\n\n1. If the insurance policy explicitly allows for it: Some insurance policies may have a clause that allows the insured to choose their own lawyer. This is often the case in liability insurance policies.\n\n2. If there is a conflict of interest: If the insurance company's lawyer has a conflict of interest, the insured may be able to choose their own lawyer. This could occur if the same lawyer is representing both the insured and the insurer, and their interests diverge.\n\n3. If the legal action is a criminal case: In some jurisdictions, the insured has the right to choose their own lawyer in criminal cases, even if their insurance policy would otherwise require them to use a lawyer provided by the insurance company.\n\n4. If the insured has a recourse guarantee: A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss. If the insured has this guarantee, they may be able to choose their own lawyer to pursue this recourse action.\n\nIt's important to note that even when the insured can choose their own lawyer, the insurance company may still have to approve the choice, and the insurer will typically only cover legal fees up to a certain amount. Always refer to the specific insurance policy and consult with a legal professional for advice.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:06.484712",
          "duration": 18.395473,
          "cost": 0.02853
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. An insured individual's ability to choose their own defense lawyer can depend on the terms of their insurance policy and local laws."
          },
          {
            "meta": {},
            "text": "2. Some insurance policies may include a clause that allows the insured to select their own lawyer."
          },
          {
            "meta": {},
            "text": "3. Liability insurance policies often permit the insured to choose their own defense lawyer."
          },
          {
            "meta": {},
            "text": "4. If a conflict of interest arises, such as the insurance company's lawyer representing both the insured and the insurer, the insured may be allowed to select their own lawyer."
          },
          {
            "meta": {},
            "text": "5. In certain jurisdictions, the insured has the right to choose their own lawyer in criminal cases."
          },
          {
            "meta": {},
            "text": "6. This right to choose a lawyer in criminal cases may exist even if the insurance policy typically requires the use of a lawyer provided by the insurance company."
          },
          {
            "meta": {},
            "text": "7. A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss."
          },
          {
            "meta": {},
            "text": "8. If an insured individual has a recourse guarantee, they may be able to choose their own lawyer to pursue the recourse action."
          },
          {
            "meta": {},
            "text": "9. Even when the insured can choose their own lawyer, the insurance company may still need to approve the choice."
          },
          {
            "meta": {},
            "text": "10. The insurer will typically only cover legal fees up to a certain amount."
          },
          {
            "meta": {},
            "text": "11. It is recommended to refer to the specific insurance policy and consult with a legal professional for advice."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.917837390047278,
              "Node id": "c1094e46-cb60-40db-a610-1104278a0902"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy."
          },
          {
            "meta": {
              "score": 11.711713889269172,
              "Node id": "aef9a952-c24c-41ea-92e7-3213bb808491"
            },
            "text": "The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn."
          },
          {
            "meta": {
              "score": 11.576257568560425,
              "Node id": "7ce792fb-168f-461b-a65f-0fd2f4a0c385"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied."
          },
          {
            "meta": {
              "score": 10.340365158322175,
              "Node id": "aeb7c54d-2f3e-4921-ab37-7b4e744166c1"
            },
            "text": "Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention. The number of tokens with\nglobal attention is generally independent of sequence length, so global\nattention is also linear in sequence length. The Longformer combines\nthis global attention with the sliding window attention.\nWeights for the global and short-ranged attention patterns are com-\nputed separately by giving the global and sliding window patterns their"
          },
          {
            "meta": {
              "score": 10.28606616416182,
              "Node id": "4493b24e-bfaf-4316-8595-62ccd811c50b"
            },
            "text": "178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions."
          },
          {
            "meta": {
              "score": 10.243567691213656,
              "Node id": "b51547de-be94-4013-bd79-e2125f5e9d64"
            },
            "text": "Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention."
          },
          {
            "meta": {
              "score": 10.055309638600292,
              "Node id": "0c8f6c49-90e5-4f0c-82ba-b018ac3f1cc0"
            },
            "text": "Thus, the left context of w5would be\n(w1,w3)and the right context would be (w7, w 9). Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns."
          },
          {
            "meta": {
              "score": 9.961330582178395,
              "Node id": "c393c8df-a3b0-40af-a184-2c050d99f4af"
            },
            "text": "The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state.\nTo understand the process of reinforcement learning, we can start\nwithaﬁniteMarkovdecisionprocess[239],whichisaformalwayoflook-\ning at a sequence of decisions that an agent which interacts with an envi-"
          },
          {
            "meta": {
              "score": 9.89882851796003,
              "Node id": "5d5949c0-a8d7-4775-87f9-18854a7d36bd"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■177\nFigure 6.4 Diagram of the Graph Transformer encoder, with edge at-\ntributes. The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state."
          },
          {
            "meta": {
              "score": 9.715941386328259,
              "Node id": "c80e3405-ad06-446a-a0cc-135c65d99c7a"
            },
            "text": "The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities."
          },
          {
            "meta": {
              "score": 0.738191737447308,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7356688966137119,
              "Node id": "c194b57c-f2ab-4d62-adce-5d3482fec02d"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          },
          {
            "meta": {
              "score": 0.735645434216246,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7344169181386778,
              "Node id": "caf645f8-31dd-4c59-b934-6d7260464148"
            },
            "text": "Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”."
          },
          {
            "meta": {
              "score": 0.7314929438151023,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7303400721262922,
              "Node id": "d1331472-353b-450d-ac4f-0d074ce47b88"
            },
            "text": "[75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314."
          },
          {
            "meta": {
              "score": 0.7297595761209416,
              "Node id": "afd870c5-cea9-4bad-a97c-98e58dd8ff7a"
            },
            "text": "Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]."
          },
          {
            "meta": {
              "score": 0.7274876695946284,
              "Node id": "845269f1-acb6-47f7-908c-905f8cee14f1"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 0.7267023873702539,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7263821997789218,
              "Node id": "9b938fdf-d400-40fc-aa69-400bd77bb390"
            },
            "text": "[95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The specifics of when an insured individual can choose their own defense lawyer in legal actions related to penal defense and recourse guarantee can vary based on the terms of the insurance policy and local laws. However, generally, the insured may have the freedom to choose their defense lawyer in the following circumstances:\n\n1. If the insurance policy explicitly allows for it: Some insurance policies may have a clause that allows the insured to choose their own lawyer. This is often the case in liability insurance policies.\n\n2. If there is a conflict of interest: If the insurance company's lawyer has a conflict of interest, the insured may be able to choose their own lawyer. This could occur if the same lawyer is representing both the insured and the insurer, and their interests diverge.\n\n3. If the legal action is a criminal case: In some jurisdictions, the insured has the right to choose their own lawyer in criminal cases, even if their insurance policy would otherwise require them to use a lawyer provided by the insurance company.\n\n4. If the insured has a recourse guarantee: A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss. If the insured has this guarantee, they may be able to choose their own lawyer to pursue this recourse action.\n\nIt's important to note that even when the insured can choose their own lawyer, the insurance company may still have to approve the choice, and the insurer will typically only cover legal fees up to a certain amount. Always refer to the specific insurance policy and consult with a legal professional for advice.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?",
                "system": "Contexte :  This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy. \n\n The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention. The number of tokens with\nglobal attention is generally independent of sequence length, so global\nattention is also linear in sequence length. The Longformer combines\nthis global attention with the sliding window attention.\nWeights for the global and short-ranged attention patterns are com-\nputed separately by giving the global and sliding window patterns their \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention. \n\n Thus, the left context of w5would be\n(w1,w3)and the right context would be (w7, w 9). Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns. \n\n The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state.\nTo understand the process of reinforcement learning, we can start\nwithaﬁniteMarkovdecisionprocess[239],whichisaformalwayoflook-\ning at a sequence of decisions that an agent which interacts with an envi- \n\n Pre-trained and Application-Speciﬁc Transformers ■177\nFigure 6.4 Diagram of the Graph Transformer encoder, with edge at-\ntributes. The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state. \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n [75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314. \n\n Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n [95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. \n\n \n La question est Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:04:57.715041",
              "duration": 19.87108,
              "cost": 0.15321
            },
            "meta": {},
            "text": "The specifics of when an insured individual can choose their own defense lawyer in legal actions related to penal defense and recourse guarantee can vary based on the terms of the insurance policy and local laws. However, generally, the insured may have the freedom to choose their defense lawyer in the following circumstances:\n\n1. If the insurance policy explicitly allows for it: Some insurance policies may have a clause that allows the insured to choose their own lawyer. This is often the case in liability insurance policies.\n\n2. If there is a conflict of interest: If the insurance company's lawyer has a conflict of interest, the insured may be able to choose their own lawyer. This could occur if the same lawyer is representing both the insured and the insurer, and their interests diverge.\n\n3. If the legal action is a criminal case: In some jurisdictions, the insured has the right to choose their own lawyer in criminal cases, even if their insurance policy would otherwise require them to use a lawyer provided by the insurance company.\n\n4. If the insured has a recourse guarantee: A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss. If the insured has this guarantee, they may be able to choose their own lawyer to pursue this recourse action.\n\nIt's important to note that even when the insured can choose their own lawyer, the insurance company may still have to approve the choice, and the insurer will typically only cover legal fees up to a certain amount. Always refer to the specific insurance policy and consult with a legal professional for advice.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The specifics of when an insured individual can choose their own defense lawyer in legal actions related to penal defense and recourse guarantee can vary based on the terms of the insurance policy and local laws. However, generally, the insured may have the freedom to choose their defense lawyer in the following circumstances:\n\n1. If the insurance policy explicitly allows for it: Some insurance policies may have a clause that allows the insured to choose their own lawyer. This is often the case in liability insurance policies.\n\n2. If there is a conflict of interest: If the insurance company's lawyer has a conflict of interest, the insured may be able to choose their own lawyer. This could occur if the same lawyer is representing both the insured and the insurer, and their interests diverge.\n\n3. If the legal action is a criminal case: In some jurisdictions, the insured has the right to choose their own lawyer in criminal cases, even if their insurance policy would otherwise require them to use a lawyer provided by the insurance company.\n\n4. If the insured has a recourse guarantee: A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss. If the insured has this guarantee, they may be able to choose their own lawyer to pursue this recourse action.\n\nIt's important to note that even when the insured can choose their own lawyer, the insurance company may still have to approve the choice, and the insurer will typically only cover legal fees up to a certain amount. Always refer to the specific insurance policy and consult with a legal professional for advice.\n\n1. 1. An insured individual's ability to choose their own defense lawyer can depend on the terms of their insurance policy and local laws.\n2. 2. Some insurance policies may include a clause that allows the insured to select their own lawyer.\n3. 3. Liability insurance policies often permit the insured to choose their own defense lawyer.\n4. 4. If a conflict of interest arises, such as the insurance company's lawyer representing both the insured and the insurer, the insured may be allowed to select their own lawyer.\n5. 5. In certain jurisdictions, the insured has the right to choose their own lawyer in criminal cases.\n6. 6. This right to choose a lawyer in criminal cases may exist even if the insurance policy typically requires the use of a lawyer provided by the insurance company.\n7. 7. A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss.\n8. 8. If an insured individual has a recourse guarantee, they may be able to choose their own lawyer to pursue the recourse action.\n9. 9. Even when the insured can choose their own lawyer, the insurance company may still need to approve the choice.\n10. 10. The insurer will typically only cover legal fees up to a certain amount.\n11. 11. It is recommended to refer to the specific insurance policy and consult with a legal professional for advice.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:22.588036",
                "duration": 2.881123,
                "cost": 0.02316
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The insured typically has the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee when they have a provision in their insurance policy that allows for such choice. This provision is often found in policies that provide legal defense coverage, where the insured can select their own legal representation.\n\nHowever, it's important to review the specific terms and conditions of the insurance policy to determine the extent of this freedom. Some policies may have restrictions on the selection of defense lawyers, such as a list of pre-approved attorneys or a requirement for the insurance company to approve the chosen lawyer.\n\nIn cases where the insured has the freedom to choose their defense lawyer, it is advisable to select a lawyer with experience in the relevant legal area to ensure effective representation and protection of their rights.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?",
                "system": "Contexte :  This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy. \n\n The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention. The number of tokens with\nglobal attention is generally independent of sequence length, so global\nattention is also linear in sequence length. The Longformer combines\nthis global attention with the sliding window attention.\nWeights for the global and short-ranged attention patterns are com-\nputed separately by giving the global and sliding window patterns their \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns.\nLongformerdecideswhichtokensareallowedtohaveglobalattention\nbasedonthetrainingtask.Forinstance,inquestionandanswertasks,all\ntokens in the question have global attention. \n\n Thus, the left context of w5would be\n(w1,w3)and the right context would be (w7, w 9). Note that even though\nthe\noverall context width has grown, this attention pattern also prevents\na token from attending to its immediate neighbor, or to any token that\nis a distance d−1away from the center of the window or to any position\nit attends to that are inside the window. Diﬀerent attention heads can\nuse diﬀerent values of d, which mitigates the issue mentioned above. Fig.\n5.8shows dilated sliding window attention for L= 2,w= 4, andd= 2.\nGlobal attention The global attention pattern chooses lets some to-\nkens attend to any other token in the sequence. In such cases, all tokens\nin the sequence attend to that token. This corresponds to choosing spe-\nciﬁc rows of an attention weight matrix and their transposed columns. \n\n The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state.\nTo understand the process of reinforcement learning, we can start\nwithaﬁniteMarkovdecisionprocess[239],whichisaformalwayoflook-\ning at a sequence of decisions that an agent which interacts with an envi- \n\n Pre-trained and Application-Speciﬁc Transformers ■177\nFigure 6.4 Diagram of the Graph Transformer encoder, with edge at-\ntributes. The encoder structure is shown on the left. The multi-head at-\ntention mechanism is shown on the right. As described in section 6.6.1.1,\nLaplacian embeddings are only applied to the input layer, l= 0.\n6.7 REINFORCEMENT LEARNING\nFigure 6.5 Depiction of the interaction between the agent and environ-\nment in reinforcement learning. At time t, the state observed by the\nagent isst. The agent then selects an action at. In the next time step,\nt+ 1, the environment updates it state to st+1and issues a reward to\nthe agent,rt+1. After this, the cycle can repeat.\nReinforcementLearning(RL)isamachinelearningmethodthatuses\nrewards to train an autonomous agent that can choose its next action\nso that it gets the best cumulative reward, given its current state. \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n [75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314. \n\n Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n [95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. \n\n \n La question est Under what circumstances does the insured have the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:05:02.021585",
              "duration": 4.301216,
              "cost": 0.0024704999999999996
            },
            "meta": {},
            "text": "The insured typically has the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee when they have a provision in their insurance policy that allows for such choice. This provision is often found in policies that provide legal defense coverage, where the insured can select their own legal representation.\n\nHowever, it's important to review the specific terms and conditions of the insurance policy to determine the extent of this freedom. Some policies may have restrictions on the selection of defense lawyers, such as a list of pre-approved attorneys or a requirement for the insurance company to approve the chosen lawyer.\n\nIn cases where the insured has the freedom to choose their defense lawyer, it is advisable to select a lawyer with experience in the relevant legal area to ensure effective representation and protection of their rights.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 9, 11",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The insured typically has the freedom to choose their defense lawyer in legal actions related to penal defense and recourse guarantee when they have a provision in their insurance policy that allows for such choice. This provision is often found in policies that provide legal defense coverage, where the insured can select their own legal representation.\n\nHowever, it's important to review the specific terms and conditions of the insurance policy to determine the extent of this freedom. Some policies may have restrictions on the selection of defense lawyers, such as a list of pre-approved attorneys or a requirement for the insurance company to approve the chosen lawyer.\n\nIn cases where the insured has the freedom to choose their defense lawyer, it is advisable to select a lawyer with experience in the relevant legal area to ensure effective representation and protection of their rights.\n\n1. 1. An insured individual's ability to choose their own defense lawyer can depend on the terms of their insurance policy and local laws.\n2. 2. Some insurance policies may include a clause that allows the insured to select their own lawyer.\n3. 3. Liability insurance policies often permit the insured to choose their own defense lawyer.\n4. 4. If a conflict of interest arises, such as the insurance company's lawyer representing both the insured and the insurer, the insured may be allowed to select their own lawyer.\n5. 5. In certain jurisdictions, the insured has the right to choose their own lawyer in criminal cases.\n6. 6. This right to choose a lawyer in criminal cases may exist even if the insurance policy typically requires the use of a lawyer provided by the insurance company.\n7. 7. A recourse guarantee is a clause in an insurance contract that allows the insured to recover costs from a third party responsible for the loss.\n8. 8. If an insured individual has a recourse guarantee, they may be able to choose their own lawyer to pursue the recourse action.\n9. 9. Even when the insured can choose their own lawyer, the insurance company may still need to approve the choice.\n10. 10. The insurer will typically only cover legal fees up to a certain amount.\n11. 11. It is recommended to refer to the specific insurance policy and consult with a legal professional for advice.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:24.034687",
                "duration": 1.443929,
                "cost": 0.0171
              },
              "meta": {},
              "text": "Validated: ['1', '2', '9', '11'] - Not validated: ['3', '4', '5', '6', '7', '8', '10']",
              "human": 1.0,
              "auto": 0.36363636363636365
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "In what situations does MAAF Assistance intervene for psychological trauma?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. MAAF Assistance offers help in instances of psychological trauma.\n2. They organize and pay for a phone consultation with a clinical psychologist.\n3. If needed, MAAF Assistance can set up an in-person meeting with a psychologist.\n4. They can also arrange for a subsequent meeting if it is required.\n5. The text does not clearly state the specific situations or conditions for providing this assistance.",
          "prompt": {
            "meta": {},
            "user": "MAAF Assistance provides support in cases of psychological trauma by organizing and covering the costs of a telephone consultation with a clinical psychologist. If necessary, they can also arrange for a face-to-face meeting with a psychologist, and even a follow-up meeting if required. The specific situations or conditions under which this assistance is provided are not explicitly stated in the text.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:12.892874",
          "duration": 6.404534,
          "cost": 0.010350000000000002
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. MAAF Assistance offers help in instances of psychological trauma."
          },
          {
            "meta": {},
            "text": "2. They organize and pay for a phone consultation with a clinical psychologist."
          },
          {
            "meta": {},
            "text": "3. If needed, MAAF Assistance can set up an in-person meeting with a psychologist."
          },
          {
            "meta": {},
            "text": "4. They can also arrange for a subsequent meeting if it is required."
          },
          {
            "meta": {},
            "text": "5. The text does not clearly state the specific situations or conditions for providing this assistance."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.148711001676539,
              "Node id": "c50ab65d-3cc1-4fc7-9e3f-23d515293896"
            },
            "text": "Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement."
          },
          {
            "meta": {
              "score": 8.824625784537997,
              "Node id": "856a33df-4139-4386-b516-1d5a75f183c2"
            },
            "text": "Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales."
          },
          {
            "meta": {
              "score": 8.71247308411479,
              "Node id": "3d9a44ab-d8b0-408f-88e0-00527b44d48b"
            },
            "text": "et Protection Juridique Automobile  “.\nÀ défaut de règlement amiable ,nous poursui -\nvons notre assistance dans le cadre judiciaire.\n> En cas de procédure judiciaire\nLorsqu’aucune issue amiable n’est possible, ou \nlorsque vous faites l’objet d’une action judiciaire, \nnous vous proposons de saisir un avocat. \nVous disposez du libre choix de votre avocat. Vous \ndevez nous communiquer par écrit ses coordon -\nnées. Devant les juridictions de France métropo -\nlitaine, si vous le souhaitez, nous pouvons, sur \nvotre demande écrite, vous communiquer les \ncoordonnées d’un avocat. \nNous vous recommandons de demander notre \naccord écrit préalable avant de le saisir. En effet, \nnous refuserons de prendre en charge les frais \net honoraires de votre conseil pour les interven -\ntions qu’il aura effectuées avant votre décla -\nration de litige , sauf si vous êtes en mesure de \njustifier d’une situation d’urgence avérée."
          },
          {
            "meta": {
              "score": 8.571591407879506,
              "Node id": "e15f3242-9845-4436-9659-55a64b08e14a"
            },
            "text": "CONDITIONS D’INTERVENTION \n     Les dépenses engagées sans l’accord préalable de \nMAAF Assistance, resteront à votre charge, de même que les dépenses que vous auriez dû normalement engager en l’absence de l’événement donnant lieu à l’intervention.\n     Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales."
          },
          {
            "meta": {
              "score": 8.510987107978659,
              "Node id": "2f44ad34-248c-4f3e-805d-64506902f514"
            },
            "text": "144–150.\n[132]S. Jain and B. C. Wallace ,Attention is not explanation ,arXiv\npreprint arXiv:1902.10186, (2019).\n[133]W. James and F. H. Burkhardt ,The principles of psychology,\nthe works of William James , Transactions of the Charles S. Peirce\nSociety, 19 (1983).\n[134]H. Jiang, B. Kim, M. Y. Guan, and M. R. Gupta ,To trust\nor not to trust a classiﬁer. , in NeurIPS, 2018, pp. 5546–5557."
          },
          {
            "meta": {
              "score": 8.42661818420032,
              "Node id": "ca99e911-4c23-4043-8bd3-2e2ededa55c1"
            },
            "text": "Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés."
          },
          {
            "meta": {
              "score": 8.343676066345061,
              "Node id": "acb5f627-2aeb-452a-8793-68ae33d5e430"
            },
            "text": "b) Soutien psychologique  \nOrganisation et prise en charge selon la situation d’un entretien téléphonique avec un psychologue clinicien et, si nécessaire, d’un \nentretien en vis -à-vis, voire d’un entretien en vis -à-vis complémentaire.  \n \nc) Aide à domicile  \nLes prestations garanties sont les suivantes :  \n aide -ménagère ,  \n présence d’un proche en France  ou dans la Principauté de Monaco,  \n transfert du bénéficiaire chez un proche en France  ou dans la Principauté de Monaco,  \n(hors transport médicalisé)  \n prise en charge des enfants (de moins de 16 ans) ou des enfants  atteints d’un handicap (sans limite d’âge) :  \n- déplacement d’un proche au domicile,  \n- transfert des enfants au domicile d’un proche,  \n- garde des enfants au domicile par un intervenant habilité,"
          },
          {
            "meta": {
              "score": 8.332663396657484,
              "Node id": "f5e055bd-1fa1-49ab-a2ea-d552f1ea1629"
            },
            "text": "Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement.\n     MAAF Assistance ne peut se substituer aux ser- \nvices publics, sapeurs-pompiers notamment, auxquels il doit être fait appel en cas d’incendie, explosions, etc."
          },
          {
            "meta": {
              "score": 8.1955949957088,
              "Node id": "85c54521-cbfd-4569-8648-8ecf3b84598a"
            },
            "text": "Maladie : altération soudaine et imprévisible de la santé, \nconsécutive ou non à une situation préexistante, n’ayant pas pour origine un accident corporel*, constatée par une auto-rité médicale compétente et qui empêche la continuation normale du voyage ou du séjour.\nExclusions\nNi les voyages à visée diagnostique et/ou thérapeutique, c'est-à-dire ayant pour objectif de consulter un praticien ou d’être hospitalisé, ni les retours pour greffe d’organe, ne peuvent être considérés comme des événements don-nant droit à une assistance au titre de la maladie si celle-ci n’est pas justifiée par une altération soudaine et imprévisible de l’état de santé au cours du voyage.\n  Prestations\nRapatriement :  sur décision de ses médecins, MAAF \nAssistance organise et prend en charge le rapatriement du bénéficiaire jusqu’à son domicile en France** ou dans un hôpital adapté le plus proche de son domicile en France**."
          },
          {
            "meta": {
              "score": 7.924858943059206,
              "Node id": "b6a0c534-5263-42e0-b312-145403ed4ff6"
            },
            "text": "Pour bénéficier d’une prise en charge financière \ndes frais de justice tout au long de votre dossier et \nquelle que soit la nature de la dépense envisagée \n(frais d’expertise amiable ou judiciaire, commissaire \nde justice * intervenant pour tous les actes relevant \nanciennement des missions d’huissier de justice. \navocat…), vous devez recueillir notre accord écrit \npréalable avant qu’elle ne soit engagée, sauf si vous \npouvez justifier d’une situation d’urgence avérée.\nVous pouvez nous contacter au 01 76 62 45 69. Un \njuriste vous donnera toute information utile pour la \nconstitution de votre dossier."
          },
          {
            "meta": {
              "score": 0.7516688412015734,
              "Node id": "3063710c-86bf-4f4e-9fd2-715230580212"
            },
            "text": "The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]."
          },
          {
            "meta": {
              "score": 0.7478339849361981,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.744699203447162,
              "Node id": "0c9c6a00-3a2f-4fa3-8c82-2a0db77bf021"
            },
            "text": "Then train an adversarial model ( Ma) that minimizes the output predic-\ntionscorefromthebasemodelyetmaximizingthechangesinthelearned\nattention distribution from the base model using following instance-wide\nloss function:\nL(Ma,Mb) =TVD (ˆy(i)\n1,ˆy(i)\n2)−λKL(α(i)\na∥α(i)\nb)(7.24)\nwhere ˆy(i)andα(i) denote predictions and attention distributions for an\ninstance i, and λcontrols the trade-oﬀ between the prediction distance\nand attention distribution changes.\nForthediabetesdataset(andtheanemiaandtheIMDB),theyﬁndit\nchallenging to ﬁnd adversarial weights that produce divergent attention\nweights and not losing the predictive performance, thus supporting the\nuse of attention for the faithful explanation. On the contrary, Stanford\nSentiment Treebank (SST) dataset shows no use of attention weights for\nthe faithful explanation."
          },
          {
            "meta": {
              "score": 0.7417835671866233,
              "Node id": "e3c25eb8-df58-4f4e-9c09-ad214f112569"
            },
            "text": "Then, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al. used a much\nlarger common-crawl corpus to train XML-R. Other models use cus-\ntom crawled data."
          },
          {
            "meta": {
              "score": 0.7404085713127602,
              "Node id": "28bd6920-d38e-4e15-adb1-3b5a928f9d5d"
            },
            "text": "•Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems."
          },
          {
            "meta": {
              "score": 0.7399553012185633,
              "Node id": "54937c3c-19b4-4d86-ab6a-04684c014713"
            },
            "text": "88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR. In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder."
          },
          {
            "meta": {
              "score": 0.7384481704488379,
              "Node id": "2e222eb3-8c36-4eca-b429-3de5d4dc99f8"
            },
            "text": "88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR."
          },
          {
            "meta": {
              "score": 0.7381863094679716,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7366635925831694,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7366543941092432,
              "Node id": "5acf8384-b218-4072-b011-2d3b55b37bd9"
            },
            "text": "It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "MAAF Assistance provides support in cases of psychological trauma by organizing and covering the costs of a telephone consultation with a clinical psychologist. If necessary, they can also arrange for a face-to-face meeting with a psychologist, and even a follow-up meeting if required. The specific situations or conditions under which this assistance is provided are not explicitly stated in the text.",
              "prompt": {
                "meta": {},
                "user": "In what situations does MAAF Assistance intervene for psychological trauma?",
                "system": "Contexte :  Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement. \n\n Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales. \n\n et Protection Juridique Automobile  “.\nÀ défaut de règlement amiable ,nous poursui -\nvons notre assistance dans le cadre judiciaire.\n> En cas de procédure judiciaire\nLorsqu’aucune issue amiable n’est possible, ou \nlorsque vous faites l’objet d’une action judiciaire, \nnous vous proposons de saisir un avocat. \nVous disposez du libre choix de votre avocat. Vous \ndevez nous communiquer par écrit ses coordon -\nnées. Devant les juridictions de France métropo -\nlitaine, si vous le souhaitez, nous pouvons, sur \nvotre demande écrite, vous communiquer les \ncoordonnées d’un avocat. \nNous vous recommandons de demander notre \naccord écrit préalable avant de le saisir. En effet, \nnous refuserons de prendre en charge les frais \net honoraires de votre conseil pour les interven -\ntions qu’il aura effectuées avant votre décla -\nration de litige , sauf si vous êtes en mesure de \njustifier d’une situation d’urgence avérée. \n\n CONDITIONS D’INTERVENTION \n     Les dépenses engagées sans l’accord préalable de \nMAAF Assistance, resteront à votre charge, de même que les dépenses que vous auriez dû normalement engager en l’absence de l’événement donnant lieu à l’intervention.\n     Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales. \n\n 144–150.\n[132]S. Jain and B. C. Wallace ,Attention is not explanation ,arXiv\npreprint arXiv:1902.10186, (2019).\n[133]W. James and F. H. Burkhardt ,The principles of psychology,\nthe works of William James , Transactions of the Charles S. Peirce\nSociety, 19 (1983).\n[134]H. Jiang, B. Kim, M. Y. Guan, and M. R. Gupta ,To trust\nor not to trust a classiﬁer. , in NeurIPS, 2018, pp. 5546–5557. \n\n Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés. \n\n b) Soutien psychologique  \nOrganisation et prise en charge selon la situation d’un entretien téléphonique avec un psychologue clinicien et, si nécessaire, d’un \nentretien en vis -à-vis, voire d’un entretien en vis -à-vis complémentaire.  \n \nc) Aide à domicile  \nLes prestations garanties sont les suivantes :  \n aide -ménagère ,  \n présence d’un proche en France  ou dans la Principauté de Monaco,  \n transfert du bénéficiaire chez un proche en France  ou dans la Principauté de Monaco,  \n(hors transport médicalisé)  \n prise en charge des enfants (de moins de 16 ans) ou des enfants  atteints d’un handicap (sans limite d’âge) :  \n- déplacement d’un proche au domicile,  \n- transfert des enfants au domicile d’un proche,  \n- garde des enfants au domicile par un intervenant habilité, \n\n Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement.\n     MAAF Assistance ne peut se substituer aux ser- \nvices publics, sapeurs-pompiers notamment, auxquels il doit être fait appel en cas d’incendie, explosions, etc. \n\n Maladie : altération soudaine et imprévisible de la santé, \nconsécutive ou non à une situation préexistante, n’ayant pas pour origine un accident corporel*, constatée par une auto-rité médicale compétente et qui empêche la continuation normale du voyage ou du séjour.\nExclusions\nNi les voyages à visée diagnostique et/ou thérapeutique, c'est-à-dire ayant pour objectif de consulter un praticien ou d’être hospitalisé, ni les retours pour greffe d’organe, ne peuvent être considérés comme des événements don-nant droit à une assistance au titre de la maladie si celle-ci n’est pas justifiée par une altération soudaine et imprévisible de l’état de santé au cours du voyage.\n  Prestations\nRapatriement :  sur décision de ses médecins, MAAF \nAssistance organise et prend en charge le rapatriement du bénéficiaire jusqu’à son domicile en France** ou dans un hôpital adapté le plus proche de son domicile en France**. \n\n Pour bénéficier d’une prise en charge financière \ndes frais de justice tout au long de votre dossier et \nquelle que soit la nature de la dépense envisagée \n(frais d’expertise amiable ou judiciaire, commissaire \nde justice * intervenant pour tous les actes relevant \nanciennement des missions d’huissier de justice. \navocat…), vous devez recueillir notre accord écrit \npréalable avant qu’elle ne soit engagée, sauf si vous \npouvez justifier d’une situation d’urgence avérée.\nVous pouvez nous contacter au 01 76 62 45 69. Un \njuriste vous donnera toute information utile pour la \nconstitution de votre dossier. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Then train an adversarial model ( Ma) that minimizes the output predic-\ntionscorefromthebasemodelyetmaximizingthechangesinthelearned\nattention distribution from the base model using following instance-wide\nloss function:\nL(Ma,Mb) =TVD (ˆy(i)\n1,ˆy(i)\n2)−λKL(α(i)\na∥α(i)\nb)(7.24)\nwhere ˆy(i)andα(i) denote predictions and attention distributions for an\ninstance i, and λcontrols the trade-oﬀ between the prediction distance\nand attention distribution changes.\nForthediabetesdataset(andtheanemiaandtheIMDB),theyﬁndit\nchallenging to ﬁnd adversarial weights that produce divergent attention\nweights and not losing the predictive performance, thus supporting the\nuse of attention for the faithful explanation. On the contrary, Stanford\nSentiment Treebank (SST) dataset shows no use of attention weights for\nthe faithful explanation. \n\n Then, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al. used a much\nlarger common-crawl corpus to train XML-R. Other models use cus-\ntom crawled data. \n\n •Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. \n\n 88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR. In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder. \n\n 88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al. \n\n \n La question est In what situations does MAAF Assistance intervene for psychological trauma?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:05:08.985201",
              "duration": 6.956603,
              "cost": 0.13683
            },
            "meta": {},
            "text": "MAAF Assistance provides support in cases of psychological trauma by organizing and covering the costs of a telephone consultation with a clinical psychologist. If necessary, they can also arrange for a face-to-face meeting with a psychologist, and even a follow-up meeting if required. The specific situations or conditions under which this assistance is provided are not explicitly stated in the text.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: MAAF Assistance provides support in cases of psychological trauma by organizing and covering the costs of a telephone consultation with a clinical psychologist. If necessary, they can also arrange for a face-to-face meeting with a psychologist, and even a follow-up meeting if required. The specific situations or conditions under which this assistance is provided are not explicitly stated in the text.\n\n1. 1. MAAF Assistance offers help in instances of psychological trauma.\n2. 2. They organize and pay for a phone consultation with a clinical psychologist.\n3. 3. If needed, MAAF Assistance can set up an in-person meeting with a psychologist.\n4. 4. They can also arrange for a subsequent meeting if it is required.\n5. 5. The text does not clearly state the specific situations or conditions for providing this assistance.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:26.336856",
                "duration": 2.298583,
                "cost": 0.00879
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "MAAF Assistance intervenes for psychological trauma by organizing and providing support such as a telephone conversation with a clinical psychologist, face-to-face meetings if necessary, and additional face-to-face meetings if needed. They also offer assistance at home, including household help, the presence of a loved one in France or Monaco, transferring the beneficiary to a loved one's home in France or Monaco (excluding medical transport), and taking care of children or disabled children by arranging for a loved one to come to the home, transferring the children to a loved one's home, or providing childcare at home by an authorized caregiver.",
              "prompt": {
                "meta": {},
                "user": "In what situations does MAAF Assistance intervene for psychological trauma?",
                "system": "Contexte :  Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement. \n\n Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales. \n\n et Protection Juridique Automobile  “.\nÀ défaut de règlement amiable ,nous poursui -\nvons notre assistance dans le cadre judiciaire.\n> En cas de procédure judiciaire\nLorsqu’aucune issue amiable n’est possible, ou \nlorsque vous faites l’objet d’une action judiciaire, \nnous vous proposons de saisir un avocat. \nVous disposez du libre choix de votre avocat. Vous \ndevez nous communiquer par écrit ses coordon -\nnées. Devant les juridictions de France métropo -\nlitaine, si vous le souhaitez, nous pouvons, sur \nvotre demande écrite, vous communiquer les \ncoordonnées d’un avocat. \nNous vous recommandons de demander notre \naccord écrit préalable avant de le saisir. En effet, \nnous refuserons de prendre en charge les frais \net honoraires de votre conseil pour les interven -\ntions qu’il aura effectuées avant votre décla -\nration de litige , sauf si vous êtes en mesure de \njustifier d’une situation d’urgence avérée. \n\n CONDITIONS D’INTERVENTION \n     Les dépenses engagées sans l’accord préalable de \nMAAF Assistance, resteront à votre charge, de même que les dépenses que vous auriez dû normalement engager en l’absence de l’événement donnant lieu à l’intervention.\n     Lorsque MAAF Assistance organise et prend en charge \nun rapatriement ou un transport, il est demandé au bénéficiaire d’utiliser son titre de voyage.\n     Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales. \n\n 144–150.\n[132]S. Jain and B. C. Wallace ,Attention is not explanation ,arXiv\npreprint arXiv:1902.10186, (2019).\n[133]W. James and F. H. Burkhardt ,The principles of psychology,\nthe works of William James , Transactions of the Charles S. Peirce\nSociety, 19 (1983).\n[134]H. Jiang, B. Kim, M. Y. Guan, and M. R. Gupta ,To trust\nor not to trust a classiﬁer. , in NeurIPS, 2018, pp. 5546–5557. \n\n Lorsque MAAF Assistance a assuré à ses frais le retour \ndu bénéficiaire, il est demandé à ce dernier d’effectuer les démarches nécessaires au remboursement de ses titres de transport non utilisés, et de reverser le mon- tant perçu à MAAF Assistance, sous un délai maximum de trois mois suivant la date du retour.\n     Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés. \n\n b) Soutien psychologique  \nOrganisation et prise en charge selon la situation d’un entretien téléphonique avec un psychologue clinicien et, si nécessaire, d’un \nentretien en vis -à-vis, voire d’un entretien en vis -à-vis complémentaire.  \n \nc) Aide à domicile  \nLes prestations garanties sont les suivantes :  \n aide -ménagère ,  \n présence d’un proche en France  ou dans la Principauté de Monaco,  \n transfert du bénéficiaire chez un proche en France  ou dans la Principauté de Monaco,  \n(hors transport médicalisé)  \n prise en charge des enfants (de moins de 16 ans) ou des enfants  atteints d’un handicap (sans limite d’âge) :  \n- déplacement d’un proche au domicile,  \n- transfert des enfants au domicile d’un proche,  \n- garde des enfants au domicile par un intervenant habilité, \n\n Les délais d’intervention et les prestations de MAAF \nAssistance sont fonction de la gravité de la situation locale et/ou des possibilités offertes par les infrastruc- tures locales.\n     MAAF Assistance n’intervient pas dans les cas sui-\nvants : \n     MAAF Assistance ne peut intervenir que dans  \nla limite des accords donnés par les autorités locales.\n     MAAF Assistance ne peut en aucun cas se subs-\ntituer aux organismes locaux de secours d’ur- gence, ni prendre en charge les frais ainsi enga- gés.\n     MAAF Assistance ne sera pas tenue d’intervenir \ndans les cas où le bénéficiaire aurait commis de façon volontaire, un crime ou un délit au regard de la législation en vigueur dans le pays de l’évè-nement.\n     MAAF Assistance ne peut se substituer aux ser- \nvices publics, sapeurs-pompiers notamment, auxquels il doit être fait appel en cas d’incendie, explosions, etc. \n\n Maladie : altération soudaine et imprévisible de la santé, \nconsécutive ou non à une situation préexistante, n’ayant pas pour origine un accident corporel*, constatée par une auto-rité médicale compétente et qui empêche la continuation normale du voyage ou du séjour.\nExclusions\nNi les voyages à visée diagnostique et/ou thérapeutique, c'est-à-dire ayant pour objectif de consulter un praticien ou d’être hospitalisé, ni les retours pour greffe d’organe, ne peuvent être considérés comme des événements don-nant droit à une assistance au titre de la maladie si celle-ci n’est pas justifiée par une altération soudaine et imprévisible de l’état de santé au cours du voyage.\n  Prestations\nRapatriement :  sur décision de ses médecins, MAAF \nAssistance organise et prend en charge le rapatriement du bénéficiaire jusqu’à son domicile en France** ou dans un hôpital adapté le plus proche de son domicile en France**. \n\n Pour bénéficier d’une prise en charge financière \ndes frais de justice tout au long de votre dossier et \nquelle que soit la nature de la dépense envisagée \n(frais d’expertise amiable ou judiciaire, commissaire \nde justice * intervenant pour tous les actes relevant \nanciennement des missions d’huissier de justice. \navocat…), vous devez recueillir notre accord écrit \npréalable avant qu’elle ne soit engagée, sauf si vous \npouvez justifier d’une situation d’urgence avérée.\nVous pouvez nous contacter au 01 76 62 45 69. Un \njuriste vous donnera toute information utile pour la \nconstitution de votre dossier. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Then train an adversarial model ( Ma) that minimizes the output predic-\ntionscorefromthebasemodelyetmaximizingthechangesinthelearned\nattention distribution from the base model using following instance-wide\nloss function:\nL(Ma,Mb) =TVD (ˆy(i)\n1,ˆy(i)\n2)−λKL(α(i)\na∥α(i)\nb)(7.24)\nwhere ˆy(i)andα(i) denote predictions and attention distributions for an\ninstance i, and λcontrols the trade-oﬀ between the prediction distance\nand attention distribution changes.\nForthediabetesdataset(andtheanemiaandtheIMDB),theyﬁndit\nchallenging to ﬁnd adversarial weights that produce divergent attention\nweights and not losing the predictive performance, thus supporting the\nuse of attention for the faithful explanation. On the contrary, Stanford\nSentiment Treebank (SST) dataset shows no use of attention weights for\nthe faithful explanation. \n\n Then, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al. used a much\nlarger common-crawl corpus to train XML-R. Other models use cus-\ntom crawled data. \n\n •Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. \n\n 88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR. In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder. \n\n 88■Transformers for Machine Learning: A Deep Dive\nFigure 4.7 Illustration of mUSE architecture for QA, NLI and the bridg-\ning translation tasks [284].\nmUSE uses dual-encoders to perform multitask learning on tasks\nthat can be represented as ranking input-response sentence pairs. Some\ntasks fall naturally within this framework such as QA and translation\ntasks.OthertasksmightneedaspecialreformulationsuchasNLI.Input-\nresponse ranking can be formally described as follows: For an input-\nresponse pair (sI\ni,sR\ni), the goal is to get sR\nito higher than all possible re-\nsponsessR\nj∈SR. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. Whereas, in [61], Conneau et al. \n\n \n La question est In what situations does MAAF Assistance intervene for psychological trauma?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:05:12.657339",
              "duration": 3.667707,
              "cost": 0.0023889999999999996
            },
            "meta": {},
            "text": "MAAF Assistance intervenes for psychological trauma by organizing and providing support such as a telephone conversation with a clinical psychologist, face-to-face meetings if necessary, and additional face-to-face meetings if needed. They also offer assistance at home, including household help, the presence of a loved one in France or Monaco, transferring the beneficiary to a loved one's home in France or Monaco (excluding medical transport), and taking care of children or disabled children by arranging for a loved one to come to the home, transferring the children to a loved one's home, or providing childcare at home by an authorized caregiver.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: MAAF Assistance intervenes for psychological trauma by organizing and providing support such as a telephone conversation with a clinical psychologist, face-to-face meetings if necessary, and additional face-to-face meetings if needed. They also offer assistance at home, including household help, the presence of a loved one in France or Monaco, transferring the beneficiary to a loved one's home in France or Monaco (excluding medical transport), and taking care of children or disabled children by arranging for a loved one to come to the home, transferring the children to a loved one's home, or providing childcare at home by an authorized caregiver.\n\n1. 1. MAAF Assistance offers help in instances of psychological trauma.\n2. 2. They organize and pay for a phone consultation with a clinical psychologist.\n3. 3. If needed, MAAF Assistance can set up an in-person meeting with a psychologist.\n4. 4. They can also arrange for a subsequent meeting if it is required.\n5. 5. The text does not clearly state the specific situations or conditions for providing this assistance.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:27.457472",
                "duration": 1.117798,
                "cost": 0.01008
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4'] - Not validated: ['5']",
              "human": 1.0,
              "auto": 0.8
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What additional options and costs are the responsibility of the renter when using a rental vehicle?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not specify the additional options available for a rental vehicle.\n2. There is no information about the costs that the renter has to bear.\n3. The responsibilities of the renter when using a rental vehicle are not mentioned in the text.\n4. The text lacks details on what is expected from the renter when using a rental vehicle.\n5. The text does not clarify the financial obligations of the renter for a rental vehicle.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on what additional options and costs are the responsibility of the renter when using a rental vehicle.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:19.954168",
          "duration": 7.058207,
          "cost": 0.00972
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not specify the additional options available for a rental vehicle."
          },
          {
            "meta": {},
            "text": "2. There is no information about the costs that the renter has to bear."
          },
          {
            "meta": {},
            "text": "3. The responsibilities of the renter when using a rental vehicle are not mentioned in the text."
          },
          {
            "meta": {},
            "text": "4. The text lacks details on what is expected from the renter when using a rental vehicle."
          },
          {
            "meta": {},
            "text": "5. The text does not clarify the financial obligations of the renter for a rental vehicle."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.847863444717856,
              "Node id": "45bc1d98-4185-4a4d-b725-78eea963ee7b"
            },
            "text": "C H A P T E R 3\nBidirectional Encoder\nRepresentations from\nTransformers (BERT)\nTheadvent of Bidirectional Encoder Representations from Trans-\nformer (BERT) [72] is considered the onset of a revolution in the\nﬁeld of Natural Language Processing (NLP). BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures."
          },
          {
            "meta": {
              "score": 13.197637526975484,
              "Node id": "a53b68ed-38fa-45f7-8fce-cbe4899e1ae0"
            },
            "text": "BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]."
          },
          {
            "meta": {
              "score": 11.467298716980773,
              "Node id": "0a65c011-14c0-4abe-92c6-c869e64eb1de"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes."
          },
          {
            "meta": {
              "score": 11.103504139987372,
              "Node id": "9a91b498-65d8-474a-b31e-d561bdcb75d2"
            },
            "text": "The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers."
          },
          {
            "meta": {
              "score": 10.80956559063321,
              "Node id": "76b48bf1-c859-46ff-8db1-c8085cc32758"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained."
          },
          {
            "meta": {
              "score": 10.681271394009249,
              "Node id": "9e66a9c7-a06d-41ec-8dfe-e64db0a7a819"
            },
            "text": "The reformulation of training tasks means you can use the same\nmodel and loss function on any NLP task. Using text-to-text trans-\nformations (sequence transduction) allows you to train one model on\nmultiple tasks at once, reusing model architecture, loss functions, and\nhyperparameters. Appendix D of Raﬀel et al. has examples of how in-\nput was formatted for each of the datasets the T5 model was trained or\nﬁne-tuned on.\n5.4 TRANSFORMER SUBMODULE CHANGES\nThissectiondiscussesmodiﬁcationstotheTransformerthatdonotmod-\nify the attention mechanism or the model’s memory proﬁle.\n5.4.1 Switch Transformer\nThe Mixture of Experts (MoE) model has contributed to many suc-\ncesses at the expense of complexity and training cost [225]. However,\nthe mixture of expert models does not share the parameters, resulting"
          },
          {
            "meta": {
              "score": 10.412685675761827,
              "Node id": "bd8fc88e-4b30-4cae-b91a-ff7244ca59e5"
            },
            "text": "Additionally, XNLG does a two-stage optimization of\nparameters. It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training."
          },
          {
            "meta": {
              "score": 10.395772017072723,
              "Node id": "955cc409-8830-43ca-8144-8a9fce3caf00"
            },
            "text": "18■Transformers for Machine Learning: A Deep Dive\nFigure 2.5 Encoder-decoder with attention layer.\na sigmoid function to combine them with the values, as given by\nα(q,k) =vTtanh(Wkk+Wqq) (2.14)\nScaled dot product, or dot product-based scoring, are faster and more\nmemory eﬃcient than additive attention mechanisms.\n2.3.3 Attention-Based Sequence-to-Sequence\nThe general changes to an encoder-decoder with the addition of an at-\ntention layer and the mapping to query, keys, and values are shown in\nFig. 2.5 and are\n1. The output of the last encoder states are used as keys kand values\nv\n2. The output of the last decoder state, at time t−1is used as query\nq\n3. The output from the attention layer o, the context variable, is used\nfor the next decoder state t"
          },
          {
            "meta": {
              "score": 10.38269753651227,
              "Node id": "5f497868-3eb4-485c-8ba0-ee2de8adb332"
            },
            "text": "In the recent past, training NLP\nalgorithms worked only with labeled data, which hindered their progress\nfor years due to the limited availability and cost of collecting data for\nevery new task and every new project. One major contribution of BERT\nis introducing a new pipeline for training NLP systems where algorithms\ncan learn about core and generic natural language concepts from huge\namount of cheap unlabeled data, a process also known as self-supervised\nlearning or model pre-training. A pre-trained model is then ﬁne-tuned\nfor any speciﬁc downstream task using much smaller task-speciﬁc\nlabeled datasets. This new pipeline of pre-training followed by ﬁne-\ntuning constitutes the core of most advances in the machine learning\nﬁeld. The major success of BERT for English NLP tasks has motivated\nits use for other languages. However, using BERT’s pipeline is only pos-\nsible for languages with suﬃciently large unlabeled data for pre-training."
          },
          {
            "meta": {
              "score": 10.372577745874292,
              "Node id": "56f57d30-7b37-4891-bcda-f0686319582c"
            },
            "text": "Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost."
          },
          {
            "meta": {
              "score": 0.7199665491565337,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.714589375321343,
              "Node id": "173dddf4-c179-4905-af71-765ba3abaaef"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...!"
          },
          {
            "meta": {
              "score": 0.7101078375829292,
              "Node id": "72c0c068-6d28-4724-a9de-815e352cf5e6"
            },
            "text": "To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis."
          },
          {
            "meta": {
              "score": 0.7098528911953761,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          },
          {
            "meta": {
              "score": 0.7090480202808015,
              "Node id": "631994bd-f09c-427f-bcb2-c78a1ebab13f"
            },
            "text": "The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities."
          },
          {
            "meta": {
              "score": 0.7090453467464524,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.709029418090284,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7051305435488789,
              "Node id": "20010ae9-c968-4c58-8312-204c37d847a5"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 0.7047623007648373,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.704459632980626,
              "Node id": "73fee44f-f78a-490b-a010-506c0aa781cb"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on what additional options and costs are the responsibility of the renter when using a rental vehicle.",
              "prompt": {
                "meta": {},
                "user": "What additional options and costs are the responsibility of the renter when using a rental vehicle?",
                "system": "Contexte :  C H A P T E R 3\nBidirectional Encoder\nRepresentations from\nTransformers (BERT)\nTheadvent of Bidirectional Encoder Representations from Trans-\nformer (BERT) [72] is considered the onset of a revolution in the\nﬁeld of Natural Language Processing (NLP). BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. \n\n BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. \n\n The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. \n\n The reformulation of training tasks means you can use the same\nmodel and loss function on any NLP task. Using text-to-text trans-\nformations (sequence transduction) allows you to train one model on\nmultiple tasks at once, reusing model architecture, loss functions, and\nhyperparameters. Appendix D of Raﬀel et al. has examples of how in-\nput was formatted for each of the datasets the T5 model was trained or\nﬁne-tuned on.\n5.4 TRANSFORMER SUBMODULE CHANGES\nThissectiondiscussesmodiﬁcationstotheTransformerthatdonotmod-\nify the attention mechanism or the model’s memory proﬁle.\n5.4.1 Switch Transformer\nThe Mixture of Experts (MoE) model has contributed to many suc-\ncesses at the expense of complexity and training cost [225]. However,\nthe mixture of expert models does not share the parameters, resulting \n\n Additionally, XNLG does a two-stage optimization of\nparameters. It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. \n\n 18■Transformers for Machine Learning: A Deep Dive\nFigure 2.5 Encoder-decoder with attention layer.\na sigmoid function to combine them with the values, as given by\nα(q,k) =vTtanh(Wkk+Wqq) (2.14)\nScaled dot product, or dot product-based scoring, are faster and more\nmemory eﬃcient than additive attention mechanisms.\n2.3.3 Attention-Based Sequence-to-Sequence\nThe general changes to an encoder-decoder with the addition of an at-\ntention layer and the mapping to query, keys, and values are shown in\nFig. 2.5 and are\n1. The output of the last encoder states are used as keys kand values\nv\n2. The output of the last decoder state, at time t−1is used as query\nq\n3. The output from the attention layer o, the context variable, is used\nfor the next decoder state t \n\n In the recent past, training NLP\nalgorithms worked only with labeled data, which hindered their progress\nfor years due to the limited availability and cost of collecting data for\nevery new task and every new project. One major contribution of BERT\nis introducing a new pipeline for training NLP systems where algorithms\ncan learn about core and generic natural language concepts from huge\namount of cheap unlabeled data, a process also known as self-supervised\nlearning or model pre-training. A pre-trained model is then ﬁne-tuned\nfor any speciﬁc downstream task using much smaller task-speciﬁc\nlabeled datasets. This new pipeline of pre-training followed by ﬁne-\ntuning constitutes the core of most advances in the machine learning\nﬁeld. The major success of BERT for English NLP tasks has motivated\nits use for other languages. However, using BERT’s pipeline is only pos-\nsible for languages with suﬃciently large unlabeled data for pre-training. \n\n Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n \n La question est What additional options and costs are the responsibility of the renter when using a rental vehicle?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:05:15.504958",
              "duration": 2.834824,
              "cost": 0.13164
            },
            "meta": {},
            "text": "The text does not provide information on what additional options and costs are the responsibility of the renter when using a rental vehicle.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on what additional options and costs are the responsibility of the renter when using a rental vehicle.\n\n1. 1. The text does not specify the additional options available for a rental vehicle.\n2. 2. There is no information about the costs that the renter has to bear.\n3. 3. The responsibilities of the renter when using a rental vehicle are not mentioned in the text.\n4. 4. The text lacks details on what is expected from the renter when using a rental vehicle.\n5. 5. The text does not clarify the financial obligations of the renter for a rental vehicle.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:29.033340",
                "duration": 1.572341,
                "cost": 0.00762
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '5'] - Not validated: ['4']",
              "human": 1.0,
              "auto": 0.8
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "When using a rental vehicle, there are several additional options and costs that are typically the responsibility of the renter. These may include:\n\n1. **Insurance**: The renter is usually responsible for purchasing insurance coverage for the rental vehicle. This may include options such as Collision Damage Waiver (CDW), Loss Damage Waiver (LDW), and Supplemental Liability Insurance (SLI).\n\n2. **Fuel**: The renter is generally responsible for refueling the vehicle before returning it. If the vehicle is returned with less fuel than when it was rented, the renter may be charged a refueling fee.\n\n3. **Additional Drivers**: Adding extra drivers to the rental agreement may come with an additional cost. Each additional driver needs to be listed on the rental agreement and may incur a fee.\n\n4. **Tolls and Traffic Violations**: Any tolls, parking fees, or traffic violations incurred during the rental period are typically the responsibility of the renter.\n\n5. **Upgrades**: If the renter chooses to upgrade to a larger vehicle, add GPS navigation, or opt for other optional features, there may be additional costs associated with these upgrades.\n\n6. **Young Driver Fees**: Drivers under a certain age (often 25) may incur additional fees due to their higher risk profile.\n\n7. **One-Way Rentals**: If the renter picks up the vehicle at one location and returns it to another, there may be a one-way drop-off fee.\n\nIt's essential for renters to carefully review the rental agreement and understand all the terms and conditions, including any additional options and costs that they may be responsible for when using a rental vehicle.",
              "prompt": {
                "meta": {},
                "user": "What additional options and costs are the responsibility of the renter when using a rental vehicle?",
                "system": "Contexte :  C H A P T E R 3\nBidirectional Encoder\nRepresentations from\nTransformers (BERT)\nTheadvent of Bidirectional Encoder Representations from Trans-\nformer (BERT) [72] is considered the onset of a revolution in the\nﬁeld of Natural Language Processing (NLP). BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. \n\n BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. \n\n The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. \n\n The reformulation of training tasks means you can use the same\nmodel and loss function on any NLP task. Using text-to-text trans-\nformations (sequence transduction) allows you to train one model on\nmultiple tasks at once, reusing model architecture, loss functions, and\nhyperparameters. Appendix D of Raﬀel et al. has examples of how in-\nput was formatted for each of the datasets the T5 model was trained or\nﬁne-tuned on.\n5.4 TRANSFORMER SUBMODULE CHANGES\nThissectiondiscussesmodiﬁcationstotheTransformerthatdonotmod-\nify the attention mechanism or the model’s memory proﬁle.\n5.4.1 Switch Transformer\nThe Mixture of Experts (MoE) model has contributed to many suc-\ncesses at the expense of complexity and training cost [225]. However,\nthe mixture of expert models does not share the parameters, resulting \n\n Additionally, XNLG does a two-stage optimization of\nparameters. It initially trains the encoder using MLM and TLM jobs.\nThen, in the second step, it repairs the encoder and trains the decoder\nusing the DAE and XAE tasks. This method eﬀectively pre-trains all\nparameters, and also bridges the gap between MLM pre-training and\nautoregressive decoding ﬁne-tuning.\n4.2 MULTILINGUAL DATA\n4.2.1 Pre-Training Data\nMultilingual Language Models (mLMs) use diﬀerent sources of data dur-\ning the pre-training stage. More speciﬁcally, large monolingual corpora\nare commonly used in individual languages and parallel corpora are\nused between some languages. The source of the monolingual corpora\nvaries for the diﬀerent existent mLMs. For example, mBERT [72] uses\nWikipedia for pre-training. \n\n 18■Transformers for Machine Learning: A Deep Dive\nFigure 2.5 Encoder-decoder with attention layer.\na sigmoid function to combine them with the values, as given by\nα(q,k) =vTtanh(Wkk+Wqq) (2.14)\nScaled dot product, or dot product-based scoring, are faster and more\nmemory eﬃcient than additive attention mechanisms.\n2.3.3 Attention-Based Sequence-to-Sequence\nThe general changes to an encoder-decoder with the addition of an at-\ntention layer and the mapping to query, keys, and values are shown in\nFig. 2.5 and are\n1. The output of the last encoder states are used as keys kand values\nv\n2. The output of the last decoder state, at time t−1is used as query\nq\n3. The output from the attention layer o, the context variable, is used\nfor the next decoder state t \n\n In the recent past, training NLP\nalgorithms worked only with labeled data, which hindered their progress\nfor years due to the limited availability and cost of collecting data for\nevery new task and every new project. One major contribution of BERT\nis introducing a new pipeline for training NLP systems where algorithms\ncan learn about core and generic natural language concepts from huge\namount of cheap unlabeled data, a process also known as self-supervised\nlearning or model pre-training. A pre-trained model is then ﬁne-tuned\nfor any speciﬁc downstream task using much smaller task-speciﬁc\nlabeled datasets. This new pipeline of pre-training followed by ﬁne-\ntuning constitutes the core of most advances in the machine learning\nﬁeld. The major success of BERT for English NLP tasks has motivated\nits use for other languages. However, using BERT’s pipeline is only pos-\nsible for languages with suﬃciently large unlabeled data for pre-training. \n\n Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n \n La question est What additional options and costs are the responsibility of the renter when using a rental vehicle?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:05:24.456503",
              "duration": 8.948324,
              "cost": 0.0026685
            },
            "meta": {},
            "text": "When using a rental vehicle, there are several additional options and costs that are typically the responsibility of the renter. These may include:\n\n1. **Insurance**: The renter is usually responsible for purchasing insurance coverage for the rental vehicle. This may include options such as Collision Damage Waiver (CDW), Loss Damage Waiver (LDW), and Supplemental Liability Insurance (SLI).\n\n2. **Fuel**: The renter is generally responsible for refueling the vehicle before returning it. If the vehicle is returned with less fuel than when it was rented, the renter may be charged a refueling fee.\n\n3. **Additional Drivers**: Adding extra drivers to the rental agreement may come with an additional cost. Each additional driver needs to be listed on the rental agreement and may incur a fee.\n\n4. **Tolls and Traffic Violations**: Any tolls, parking fees, or traffic violations incurred during the rental period are typically the responsibility of the renter.\n\n5. **Upgrades**: If the renter chooses to upgrade to a larger vehicle, add GPS navigation, or opt for other optional features, there may be additional costs associated with these upgrades.\n\n6. **Young Driver Fees**: Drivers under a certain age (often 25) may incur additional fees due to their higher risk profile.\n\n7. **One-Way Rentals**: If the renter picks up the vehicle at one location and returns it to another, there may be a one-way drop-off fee.\n\nIt's essential for renters to carefully review the rental agreement and understand all the terms and conditions, including any additional options and costs that they may be responsible for when using a rental vehicle.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "None of the facts are present in the response.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: When using a rental vehicle, there are several additional options and costs that are typically the responsibility of the renter. These may include:\n\n1. **Insurance**: The renter is usually responsible for purchasing insurance coverage for the rental vehicle. This may include options such as Collision Damage Waiver (CDW), Loss Damage Waiver (LDW), and Supplemental Liability Insurance (SLI).\n\n2. **Fuel**: The renter is generally responsible for refueling the vehicle before returning it. If the vehicle is returned with less fuel than when it was rented, the renter may be charged a refueling fee.\n\n3. **Additional Drivers**: Adding extra drivers to the rental agreement may come with an additional cost. Each additional driver needs to be listed on the rental agreement and may incur a fee.\n\n4. **Tolls and Traffic Violations**: Any tolls, parking fees, or traffic violations incurred during the rental period are typically the responsibility of the renter.\n\n5. **Upgrades**: If the renter chooses to upgrade to a larger vehicle, add GPS navigation, or opt for other optional features, there may be additional costs associated with these upgrades.\n\n6. **Young Driver Fees**: Drivers under a certain age (often 25) may incur additional fees due to their higher risk profile.\n\n7. **One-Way Rentals**: If the renter picks up the vehicle at one location and returns it to another, there may be a one-way drop-off fee.\n\nIt's essential for renters to carefully review the rental agreement and understand all the terms and conditions, including any additional options and costs that they may be responsible for when using a rental vehicle.\n\n1. 1. The text does not specify the additional options available for a rental vehicle.\n2. 2. There is no information about the costs that the renter has to bear.\n3. 3. The responsibilities of the renter when using a rental vehicle are not mentioned in the text.\n4. 4. The text lacks details on what is expected from the renter when using a rental vehicle.\n5. 5. The text does not clarify the financial obligations of the renter for a rental vehicle.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:31.114517",
                "duration": 2.078815,
                "cost": 0.01686
              },
              "meta": {},
              "text": "Validated: ['None of the facts are present in the response.'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Discuss the distinction between a \"passager à titre gratuit\" and a regular passenger in terms of insurance coverage."
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. A \"passager à titre gratuit\" is a passenger who travels without paying any fare.\n2. This passenger can voluntarily contribute to travel expenses.\n3. The vehicle owner's insurance policy usually covers a \"passager à titre gratuit\".\n4. In case of an accident, the insurance company would cover the costs for this passenger.\n5. A regular passenger, who pays for the service, may not be covered by the vehicle owner's insurance.\n6. Regular passengers might need their own personal insurance for potential injuries or damages.\n7. Insurance coverage details can vary based on the policy terms and local regulations.\n8. It is crucial to review an insurance policy to understand who is covered.",
          "prompt": {
            "meta": {},
            "user": "A \"passager à titre gratuit\" refers to a passenger who is transported for free, without having to pay any form of compensation. This person can, however, choose to contribute to travel expenses on their own initiative. In terms of insurance coverage, this passenger is usually covered under the vehicle owner's insurance policy. This means that in the event of an accident, the insurance company would cover the costs of any injuries or damages incurred by this passenger.\n\nOn the other hand, a regular passenger, who might be paying for the transportation service, may not be covered under the vehicle owner's insurance policy. This is because the insurance policy might only cover the vehicle owner and not the passengers. In this case, the passenger would need to have their own personal insurance coverage to cover any potential injuries or damages.\n\nHowever, the specifics of insurance coverage can vary greatly depending on the terms of the insurance policy and local regulations. Therefore, it's always important to check the details of an insurance policy to understand who is covered.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:28.893552",
          "duration": 8.936206,
          "cost": 0.01785
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. A \"passager à titre gratuit\" is a passenger who travels without paying any fare."
          },
          {
            "meta": {},
            "text": "2. This passenger can voluntarily contribute to travel expenses."
          },
          {
            "meta": {},
            "text": "3. The vehicle owner's insurance policy usually covers a \"passager à titre gratuit\"."
          },
          {
            "meta": {},
            "text": "4. In case of an accident, the insurance company would cover the costs for this passenger."
          },
          {
            "meta": {},
            "text": "5. A regular passenger, who pays for the service, may not be covered by the vehicle owner's insurance."
          },
          {
            "meta": {},
            "text": "6. Regular passengers might need their own personal insurance for potential injuries or damages."
          },
          {
            "meta": {},
            "text": "7. Insurance coverage details can vary based on the policy terms and local regulations."
          },
          {
            "meta": {},
            "text": "8. It is crucial to review an insurance policy to understand who is covered."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 16.193609497280722,
              "Node id": "ffdb9021-918d-4627-8d02-412479040f4f"
            },
            "text": "PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur."
          },
          {
            "meta": {
              "score": 15.59851496651043,
              "Node id": "6ef17292-3447-4946-84a8-a8730c25578d"
            },
            "text": "P                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat."
          },
          {
            "meta": {
              "score": 15.517128399323871,
              "Node id": "6befb900-ccd1-49b6-bb94-d72177186fa7"
            },
            "text": "Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu."
          },
          {
            "meta": {
              "score": 15.436604496180538,
              "Node id": "9c20f52e-9543-4acb-bb39-bb3d84a6f68d"
            },
            "text": "PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur. C’est son nom qui apparaît \nsur le contrat et il est généralement désigné par le terme « vous »."
          },
          {
            "meta": {
              "score": 15.213200086097359,
              "Node id": "43ef21f7-218c-4089-ae7d-0810bb0d0d82"
            },
            "text": "Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières ."
          },
          {
            "meta": {
              "score": 14.738902258963405,
              "Node id": "ec3021b9-b66f-4550-aa2a-a4499e4767ed"
            },
            "text": "•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier.\n•  Les caravanes ou les remorques attelées au véhicule assuré  d’un poids total \nautorisé en charge maximum de 750 kg."
          },
          {
            "meta": {
              "score": 14.46825846875594,
              "Node id": "68a7d238-26c6-4140-8cfa-28d720e6ec01"
            },
            "text": "Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier."
          },
          {
            "meta": {
              "score": 13.626073136048536,
              "Node id": "135c03dc-ddc4-4d6a-baef-b91b00023633"
            },
            "text": "50  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) TITRE III \n        GARANTIES DE PROTECTION JURIDIQUE  \nLes seuils de déclenchement et les plafonds applicables aux garanties de Protection Juridique figurent à l’article 3  des présentes \nconditions  générales . \n \nARTICLE  28    Protection Juridique suite à accident  \n \nLa gestion des sinistres de Protection Juridique suite à accident  est effectuée dans le cadre de la première des modalités de  gestion \nprévues par l’article L. 322 -2-3 du Code des assurances : elle est confiée à  un personnel distinct au sein de l’entreprise."
          },
          {
            "meta": {
              "score": 13.277577488080123,
              "Node id": "c10b9cb0-b146-46fa-ab16-22727877d0bf"
            },
            "text": "L                                                      \nLOCATAIRE\nPersonne qui obtient le droit d’utiliser le bien loué contre le versement d’une \nsomme d’argent appelée loyer.\nM                                                       \nMALADIE\nAltération soudaine et imprévisible de la santé, constatée par une autorité médicale \ncompétente. Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution."
          },
          {
            "meta": {
              "score": 13.229595024512463,
              "Node id": "48fb2026-0a56-4c1b-8a00-33396acc35b8"
            },
            "text": "Nous\nLa société d’assurances désignée aux Conditions \npersonnelles.\nNullité\nVoir \" Sanctions  \".\nOption\nVoir \" Accessoires  \".\nPassager\nPersonne transportée à l’intérieur du véhicule.\n • Passager à titre gratuit : il ne verse pas de rému -\nnération, même s’il participe aux frais de route.\nIl peut s’agir de co-voiturage, c’est-à-dire l’utili -\nsation conjointe et organisée d’un véhicule par un \nconducteur non professionnel et un ou plusieurs \ntiers passagers, dans le but d’effectuer un trajet \ncommun. Le co-voiturage ne doit pas être rému -\nnéré, hormis le partage des frais liés au trajet."
          },
          {
            "meta": {
              "score": 0.748504516560445,
              "Node id": "ea60f0c6-b524-4065-949a-f71285cfb01c"
            },
            "text": "Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots."
          },
          {
            "meta": {
              "score": 0.7479765889353686,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7460846837016396,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7439817676916031,
              "Node id": "a050b38f-4729-4768-8310-d7b9380bca56"
            },
            "text": "Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”."
          },
          {
            "meta": {
              "score": 0.7380490020158721,
              "Node id": "56a0beb5-78d0-4259-9ff6-501fa5d8c3cc"
            },
            "text": "40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here?"
          },
          {
            "meta": {
              "score": 0.7375765956804416,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          },
          {
            "meta": {
              "score": 0.734940088323484,
              "Node id": "b9c34789-6d83-4615-b032-030eb4446371"
            },
            "text": "Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp."
          },
          {
            "meta": {
              "score": 0.7345575976045345,
              "Node id": "a8680bb9-6b9c-4c88-a0c8-81403f9476c6"
            },
            "text": "I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit."
          },
          {
            "meta": {
              "score": 0.7342283366139932,
              "Node id": "d708cbf1-2f1d-4df5-9eeb-9f80c77f2811"
            },
            "text": "32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015).\n[212]F. Rosenblatt ,The perceptron: A probabilistic model for infor-\nmation storage and organization in the brain , Psychological Re-\nview, (1958), pp. 65–386.\n[213]D. Rothman ,Transformers for Natural Language Processing ,\nPackt, 2021.\n[214]A.Roy,M.Saffar,A.Vaswani,andD.Grangier ,Eﬃcient\ncontent-based sparse attention with routing transformers , Transac-\ntions of the Association for Computational Linguistics, 9 (2021),\npp."
          },
          {
            "meta": {
              "score": 0.7332929177106169,
              "Node id": "ec83a3f1-b11c-49be-8b91-69da619d59ca"
            },
            "text": "[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "A \"passager à titre gratuit\" refers to a passenger who is transported for free, without having to pay any form of compensation. This person can, however, choose to contribute to travel expenses on their own initiative. In terms of insurance coverage, this passenger is usually covered under the vehicle owner's insurance policy. This means that in the event of an accident, the insurance company would cover the costs of any injuries or damages incurred by this passenger.\n\nOn the other hand, a regular passenger, who might be paying for the transportation service, may not be covered under the vehicle owner's insurance policy. This is because the insurance policy might only cover the vehicle owner and not the passengers. In this case, the passenger would need to have their own personal insurance coverage to cover any potential injuries or damages.\n\nHowever, the specifics of insurance coverage can vary greatly depending on the terms of the insurance policy and local regulations. Therefore, it's always important to check the details of an insurance policy to understand who is covered.",
              "prompt": {
                "meta": {},
                "user": "Discuss the distinction between a \"passager à titre gratuit\" and a regular passenger in terms of insurance coverage.",
                "system": "Contexte :  PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur. \n\n P                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat. \n\n Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu. \n\n PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur. C’est son nom qui apparaît \nsur le contrat et il est généralement désigné par le terme « vous ». \n\n Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières . \n\n •  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier.\n•  Les caravanes ou les remorques attelées au véhicule assuré  d’un poids total \nautorisé en charge maximum de 750 kg. \n\n Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier. \n\n 50  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) TITRE III \n        GARANTIES DE PROTECTION JURIDIQUE  \nLes seuils de déclenchement et les plafonds applicables aux garanties de Protection Juridique figurent à l’article 3  des présentes \nconditions  générales . \n \nARTICLE  28    Protection Juridique suite à accident  \n \nLa gestion des sinistres de Protection Juridique suite à accident  est effectuée dans le cadre de la première des modalités de  gestion \nprévues par l’article L. 322 -2-3 du Code des assurances : elle est confiée à  un personnel distinct au sein de l’entreprise. \n\n L                                                      \nLOCATAIRE\nPersonne qui obtient le droit d’utiliser le bien loué contre le versement d’une \nsomme d’argent appelée loyer.\nM                                                       \nMALADIE\nAltération soudaine et imprévisible de la santé, constatée par une autorité médicale \ncompétente. Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. \n\n Nous\nLa société d’assurances désignée aux Conditions \npersonnelles.\nNullité\nVoir \" Sanctions  \".\nOption\nVoir \" Accessoires  \".\nPassager\nPersonne transportée à l’intérieur du véhicule.\n • Passager à titre gratuit : il ne verse pas de rému -\nnération, même s’il participe aux frais de route.\nIl peut s’agir de co-voiturage, c’est-à-dire l’utili -\nsation conjointe et organisée d’un véhicule par un \nconducteur non professionnel et un ou plusieurs \ntiers passagers, dans le but d’effectuer un trajet \ncommun. Le co-voiturage ne doit pas être rému -\nnéré, hormis le partage des frais liés au trajet. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015).\n[212]F. Rosenblatt ,The perceptron: A probabilistic model for infor-\nmation storage and organization in the brain , Psychological Re-\nview, (1958), pp. 65–386.\n[213]D. Rothman ,Transformers for Natural Language Processing ,\nPackt, 2021.\n[214]A.Roy,M.Saffar,A.Vaswani,andD.Grangier ,Eﬃcient\ncontent-based sparse attention with routing transformers , Transac-\ntions of the Association for Computational Linguistics, 9 (2021),\npp. \n\n [147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604. \n\n \n La question est Discuss the distinction between a \"passager à titre gratuit\" and a regular passenger in terms of insurance coverage."
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:05:39.187922",
              "duration": 14.727314,
              "cost": 0.12936
            },
            "meta": {},
            "text": "A \"passager à titre gratuit\" refers to a passenger who is transported for free, without having to pay any form of compensation. This person can, however, choose to contribute to travel expenses on their own initiative. In terms of insurance coverage, this passenger is usually covered under the vehicle owner's insurance policy. This means that in the event of an accident, the insurance company would cover the costs of any injuries or damages incurred by this passenger.\n\nOn the other hand, a regular passenger, who might be paying for the transportation service, may not be covered under the vehicle owner's insurance policy. This is because the insurance policy might only cover the vehicle owner and not the passengers. In this case, the passenger would need to have their own personal insurance coverage to cover any potential injuries or damages.\n\nHowever, the specifics of insurance coverage can vary greatly depending on the terms of the insurance policy and local regulations. Therefore, it's always important to check the details of an insurance policy to understand who is covered.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6, 7, 8",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: A \"passager à titre gratuit\" refers to a passenger who is transported for free, without having to pay any form of compensation. This person can, however, choose to contribute to travel expenses on their own initiative. In terms of insurance coverage, this passenger is usually covered under the vehicle owner's insurance policy. This means that in the event of an accident, the insurance company would cover the costs of any injuries or damages incurred by this passenger.\n\nOn the other hand, a regular passenger, who might be paying for the transportation service, may not be covered under the vehicle owner's insurance policy. This is because the insurance policy might only cover the vehicle owner and not the passengers. In this case, the passenger would need to have their own personal insurance coverage to cover any potential injuries or damages.\n\nHowever, the specifics of insurance coverage can vary greatly depending on the terms of the insurance policy and local regulations. Therefore, it's always important to check the details of an insurance policy to understand who is covered.\n\n1. 1. A \"passager à titre gratuit\" is a passenger who travels without paying any fare.\n2. 2. This passenger can voluntarily contribute to travel expenses.\n3. 3. The vehicle owner's insurance policy usually covers a \"passager à titre gratuit\".\n4. 4. In case of an accident, the insurance company would cover the costs for this passenger.\n5. 5. A regular passenger, who pays for the service, may not be covered by the vehicle owner's insurance.\n6. 6. Regular passengers might need their own personal insurance for potential injuries or damages.\n7. 7. Insurance coverage details can vary based on the policy terms and local regulations.\n8. 8. It is crucial to review an insurance policy to understand who is covered.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:34.176335",
                "duration": 3.0594,
                "cost": 0.01533
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6', '7', '8'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the context of insurance coverage, there is a distinction between a \"passager à titre gratuit\" (a passenger transported free of charge) and a regular passenger. Here are some key points to consider regarding their insurance coverage:\n\n1. **Passager à Titre Gratuit (Passenger Transported Free of Charge):**\n   - A passager à titre gratuit is someone who is being transported in a vehicle without having to pay any fare or fee for the transportation.\n   - This type of passenger is typically not considered a commercial passenger and is not paying for the transportation service.\n   - In terms of insurance coverage, a passager à titre gratuit may still be covered under the vehicle owner's insurance policy, especially in cases of accidents, mechanical breakdowns, theft, vandalism, or other covered events.\n   - The insurance coverage for a passager à titre gratuit may vary depending on the specific terms and conditions of the insurance policy.\n\n2. **Regular Passenger:**\n   - A regular passenger is someone who is being transported in a vehicle and may be paying a fare or fee for the transportation service.\n   - Regular passengers are often covered under the vehicle owner's insurance policy, especially in cases of accidents or other covered events.\n   - The insurance coverage for regular passengers may also depend on the specific terms and conditions of the insurance policy, including any limitations or exclusions related to passengers.\n\n3. **Insurance Coverage Considerations:**\n   - Insurance policies typically outline who is covered under the policy, including passengers, and the extent of coverage provided.\n   - It's important for vehicle owners to review their insurance policies to understand who is covered as a passenger and under what circumstances.\n   - In the case of passagers à titre gratuit, it's advisable for both the vehicle owner and the passenger to clarify any insurance coverage questions or concerns before the transportation takes place.\n\nIn summary, while both passagers à titre gratuit and regular passengers may be covered under a vehicle owner's insurance policy, there may be differences in coverage or considerations based on the specific circumstances of the transportation and the terms of the insurance policy. It's essential for all parties involved to be aware of the insurance coverage in place to ensure protection in case of any unforeseen events.",
              "prompt": {
                "meta": {},
                "user": "Discuss the distinction between a \"passager à titre gratuit\" and a regular passenger in terms of insurance coverage.",
                "system": "Contexte :  PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur. \n\n P                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat. \n\n Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu. \n\n PERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. Cette personne peut cependant, si elle le souhaite, participer aux frais \nde route de sa propre initiative.\nPERTE TOTALE\nUn véhicule est considéré en perte totale lorsque le montant des réparations à \neffectuer est supérieur à la valeur de remplacement à dire d’expert. Elle est \ncontrôlée par expert.PRESCRIPTION\nLaps de temps au terme duquel un droit non exercé est perdu.\nS                                                     \nSINISTRE\nÉvènement susceptible d’entraîner l’application des garanties du contrat.\nSOUSCRIPTEUR\nPersonne qui a conclu et signé le contrat avec l’assureur. C’est son nom qui apparaît \nsur le contrat et il est généralement désigné par le terme « vous ». \n\n Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières . \n\n •  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier.\n•  Les caravanes ou les remorques attelées au véhicule assuré  d’un poids total \nautorisé en charge maximum de 750 kg. \n\n Qui bénéficie des garanties d’Assistance ?\n• Le souscripteur  du contrat d’assurance AUTO .\n• Son conjoint  de droit ou de fait.\n•  Leurs enfants, petits-enfants, parents et grands-parents fiscalement à charge \net vivant sous le même toit.\n•  Le conducteur  ou les passagers transportés à titre gratuit dans le véhicule \nassuré  en cas de panne mécanique, électronique ou électrique , d’accident \nde la circulation, de vol , d’incendie , de vandalisme  ou de bris de glaces. \nLe bénéficiaire est généralement désigné par le terme « Vous » dans les \nconditions ci-après. \nPour quel véhicule ?\n• Le véhicule assuré  désigné aux conditions particulières .\n•  Le véhicule loué ou emprunté pour remplacer provisoirement le véhicule \nassuré  en cas d’indisponibilité fortuite de ce dernier. \n\n 50  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) TITRE III \n        GARANTIES DE PROTECTION JURIDIQUE  \nLes seuils de déclenchement et les plafonds applicables aux garanties de Protection Juridique figurent à l’article 3  des présentes \nconditions  générales . \n \nARTICLE  28    Protection Juridique suite à accident  \n \nLa gestion des sinistres de Protection Juridique suite à accident  est effectuée dans le cadre de la première des modalités de  gestion \nprévues par l’article L. 322 -2-3 du Code des assurances : elle est confiée à  un personnel distinct au sein de l’entreprise. \n\n L                                                      \nLOCATAIRE\nPersonne qui obtient le droit d’utiliser le bien loué contre le versement d’une \nsomme d’argent appelée loyer.\nM                                                       \nMALADIE\nAltération soudaine et imprévisible de la santé, constatée par une autorité médicale \ncompétente. Elle peut être consécutive ou non à une situation préexistante. Elle n’a \npas pour origine un accident corporel.\nP                                                            \nPANNE MÉCANIQUE, ÉLECTRONIQUE OU ÉLECTRIQUE\nIncident mécanique, électronique ou électrique d’origine fortuite du véhicule assuré, \nnon imputable à une erreur de son utilisateur.\nPERSONNE TRANSPORTÉE À TITRE GRATUIT\nTout passager transporté gratuitement sans avoir à verser une quelconque \nrétribution. \n\n Nous\nLa société d’assurances désignée aux Conditions \npersonnelles.\nNullité\nVoir \" Sanctions  \".\nOption\nVoir \" Accessoires  \".\nPassager\nPersonne transportée à l’intérieur du véhicule.\n • Passager à titre gratuit : il ne verse pas de rému -\nnération, même s’il participe aux frais de route.\nIl peut s’agir de co-voiturage, c’est-à-dire l’utili -\nsation conjointe et organisée d’un véhicule par un \nconducteur non professionnel et un ou plusieurs \ntiers passagers, dans le but d’effectuer un trajet \ncommun. Le co-voiturage ne doit pas être rému -\nnéré, hormis le partage des frais liés au trajet. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015).\n[212]F. Rosenblatt ,The perceptron: A probabilistic model for infor-\nmation storage and organization in the brain , Psychological Re-\nview, (1958), pp. 65–386.\n[213]D. Rothman ,Transformers for Natural Language Processing ,\nPackt, 2021.\n[214]A.Roy,M.Saffar,A.Vaswani,andD.Grangier ,Eﬃcient\ncontent-based sparse attention with routing transformers , Transac-\ntions of the Association for Computational Linguistics, 9 (2021),\npp. \n\n [147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604. \n\n \n La question est Discuss the distinction between a \"passager à titre gratuit\" and a regular passenger in terms of insurance coverage."
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:05:49.894859",
              "duration": 10.701521,
              "cost": 0.0026245
            },
            "meta": {},
            "text": "In the context of insurance coverage, there is a distinction between a \"passager à titre gratuit\" (a passenger transported free of charge) and a regular passenger. Here are some key points to consider regarding their insurance coverage:\n\n1. **Passager à Titre Gratuit (Passenger Transported Free of Charge):**\n   - A passager à titre gratuit is someone who is being transported in a vehicle without having to pay any fare or fee for the transportation.\n   - This type of passenger is typically not considered a commercial passenger and is not paying for the transportation service.\n   - In terms of insurance coverage, a passager à titre gratuit may still be covered under the vehicle owner's insurance policy, especially in cases of accidents, mechanical breakdowns, theft, vandalism, or other covered events.\n   - The insurance coverage for a passager à titre gratuit may vary depending on the specific terms and conditions of the insurance policy.\n\n2. **Regular Passenger:**\n   - A regular passenger is someone who is being transported in a vehicle and may be paying a fare or fee for the transportation service.\n   - Regular passengers are often covered under the vehicle owner's insurance policy, especially in cases of accidents or other covered events.\n   - The insurance coverage for regular passengers may also depend on the specific terms and conditions of the insurance policy, including any limitations or exclusions related to passengers.\n\n3. **Insurance Coverage Considerations:**\n   - Insurance policies typically outline who is covered under the policy, including passengers, and the extent of coverage provided.\n   - It's important for vehicle owners to review their insurance policies to understand who is covered as a passenger and under what circumstances.\n   - In the case of passagers à titre gratuit, it's advisable for both the vehicle owner and the passenger to clarify any insurance coverage questions or concerns before the transportation takes place.\n\nIn summary, while both passagers à titre gratuit and regular passengers may be covered under a vehicle owner's insurance policy, there may be differences in coverage or considerations based on the specific circumstances of the transportation and the terms of the insurance policy. It's essential for all parties involved to be aware of the insurance coverage in place to ensure protection in case of any unforeseen events.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 3, 4, 5, 7, 8",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the context of insurance coverage, there is a distinction between a \"passager à titre gratuit\" (a passenger transported free of charge) and a regular passenger. Here are some key points to consider regarding their insurance coverage:\n\n1. **Passager à Titre Gratuit (Passenger Transported Free of Charge):**\n   - A passager à titre gratuit is someone who is being transported in a vehicle without having to pay any fare or fee for the transportation.\n   - This type of passenger is typically not considered a commercial passenger and is not paying for the transportation service.\n   - In terms of insurance coverage, a passager à titre gratuit may still be covered under the vehicle owner's insurance policy, especially in cases of accidents, mechanical breakdowns, theft, vandalism, or other covered events.\n   - The insurance coverage for a passager à titre gratuit may vary depending on the specific terms and conditions of the insurance policy.\n\n2. **Regular Passenger:**\n   - A regular passenger is someone who is being transported in a vehicle and may be paying a fare or fee for the transportation service.\n   - Regular passengers are often covered under the vehicle owner's insurance policy, especially in cases of accidents or other covered events.\n   - The insurance coverage for regular passengers may also depend on the specific terms and conditions of the insurance policy, including any limitations or exclusions related to passengers.\n\n3. **Insurance Coverage Considerations:**\n   - Insurance policies typically outline who is covered under the policy, including passengers, and the extent of coverage provided.\n   - It's important for vehicle owners to review their insurance policies to understand who is covered as a passenger and under what circumstances.\n   - In the case of passagers à titre gratuit, it's advisable for both the vehicle owner and the passenger to clarify any insurance coverage questions or concerns before the transportation takes place.\n\nIn summary, while both passagers à titre gratuit and regular passengers may be covered under a vehicle owner's insurance policy, there may be differences in coverage or considerations based on the specific circumstances of the transportation and the terms of the insurance policy. It's essential for all parties involved to be aware of the insurance coverage in place to ensure protection in case of any unforeseen events.\n\n1. 1. A \"passager à titre gratuit\" is a passenger who travels without paying any fare.\n2. 2. This passenger can voluntarily contribute to travel expenses.\n3. 3. The vehicle owner's insurance policy usually covers a \"passager à titre gratuit\".\n4. 4. In case of an accident, the insurance company would cover the costs for this passenger.\n5. 5. A regular passenger, who pays for the service, may not be covered by the vehicle owner's insurance.\n6. 6. Regular passengers might need their own personal insurance for potential injuries or damages.\n7. 7. Insurance coverage details can vary based on the policy terms and local regulations.\n8. 8. It is crucial to review an insurance policy to understand who is covered.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:35.901900",
                "duration": 1.7228720000000002,
                "cost": 0.02232
              },
              "meta": {},
              "text": "Validated: ['1', '3', '4', '5', '7', '8'] - Not validated: ['2', '6']",
              "human": 1.0,
              "auto": 0.75
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The insured can approach the Insurance Mediation if the dispute continues after internal resolution attempts.\n2. The insured must be a non-professional individual who has subscribed to the contract for non-professional needs to seize the Insurance Mediation.\n3. The Insurance Mediation can be contacted at TSA 50110 75441 Paris Cedex 09.\n4. The Insurance Mediation can also be reached online at http://www.mediation-assurance.org.",
          "prompt": {
            "meta": {},
            "user": "If the dispute persists after attempting to resolve it internally, the insured, if they are a non-professional individual who has subscribed to the contract for non-professional needs, can then seize the Insurance Mediation. The contact details are as follows:\n- Address: TSA 50110 75441 Paris Cedex 09\n- Internet: http://www.mediation-assurance.org",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:35.776733",
          "duration": 6.871486,
          "cost": 0.01125
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The insured can approach the Insurance Mediation if the dispute continues after internal resolution attempts."
          },
          {
            "meta": {},
            "text": "2. The insured must be a non-professional individual who has subscribed to the contract for non-professional needs to seize the Insurance Mediation."
          },
          {
            "meta": {},
            "text": "3. The Insurance Mediation can be contacted at TSA 50110 75441 Paris Cedex 09."
          },
          {
            "meta": {},
            "text": "4. The Insurance Mediation can also be reached online at http://www.mediation-assurance.org."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.16097445323362,
              "Node id": "aef9a952-c24c-41ea-92e7-3213bb808491"
            },
            "text": "The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn."
          },
          {
            "meta": {
              "score": 12.019659843233192,
              "Node id": "7ce792fb-168f-461b-a65f-0fd2f4a0c385"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied."
          },
          {
            "meta": {
              "score": 11.237694520873244,
              "Node id": "cb51a559-c37c-46be-a076-6f19a31ae981"
            },
            "text": "6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t."
          },
          {
            "meta": {
              "score": 10.461656665220957,
              "Node id": "b7c9a1c2-2f00-4f73-bb3c-4383548eb4c2"
            },
            "text": "Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62"
          },
          {
            "meta": {
              "score": 10.461656665220957,
              "Node id": "f8fc903c-a901-4c7e-99bf-fcd9fbb0a3e0"
            },
            "text": "Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62"
          },
          {
            "meta": {
              "score": 9.764416066970359,
              "Node id": "e715a76a-4130-45fd-af03-7c95c4868b0d"
            },
            "text": "The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training."
          },
          {
            "meta": {
              "score": 9.708203786766155,
              "Node id": "6a6134e7-dbc6-46ce-b4b2-dc9f438b9704"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm."
          },
          {
            "meta": {
              "score": 9.707493904189937,
              "Node id": "829217f7-d068-4f20-a0e9-6a4dece3e891"
            },
            "text": "La politique de l’Assureur en matière de satisfaction client est disponible sur s on site à l’adresse suivante : http://www.aig.com.  \n \nConformément à la recommandation 2011-R05 de l’ACPR, une réponse sera apportée à l’Assuré dans un délai maximum de 2 mois (sauf circonstances particulières dont l’Assuré sera tenu informé).   Si le désaccord persiste après la réponse donnée par l’Assureur, l’Assuré peut, sans préjudice de \nses droits à intenter une action en justice, saisir le Médiateur : La Médiation de l’Assurance - TSA \n50110 - 75441 Paris Cedex 09. http://www.mediation-assurance.org\n/. Ce recours est gratuit."
          },
          {
            "meta": {
              "score": 9.5292395879943,
              "Node id": "ca40e626-adb5-4907-ae9b-d42486cb5012"
            },
            "text": "Préjudice d’affection  \nSouffrances morales subies par le/les proches, ayant la qualité de bénéficiaire(s) au titre du contrat, suite au décès de l’a ssuré.  \n \nPréjudice écologique  \nAtteinte non négligeable aux éléments ou aux fonctions des écosystèmes ou aux bénéfices c ollectifs tirés par l’homme de \nl’environnement.  \n \nPréjudice esthétique permanent  \nAtteintes altérant l’apparence physique de l’assuré persistant après consolidation.  \n \nPréposé  \nPersonne qui accomplit un acte ou une formation déterminée sous la direction ou le contrôle d’une autre.  \n \nPrescription  \nDélai à l’issue duquel le titulaire d’un droit ne dispose plus d’action pour le faire valoir."
          },
          {
            "meta": {
              "score": 9.384441858606406,
              "Node id": "b5b7dece-cea2-490f-b134-19a50ec0a6c9"
            },
            "text": "Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training. Note that\nforimageinputs,theembeddinglayerisaconvolutionalencoder,ascom-\nmonly used in computer vision. A diagram of the Decision Transformer\narchitecture is shown in Fig. 6.6.\nSince the input tokens are a sequence of rewards-to-go, states, and\nactions, the multi-head self-attention mechanism computes attention be-\ntween rewards, states, and actions."
          },
          {
            "meta": {
              "score": 0.7251924371260682,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7230792850873414,
              "Node id": "73fee44f-f78a-490b-a010-506c0aa781cb"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400."
          },
          {
            "meta": {
              "score": 0.7219047293347595,
              "Node id": "c194b57c-f2ab-4d62-adce-5d3482fec02d"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          },
          {
            "meta": {
              "score": 0.721377576723345,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7209156413427799,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7203201066807424,
              "Node id": "cd221c3b-36c2-4216-a212-bea04249287e"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe."
          },
          {
            "meta": {
              "score": 0.7187905119760667,
              "Node id": "417fe1e0-ac85-4c9c-84be-650b6d0505ab"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 0.718508823914254,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          },
          {
            "meta": {
              "score": 0.7172882139476099,
              "Node id": "55ff27c9-0d8d-492c-99f3-014cd0d0b3e1"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies."
          },
          {
            "meta": {
              "score": 0.7148800475464939,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "If the dispute persists after attempting to resolve it internally, the insured, if they are a non-professional individual who has subscribed to the contract for non-professional needs, can then seize the Insurance Mediation. The contact details are as follows:\n- Address: TSA 50110 75441 Paris Cedex 09\n- Internet: http://www.mediation-assurance.org",
              "prompt": {
                "meta": {},
                "user": "What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?",
                "system": "Contexte :  The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n 6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t. \n\n Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62 \n\n Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62 \n\n The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training. \n\n Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. \n\n La politique de l’Assureur en matière de satisfaction client est disponible sur s on site à l’adresse suivante : http://www.aig.com.  \n \nConformément à la recommandation 2011-R05 de l’ACPR, une réponse sera apportée à l’Assuré dans un délai maximum de 2 mois (sauf circonstances particulières dont l’Assuré sera tenu informé).   Si le désaccord persiste après la réponse donnée par l’Assureur, l’Assuré peut, sans préjudice de \nses droits à intenter une action en justice, saisir le Médiateur : La Médiation de l’Assurance - TSA \n50110 - 75441 Paris Cedex 09. http://www.mediation-assurance.org\n/. Ce recours est gratuit. \n\n Préjudice d’affection  \nSouffrances morales subies par le/les proches, ayant la qualité de bénéficiaire(s) au titre du contrat, suite au décès de l’a ssuré.  \n \nPréjudice écologique  \nAtteinte non négligeable aux éléments ou aux fonctions des écosystèmes ou aux bénéfices c ollectifs tirés par l’homme de \nl’environnement.  \n \nPréjudice esthétique permanent  \nAtteintes altérant l’apparence physique de l’assuré persistant après consolidation.  \n \nPréposé  \nPersonne qui accomplit un acte ou une formation déterminée sous la direction ou le contrôle d’une autre.  \n \nPrescription  \nDélai à l’issue duquel le titulaire d’un droit ne dispose plus d’action pour le faire valoir. \n\n Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training. Note that\nforimageinputs,theembeddinglayerisaconvolutionalencoder,ascom-\nmonly used in computer vision. A diagram of the Decision Transformer\narchitecture is shown in Fig. 6.6.\nSince the input tokens are a sequence of rewards-to-go, states, and\nactions, the multi-head self-attention mechanism computes attention be-\ntween rewards, states, and actions. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n \n La question est What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:05:55.416757",
              "duration": 5.517807,
              "cost": 0.13467
            },
            "meta": {},
            "text": "If the dispute persists after attempting to resolve it internally, the insured, if they are a non-professional individual who has subscribed to the contract for non-professional needs, can then seize the Insurance Mediation. The contact details are as follows:\n- Address: TSA 50110 75441 Paris Cedex 09\n- Internet: http://www.mediation-assurance.org",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: If the dispute persists after attempting to resolve it internally, the insured, if they are a non-professional individual who has subscribed to the contract for non-professional needs, can then seize the Insurance Mediation. The contact details are as follows:\n- Address: TSA 50110 75441 Paris Cedex 09\n- Internet: http://www.mediation-assurance.org\n\n1. 1. The insured can approach the Insurance Mediation if the dispute continues after internal resolution attempts.\n2. 2. The insured must be a non-professional individual who has subscribed to the contract for non-professional needs to seize the Insurance Mediation.\n3. 3. The Insurance Mediation can be contacted at TSA 50110 75441 Paris Cedex 09.\n4. 4. The Insurance Mediation can also be reached online at http://www.mediation-assurance.org.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:37.124002",
                "duration": 1.218722,
                "cost": 0.00909
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "If a dispute persists between the insured and the insurer after attempting to resolve it internally, the next course of action would be to seek external resolution through the following steps:\n\n1. **Commission de recours interne**: The insured can address the Commission de recours interne. The insurer will provide the insured with the contact details upon request.\n\n2. **Médiation de l’Assurance**: If the disagreement continues and the insured is an individual who has taken out the contract for non-professional purposes, they can contact the Médiation de l’Assurance. The insured can find the details as follows:\n   - Address: TSA 50110 75441 Paris Cedex 09\n   - Website: http://www.mediation-assurance.org\n\n3. **Legal Action**: If the dispute remains unresolved even after the above steps, the insured can consider taking legal action to address the issue.\n\nIt's important to follow the specific procedures outlined by the insurer and regulatory bodies to ensure a fair resolution of the dispute.",
              "prompt": {
                "meta": {},
                "user": "What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?",
                "system": "Contexte :  The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n 6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t. \n\n Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62 \n\n Si cette démarche ne permet pas d’y mettre un terme, vous avez la possibilité de vous adresser à la\nCommission de recours interne dont nous vous communiquerons les coordonnées sur simple demande.\nSi ce désaccord devait persister, et si vous êtes un particulier ayant souscrit ce contrat pour des\nbesoins non professionnels, vous pouvez alors saisir. La Médiation de l’Assurance :\n- Adresse : TSA 50110 75441 Paris Cedex 09 ;\n- Internet : http://www.mediation-assurance.orgMédiationL1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page62 \n\n The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training. \n\n Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. \n\n La politique de l’Assureur en matière de satisfaction client est disponible sur s on site à l’adresse suivante : http://www.aig.com.  \n \nConformément à la recommandation 2011-R05 de l’ACPR, une réponse sera apportée à l’Assuré dans un délai maximum de 2 mois (sauf circonstances particulières dont l’Assuré sera tenu informé).   Si le désaccord persiste après la réponse donnée par l’Assureur, l’Assuré peut, sans préjudice de \nses droits à intenter une action en justice, saisir le Médiateur : La Médiation de l’Assurance - TSA \n50110 - 75441 Paris Cedex 09. http://www.mediation-assurance.org\n/. Ce recours est gratuit. \n\n Préjudice d’affection  \nSouffrances morales subies par le/les proches, ayant la qualité de bénéficiaire(s) au titre du contrat, suite au décès de l’a ssuré.  \n \nPréjudice écologique  \nAtteinte non négligeable aux éléments ou aux fonctions des écosystèmes ou aux bénéfices c ollectifs tirés par l’homme de \nl’environnement.  \n \nPréjudice esthétique permanent  \nAtteintes altérant l’apparence physique de l’assuré persistant après consolidation.  \n \nPréposé  \nPersonne qui accomplit un acte ou une formation déterminée sous la direction ou le contrôle d’une autre.  \n \nPrescription  \nDélai à l’issue duquel le titulaire d’un droit ne dispose plus d’action pour le faire valoir. \n\n Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. There are also learned positional embeddings added to each type\nof token embeddings that are diﬀerent that in the standard transformer\nbecause each time step has three tokens instead of one. The embeddings\nare then passed to a GPT-2 model, for autoregressive training. Note that\nforimageinputs,theembeddinglayerisaconvolutionalencoder,ascom-\nmonly used in computer vision. A diagram of the Decision Transformer\narchitecture is shown in Fig. 6.6.\nSince the input tokens are a sequence of rewards-to-go, states, and\nactions, the multi-head self-attention mechanism computes attention be-\ntween rewards, states, and actions. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n \n La question est What is the next course of action if a dispute persists between the insured and the insurer after attempting to resolve it internally?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:06:00.782644",
              "duration": 5.362778,
              "cost": 0.0024774999999999997
            },
            "meta": {},
            "text": "If a dispute persists between the insured and the insurer after attempting to resolve it internally, the next course of action would be to seek external resolution through the following steps:\n\n1. **Commission de recours interne**: The insured can address the Commission de recours interne. The insurer will provide the insured with the contact details upon request.\n\n2. **Médiation de l’Assurance**: If the disagreement continues and the insured is an individual who has taken out the contract for non-professional purposes, they can contact the Médiation de l’Assurance. The insured can find the details as follows:\n   - Address: TSA 50110 75441 Paris Cedex 09\n   - Website: http://www.mediation-assurance.org\n\n3. **Legal Action**: If the dispute remains unresolved even after the above steps, the insured can consider taking legal action to address the issue.\n\nIt's important to follow the specific procedures outlined by the insurer and regulatory bodies to ensure a fair resolution of the dispute.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: If a dispute persists between the insured and the insurer after attempting to resolve it internally, the next course of action would be to seek external resolution through the following steps:\n\n1. **Commission de recours interne**: The insured can address the Commission de recours interne. The insurer will provide the insured with the contact details upon request.\n\n2. **Médiation de l’Assurance**: If the disagreement continues and the insured is an individual who has taken out the contract for non-professional purposes, they can contact the Médiation de l’Assurance. The insured can find the details as follows:\n   - Address: TSA 50110 75441 Paris Cedex 09\n   - Website: http://www.mediation-assurance.org\n\n3. **Legal Action**: If the dispute remains unresolved even after the above steps, the insured can consider taking legal action to address the issue.\n\nIt's important to follow the specific procedures outlined by the insurer and regulatory bodies to ensure a fair resolution of the dispute.\n\n1. 1. The insured can approach the Insurance Mediation if the dispute continues after internal resolution attempts.\n2. 2. The insured must be a non-professional individual who has subscribed to the contract for non-professional needs to seize the Insurance Mediation.\n3. 3. The Insurance Mediation can be contacted at TSA 50110 75441 Paris Cedex 09.\n4. 4. The Insurance Mediation can also be reached online at http://www.mediation-assurance.org.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:38.561298",
                "duration": 1.434597,
                "cost": 0.01296
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Missing an insurance premium payment can lead to policy cancellation.\n2. Once an insurance policy is cancelled, the individual loses their coverage.\n3. The person will be financially liable for any damages or losses after the policy cancellation.\n4. Non-payment can result in higher insurance premiums in the future.\n5. A history of non-payment can make it harder to obtain insurance later on.\n6. Some insurers may refuse to provide coverage to individuals with a history of non-payment.\n7. Driving without insurance can lead to legal consequences such as fines or license suspension.\n8. Missed insurance payments can negatively affect one's credit score.\n9. It's crucial to contact the insurance company immediately if a payment cannot be made.\n10. Insurance companies may offer payment plans or other solutions to maintain coverage.",
          "prompt": {
            "meta": {},
            "user": "Failing to pay one installment of your insurance premium can have several implications:\n\n1. Cancellation of Policy: If you miss a payment and do not make up for it within the grace period, your insurance company may cancel your policy. This means you will no longer be covered under the policy.\n\n2. Loss of Coverage: If your policy is cancelled, you will lose your coverage. This means you will be financially responsible for any losses or damages that occur after the cancellation.\n\n3. Higher Premiums in the Future: If your policy is cancelled due to non-payment, it could lead to higher premiums in the future. Insurance companies often consider your payment history when determining your premium rates.\n\n4. Difficulty Getting New Insurance: If your policy is cancelled due to non-payment, it could make it more difficult for you to get insurance in the future. Some insurance companies may not be willing to insure you if you have a history of non-payment.\n\n5. Legal Consequences: Depending on the laws in your area, driving without insurance could result in fines, license suspension, or even jail time.\n\n6. Credit Score Impact: Some insurance companies report missed payments to credit bureaus, which could negatively impact your credit score. \n\nIt's important to contact your insurance company as soon as possible if you're unable to make a payment. They may be able to work out a payment plan or offer other solutions to help you keep your coverage.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:45.371138",
          "duration": 9.59129,
          "cost": 0.02157
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Missing an insurance premium payment can lead to policy cancellation."
          },
          {
            "meta": {},
            "text": "2. Once an insurance policy is cancelled, the individual loses their coverage."
          },
          {
            "meta": {},
            "text": "3. The person will be financially liable for any damages or losses after the policy cancellation."
          },
          {
            "meta": {},
            "text": "4. Non-payment can result in higher insurance premiums in the future."
          },
          {
            "meta": {},
            "text": "5. A history of non-payment can make it harder to obtain insurance later on."
          },
          {
            "meta": {},
            "text": "6. Some insurers may refuse to provide coverage to individuals with a history of non-payment."
          },
          {
            "meta": {},
            "text": "7. Driving without insurance can lead to legal consequences such as fines or license suspension."
          },
          {
            "meta": {},
            "text": "8. Missed insurance payments can negatively affect one's credit score."
          },
          {
            "meta": {},
            "text": "9. It's crucial to contact the insurance company immediately if a payment cannot be made."
          },
          {
            "meta": {},
            "text": "10. Insurance companies may offer payment plans or other solutions to maintain coverage."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.235125658994795,
              "Node id": "6daf5cfe-d29b-47db-8722-5af5ed8c659e"
            },
            "text": "Transformers: Basics and Introduction ■17\nFigure 2.4 Attention mechanism showing query, keys, values, and output\nvector interactions.\n2.3.2.1 Dot product (multiplicative)\nThe dot product-based scoring function is the simplest one and has no\nparameters to tune [176].\nα(q,k) =q·k (2.12)\n2.3.2.2 Scaled dot product or multiplicative\nThe scaled dot product-based scoring function divides the dot product\nby√dkto remove the inﬂuence of dimension dk[254]. According to\nVaswani et al., as the dimension increases, the dot products grow larger,\nwhich pushes the softmax function into regions with extreme gradients.\nα(q,k) =q·k√dk(2.13)\n2.3.2.3 Linear, MLP , or Additive\nLuong et al. also experimented by projecting the query and keys to a\nhidden layer of dimension h, learning the weights (W k,Wq), and using"
          },
          {
            "meta": {
              "score": 11.329324065364197,
              "Node id": "b0218249-d564-490e-8c7b-056856a6d382"
            },
            "text": "Note that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel. This\nmeans that Q,K,VareL×dmodelmatrices and the attention weight\nmatrix from (5.23) is L×L."
          },
          {
            "meta": {
              "score": 10.955112758582455,
              "Node id": "cb8ef037-4aae-4c91-8557-1d6e42107e0f"
            },
            "text": "Transformer Modiﬁcations ■123\nFigure 5.6 The query matrix, Q, can be partitioned into hheads, as de-\nscribed in (5.28).\nNote that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel."
          },
          {
            "meta": {
              "score": 10.955112758582455,
              "Node id": "0d268076-f355-454b-8b0c-8a130829e26c"
            },
            "text": "This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2."
          },
          {
            "meta": {
              "score": 10.673084364005476,
              "Node id": "4550b407-08f8-44da-aff8-e7a187887395"
            },
            "text": "Transformer Modiﬁcations ■137\nEﬃcient sparse matrix multiplication By making the adjacency\nmatrix (and the corresponding attention matrix) sparse, the speedups\nprovided by GPUs are removed. This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to."
          },
          {
            "meta": {
              "score": 10.397280292936124,
              "Node id": "88e133fd-605a-4f51-a8a7-594b3982e644"
            },
            "text": "Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots."
          },
          {
            "meta": {
              "score": 10.35171028819242,
              "Node id": "0a65c011-14c0-4abe-92c6-c869e64eb1de"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes."
          },
          {
            "meta": {
              "score": 10.08174083885206,
              "Node id": "28d3302e-7be6-4e14-afda-21d6c151b2aa"
            },
            "text": "The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. Each was evaluated\non several language modeling datasets without any additional training.\nGPT-2 achieved state-of-the-art on seven out of eight datasets."
          },
          {
            "meta": {
              "score": 9.951968683787062,
              "Node id": "36c7dff2-03af-4c45-8359-070dded4e176"
            },
            "text": "Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary."
          },
          {
            "meta": {
              "score": 9.888327414926128,
              "Node id": "9a91b498-65d8-474a-b31e-d561bdcb75d2"
            },
            "text": "The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers."
          },
          {
            "meta": {
              "score": 0.7111953004235174,
              "Node id": "0e0fbb72-d00b-4ffa-8e2e-0167c0498623"
            },
            "text": "It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al."
          },
          {
            "meta": {
              "score": 0.7101227645728155,
              "Node id": "2436ee17-46ed-4ffd-8494-0c49400e40e3"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to."
          },
          {
            "meta": {
              "score": 0.7100138092543339,
              "Node id": "c1f0536b-7f70-4b41-a217-33f4231f7699"
            },
            "text": "Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages."
          },
          {
            "meta": {
              "score": 0.7090804729798413,
              "Node id": "ea60f0c6-b524-4065-949a-f71285cfb01c"
            },
            "text": "Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots."
          },
          {
            "meta": {
              "score": 0.7083030119244069,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7075175181442932,
              "Node id": "8e666c52-3b9f-4ce1-8c30-bb0ca634f7b6"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to."
          },
          {
            "meta": {
              "score": 0.7065273027714744,
              "Node id": "c6a836a4-3481-42cc-81a9-7b3e84e3156d"
            },
            "text": "First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space."
          },
          {
            "meta": {
              "score": 0.7062652292302782,
              "Node id": "a050b38f-4729-4768-8310-d7b9380bca56"
            },
            "text": "Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”."
          },
          {
            "meta": {
              "score": 0.7061455886361714,
              "Node id": "601a1b02-c68b-45a9-911d-1ad170bfaf58"
            },
            "text": "They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training."
          },
          {
            "meta": {
              "score": 0.7058309657237797,
              "Node id": "f0d6fd76-660d-4ee9-837f-c56911567865"
            },
            "text": "In [260] and [45] it is contended that since mLMs have limited model\ncapacity that gets used by multiple languages, they cannot capture all\nthe subtleties of several languages as a pre-trained monolingual model\ncan. They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "Failing to pay one installment of your insurance premium can have several implications:\n\n1. Cancellation of Policy: If you miss a payment and do not make up for it within the grace period, your insurance company may cancel your policy. This means you will no longer be covered under the policy.\n\n2. Loss of Coverage: If your policy is cancelled, you will lose your coverage. This means you will be financially responsible for any losses or damages that occur after the cancellation.\n\n3. Higher Premiums in the Future: If your policy is cancelled due to non-payment, it could lead to higher premiums in the future. Insurance companies often consider your payment history when determining your premium rates.\n\n4. Difficulty Getting New Insurance: If your policy is cancelled due to non-payment, it could make it more difficult for you to get insurance in the future. Some insurance companies may not be willing to insure you if you have a history of non-payment.\n\n5. Legal Consequences: Depending on the laws in your area, driving without insurance could result in fines, license suspension, or even jail time.\n\n6. Credit Score Impact: Some insurance companies report missed payments to credit bureaus, which could negatively impact your credit score. \n\nIt's important to contact your insurance company as soon as possible if you're unable to make a payment. They may be able to work out a payment plan or offer other solutions to help you keep your coverage.",
              "prompt": {
                "meta": {},
                "user": "What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?",
                "system": "Contexte :  Transformers: Basics and Introduction ■17\nFigure 2.4 Attention mechanism showing query, keys, values, and output\nvector interactions.\n2.3.2.1 Dot product (multiplicative)\nThe dot product-based scoring function is the simplest one and has no\nparameters to tune [176].\nα(q,k) =q·k (2.12)\n2.3.2.2 Scaled dot product or multiplicative\nThe scaled dot product-based scoring function divides the dot product\nby√dkto remove the inﬂuence of dimension dk[254]. According to\nVaswani et al., as the dimension increases, the dot products grow larger,\nwhich pushes the softmax function into regions with extreme gradients.\nα(q,k) =q·k√dk(2.13)\n2.3.2.3 Linear, MLP , or Additive\nLuong et al. also experimented by projecting the query and keys to a\nhidden layer of dimension h, learning the weights (W k,Wq), and using \n\n Note that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel. This\nmeans that Q,K,VareL×dmodelmatrices and the attention weight\nmatrix from (5.23) is L×L. \n\n Transformer Modiﬁcations ■123\nFigure 5.6 The query matrix, Q, can be partitioned into hheads, as de-\nscribed in (5.28).\nNote that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel. \n\n This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2. \n\n Transformer Modiﬁcations ■137\nEﬃcient sparse matrix multiplication By making the adjacency\nmatrix (and the corresponding attention matrix) sparse, the speedups\nprovided by GPUs are removed. This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. \n\n The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. Each was evaluated\non several language modeling datasets without any additional training.\nGPT-2 achieved state-of-the-art on seven out of eight datasets. \n\n Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. \n\n The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space. \n\n Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”. \n\n They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. \n\n In [260] and [45] it is contended that since mLMs have limited model\ncapacity that gets used by multiple languages, they cannot capture all\nthe subtleties of several languages as a pre-trained monolingual model\ncan. They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. \n\n \n La question est What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:06:21.217450",
              "duration": 20.429277,
              "cost": 0.13827
            },
            "meta": {},
            "text": "Failing to pay one installment of your insurance premium can have several implications:\n\n1. Cancellation of Policy: If you miss a payment and do not make up for it within the grace period, your insurance company may cancel your policy. This means you will no longer be covered under the policy.\n\n2. Loss of Coverage: If your policy is cancelled, you will lose your coverage. This means you will be financially responsible for any losses or damages that occur after the cancellation.\n\n3. Higher Premiums in the Future: If your policy is cancelled due to non-payment, it could lead to higher premiums in the future. Insurance companies often consider your payment history when determining your premium rates.\n\n4. Difficulty Getting New Insurance: If your policy is cancelled due to non-payment, it could make it more difficult for you to get insurance in the future. Some insurance companies may not be willing to insure you if you have a history of non-payment.\n\n5. Legal Consequences: Depending on the laws in your area, driving without insurance could result in fines, license suspension, or even jail time.\n\n6. Credit Score Impact: Some insurance companies report missed payments to credit bureaus, which could negatively impact your credit score. \n\nIt's important to contact your insurance company as soon as possible if you're unable to make a payment. They may be able to work out a payment plan or offer other solutions to help you keep your coverage.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Failing to pay one installment of your insurance premium can have several implications:\n\n1. Cancellation of Policy: If you miss a payment and do not make up for it within the grace period, your insurance company may cancel your policy. This means you will no longer be covered under the policy.\n\n2. Loss of Coverage: If your policy is cancelled, you will lose your coverage. This means you will be financially responsible for any losses or damages that occur after the cancellation.\n\n3. Higher Premiums in the Future: If your policy is cancelled due to non-payment, it could lead to higher premiums in the future. Insurance companies often consider your payment history when determining your premium rates.\n\n4. Difficulty Getting New Insurance: If your policy is cancelled due to non-payment, it could make it more difficult for you to get insurance in the future. Some insurance companies may not be willing to insure you if you have a history of non-payment.\n\n5. Legal Consequences: Depending on the laws in your area, driving without insurance could result in fines, license suspension, or even jail time.\n\n6. Credit Score Impact: Some insurance companies report missed payments to credit bureaus, which could negatively impact your credit score. \n\nIt's important to contact your insurance company as soon as possible if you're unable to make a payment. They may be able to work out a payment plan or offer other solutions to help you keep your coverage.\n\n1. 1. Missing an insurance premium payment can lead to policy cancellation.\n2. 2. Once an insurance policy is cancelled, the individual loses their coverage.\n3. 3. The person will be financially liable for any damages or losses after the policy cancellation.\n4. 4. Non-payment can result in higher insurance premiums in the future.\n5. 5. A history of non-payment can make it harder to obtain insurance later on.\n6. 6. Some insurers may refuse to provide coverage to individuals with a history of non-payment.\n7. 7. Driving without insurance can lead to legal consequences such as fines or license suspension.\n8. 8. Missed insurance payments can negatively affect one's credit score.\n9. 9. It's crucial to contact the insurance company immediately if a payment cannot be made.\n10. 10. Insurance companies may offer payment plans or other solutions to maintain coverage.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:42.182895",
                "duration": 3.618162,
                "cost": 0.01899
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Failing to pay one installment of your insurance premium when it is divided into multiple payments can have several implications:\n\n1. **Policy Lapse**: Missing a payment could lead to your insurance policy lapsing. This means that your coverage will no longer be active, and you will not be protected in case of any incidents.\n\n2. **Late Fees**: Insurance companies often charge late fees for missed payments. These fees can add up over time, increasing the overall cost of your insurance.\n\n3. **Reinstatement**: If your policy lapses due to missed payments, you may have the option to reinstate it by paying the overdue amount along with any applicable fees. However, this process can be cumbersome and may involve additional paperwork.\n\n4. **Impact on Coverage**: Even if you reinstate your policy, missing payments can still have an impact on your coverage. Some insurance companies may restrict coverage or increase premiums for policyholders with a history of missed payments.\n\n5. **Credit Score**: Failing to pay insurance premiums on time could also have a negative impact on your credit score. Insurance companies may report late payments to credit bureaus, affecting your creditworthiness.\n\nIt is essential to communicate with your insurance provider if you are facing difficulties making payments. They may offer options such as payment extensions or restructuring your payment plan to help you maintain your coverage without negative consequences.",
              "prompt": {
                "meta": {},
                "user": "What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?",
                "system": "Contexte :  Transformers: Basics and Introduction ■17\nFigure 2.4 Attention mechanism showing query, keys, values, and output\nvector interactions.\n2.3.2.1 Dot product (multiplicative)\nThe dot product-based scoring function is the simplest one and has no\nparameters to tune [176].\nα(q,k) =q·k (2.12)\n2.3.2.2 Scaled dot product or multiplicative\nThe scaled dot product-based scoring function divides the dot product\nby√dkto remove the inﬂuence of dimension dk[254]. According to\nVaswani et al., as the dimension increases, the dot products grow larger,\nwhich pushes the softmax function into regions with extreme gradients.\nα(q,k) =q·k√dk(2.13)\n2.3.2.3 Linear, MLP , or Additive\nLuong et al. also experimented by projecting the query and keys to a\nhidden layer of dimension h, learning the weights (W k,Wq), and using \n\n Note that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel. This\nmeans that Q,K,VareL×dmodelmatrices and the attention weight\nmatrix from (5.23) is L×L. \n\n Transformer Modiﬁcations ■123\nFigure 5.6 The query matrix, Q, can be partitioned into hheads, as de-\nscribed in (5.28).\nNote that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel. \n\n This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2. \n\n Transformer Modiﬁcations ■137\nEﬃcient sparse matrix multiplication By making the adjacency\nmatrix (and the corresponding attention matrix) sparse, the speedups\nprovided by GPUs are removed. This is because multiplication of matri-\nces with arbitrary sparsity is not eﬃcient on GPUs. However, matrices\nwhose sparsity is grouped into blocks can be eﬃciently multiplied on\nGPUs [100].\nTo address this, Big Bird groups blocks of queries and keys together\nand then adds sparsity by block rather than by individual sequence po-\nsition [292]. Select a block size, b, and then divide the sequence length\nLintoL/bblocks. So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n Pre-trained and Application-Speciﬁc Transformers ■161\nand after the position-wise feedforward network, as part of the resid-\nual connection. In GPT-2, the layer norm module instead comes before\nthe multi-head attention and before the position-wise feedforward. The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. \n\n The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. Each was evaluated\non several language modeling datasets without any additional training.\nGPT-2 achieved state-of-the-art on seven out of eight datasets. \n\n Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. \n\n The\nresidual connection now only includes addition, rather than addition and\nlayer norm. One additional layer norm module is placed in the ﬁnal de-\ncoder block, after the multi-head attention. The weights of the residual\nlayers are initialized diﬀerently than in the GPT model. The weights for\na residual layer are divided by 1/√\nN, whereNis the number of residual\nlayers in the entire model.\nGPT-2 uses byte-pair encoding (BPE) tokenization [92] so that any\nUTF-8 string can be represented using a vocabulary that of only 256\nbytes. Computing with the raw UTF-8 bytes was not done here, since\nbyte-level language models were not performing at the level of word-level\nlanguage models.1\nFour variants with the described architecture were trained. The\nlargest model of the four is the one called \"GPT-2\". It has 1.542 billion\nparameters and uses 48 transformer decoder layers. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space. \n\n Transformers: Basics and Introduction ■39\nFigure 2.19 Loss and perplexity on the test set.\nFig. 2.20 shows RNN vs. Transformer for the translation and it can\nbe seen that the RNN pairs the formal/plural “serez” with the infor-\nmal/singular “tu” while transformer matches “seras tu”.\nFig. 2.21 shows another RNN vs. Transformer for the translation\nand it can be seen that the RNN doesn’t capture the “snowing” portion\nof the sentence and produces a diﬀerent phrasing of “my house”. \n\n They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. \n\n In [260] and [45] it is contended that since mLMs have limited model\ncapacity that gets used by multiple languages, they cannot capture all\nthe subtleties of several languages as a pre-trained monolingual model\ncan. They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. \n\n \n La question est What are the implications of failing to pay one installment of your insurance premium if it is divided into multiple payments?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:06:32.123920",
              "duration": 10.900706,
              "cost": 0.002428
            },
            "meta": {},
            "text": "Failing to pay one installment of your insurance premium when it is divided into multiple payments can have several implications:\n\n1. **Policy Lapse**: Missing a payment could lead to your insurance policy lapsing. This means that your coverage will no longer be active, and you will not be protected in case of any incidents.\n\n2. **Late Fees**: Insurance companies often charge late fees for missed payments. These fees can add up over time, increasing the overall cost of your insurance.\n\n3. **Reinstatement**: If your policy lapses due to missed payments, you may have the option to reinstate it by paying the overdue amount along with any applicable fees. However, this process can be cumbersome and may involve additional paperwork.\n\n4. **Impact on Coverage**: Even if you reinstate your policy, missing payments can still have an impact on your coverage. Some insurance companies may restrict coverage or increase premiums for policyholders with a history of missed payments.\n\n5. **Credit Score**: Failing to pay insurance premiums on time could also have a negative impact on your credit score. Insurance companies may report late payments to credit bureaus, affecting your creditworthiness.\n\nIt is essential to communicate with your insurance provider if you are facing difficulties making payments. They may offer options such as payment extensions or restructuring your payment plan to help you maintain your coverage without negative consequences.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 4, 8, 9, 10",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Failing to pay one installment of your insurance premium when it is divided into multiple payments can have several implications:\n\n1. **Policy Lapse**: Missing a payment could lead to your insurance policy lapsing. This means that your coverage will no longer be active, and you will not be protected in case of any incidents.\n\n2. **Late Fees**: Insurance companies often charge late fees for missed payments. These fees can add up over time, increasing the overall cost of your insurance.\n\n3. **Reinstatement**: If your policy lapses due to missed payments, you may have the option to reinstate it by paying the overdue amount along with any applicable fees. However, this process can be cumbersome and may involve additional paperwork.\n\n4. **Impact on Coverage**: Even if you reinstate your policy, missing payments can still have an impact on your coverage. Some insurance companies may restrict coverage or increase premiums for policyholders with a history of missed payments.\n\n5. **Credit Score**: Failing to pay insurance premiums on time could also have a negative impact on your credit score. Insurance companies may report late payments to credit bureaus, affecting your creditworthiness.\n\nIt is essential to communicate with your insurance provider if you are facing difficulties making payments. They may offer options such as payment extensions or restructuring your payment plan to help you maintain your coverage without negative consequences.\n\n1. 1. Missing an insurance premium payment can lead to policy cancellation.\n2. 2. Once an insurance policy is cancelled, the individual loses their coverage.\n3. 3. The person will be financially liable for any damages or losses after the policy cancellation.\n4. 4. Non-payment can result in higher insurance premiums in the future.\n5. 5. A history of non-payment can make it harder to obtain insurance later on.\n6. 6. Some insurers may refuse to provide coverage to individuals with a history of non-payment.\n7. 7. Driving without insurance can lead to legal consequences such as fines or license suspension.\n8. 8. Missed insurance payments can negatively affect one's credit score.\n9. 9. It's crucial to contact the insurance company immediately if a payment cannot be made.\n10. 10. Insurance companies may offer payment plans or other solutions to maintain coverage.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:43.906451",
                "duration": 1.714937,
                "cost": 0.01788
              },
              "meta": {},
              "text": "Validated: ['1', '2', '4', '8', '9', '10'] - Not validated: ['3', '5', '6', '7']",
              "human": 1.0,
              "auto": 0.6
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What type of guarantee does the insurance provide for legal defense in criminal cases?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not specify the type of guarantee provided by the insurance for legal defense.\n2. There is no information about the insurance coverage for legal defense in criminal cases.\n3. The document lacks details on the insurance's guarantee for legal defense in criminal situations.\n4. The insurance's provision for legal defense in criminal cases is not clarified in the text.\n5. The text does not elaborate on the guarantee offered by the insurance for legal defense in criminal cases.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on what type of guarantee the insurance provides for legal defense in criminal cases.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:51.391969",
          "duration": 6.017722,
          "cost": 0.00972
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not specify the type of guarantee provided by the insurance for legal defense."
          },
          {
            "meta": {},
            "text": "2. There is no information about the insurance coverage for legal defense in criminal cases."
          },
          {
            "meta": {},
            "text": "3. The document lacks details on the insurance's guarantee for legal defense in criminal situations."
          },
          {
            "meta": {},
            "text": "4. The insurance's provision for legal defense in criminal cases is not clarified in the text."
          },
          {
            "meta": {},
            "text": "5. The text does not elaborate on the guarantee offered by the insurance for legal defense in criminal cases."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.392012111781408,
              "Node id": "8a83916f-aaaa-418b-a13a-66ee35c49b0d"
            },
            "text": "Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size."
          },
          {
            "meta": {
              "score": 11.380619035623342,
              "Node id": "a7425db2-dfd8-484c-9596-c2b7c47402a5"
            },
            "text": "Section 4.3 provides\nsomeinsightsintotheinnerworkingsofmultilingualtransformermodels.\nFinally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer."
          },
          {
            "meta": {
              "score": 10.956887150018643,
              "Node id": "36c7dff2-03af-4c45-8359-070dded4e176"
            },
            "text": "Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary."
          },
          {
            "meta": {
              "score": 10.785073661288475,
              "Node id": "895f2b1f-b165-4cba-b75a-9e30394cc844"
            },
            "text": "10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free)."
          },
          {
            "meta": {
              "score": 10.729028179677377,
              "Node id": "0f01b3db-3659-400d-8573-21a78b9519de"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier."
          },
          {
            "meta": {
              "score": 10.591791935592013,
              "Node id": "d60c06d9-b06b-48b1-9540-f2540c28c5b0"
            },
            "text": "For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture."
          },
          {
            "meta": {
              "score": 10.400385645794813,
              "Node id": "5eb29d1a-3923-4fe4-ab62-f34065e5df2f"
            },
            "text": "GPT-3 had over 1000 times as many parameters as\nGPT-3 small. All models were trained on 300 billion tokens and used a\ncontext window of 2048 tokens. Note that the position-wise feedforward\nnetwork had dff= 4dmodel. Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets."
          },
          {
            "meta": {
              "score": 10.352870241621059,
              "Node id": "75510e13-377c-4906-95f2-912106270ca9"
            },
            "text": "10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free). Case\nstudies are also available in the Github repository which accompanies\nthis book: https://github.com/CRCTransformers/deepdive-book."
          },
          {
            "meta": {
              "score": 10.284216251961542,
              "Node id": "0347bb99-c052-456e-b060-42143a7ed656"
            },
            "text": "Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets.\nResults showed that as the number of model parameters increases,\nthe model needs fewer demonstrations to learn how to perform a task\n(and hence to reach a given accuracy target). When few-shot is used,\nmodel performance increases more quickly with model size, suggesting\nlarger models are better at in-context learning."
          },
          {
            "meta": {
              "score": 9.866534192035479,
              "Node id": "7e45352b-af1d-4ef1-b0c9-d5012932e7ae"
            },
            "text": "Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset."
          },
          {
            "meta": {
              "score": 0.7226976079700336,
              "Node id": "78b973d1-bc09-46b8-b151-630ed535c4a7"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 0.7221842689542713,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7193870657266391,
              "Node id": "9360e0b9-165e-4945-b9b3-b317f2eee854"
            },
            "text": "•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. Adversarial defense is about making the model robust"
          },
          {
            "meta": {
              "score": 0.7180643890292827,
              "Node id": "c194b57c-f2ab-4d62-adce-5d3482fec02d"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          },
          {
            "meta": {
              "score": 0.7178014463341333,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7156949643845549,
              "Node id": "ebc44546-5749-4af4-9b1c-03b987744c1e"
            },
            "text": "Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]."
          },
          {
            "meta": {
              "score": 0.7152164383988139,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7145491935244835,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7143973059241705,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7114303200811181,
              "Node id": "c519dce1-9d5f-44a7-b859-9657bbd78629"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on what type of guarantee the insurance provides for legal defense in criminal cases.",
              "prompt": {
                "meta": {},
                "user": "What type of guarantee does the insurance provide for legal defense in criminal cases?",
                "system": "Contexte :  Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size. \n\n Section 4.3 provides\nsomeinsightsintotheinnerworkingsofmultilingualtransformermodels.\nFinally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. \n\n Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. \n\n 10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free). \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier. \n\n For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. \n\n GPT-3 had over 1000 times as many parameters as\nGPT-3 small. All models were trained on 300 billion tokens and used a\ncontext window of 2048 tokens. Note that the position-wise feedforward\nnetwork had dff= 4dmodel. Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets. \n\n 10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free). Case\nstudies are also available in the Github repository which accompanies\nthis book: https://github.com/CRCTransformers/deepdive-book. \n\n Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets.\nResults showed that as the number of model parameters increases,\nthe model needs fewer demonstrations to learn how to perform a task\n(and hence to reach a given accuracy target). When few-shot is used,\nmodel performance increases more quickly with model size, suggesting\nlarger models are better at in-context learning. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n •AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. Adversarial defense is about making the model robust \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600. \n\n \n La question est What type of guarantee does the insurance provide for legal defense in criminal cases?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:06:34.255446",
              "duration": 2.127283,
              "cost": 0.12522
            },
            "meta": {},
            "text": "The text does not provide information on what type of guarantee the insurance provides for legal defense in criminal cases.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on what type of guarantee the insurance provides for legal defense in criminal cases.\n\n1. 1. The text does not specify the type of guarantee provided by the insurance for legal defense.\n2. 2. There is no information about the insurance coverage for legal defense in criminal cases.\n3. 3. The document lacks details on the insurance's guarantee for legal defense in criminal situations.\n4. 4. The insurance's provision for legal defense in criminal cases is not clarified in the text.\n5. 5. The text does not elaborate on the guarantee offered by the insurance for legal defense in criminal cases.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:45.393521",
                "duration": 1.483884,
                "cost": 0.00774
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The insurance typically provides coverage for legal defense in criminal cases, which means that the policyholder will have financial assistance to hire a lawyer and cover legal fees associated with defending against criminal charges. This coverage can vary depending on the specific terms and conditions of the insurance policy, so it is important to review the policy documents to understand the extent of coverage provided for legal defense in criminal cases.",
              "prompt": {
                "meta": {},
                "user": "What type of guarantee does the insurance provide for legal defense in criminal cases?",
                "system": "Contexte :  Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size. \n\n Section 4.3 provides\nsomeinsightsintotheinnerworkingsofmultilingualtransformermodels.\nFinally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. \n\n Finally,Section 4.4 provides a practical case study on zero-shot multi-\nlingual sentiment classiﬁcation.\n4.1 MULTILINGUAL TRANSFORMER ARCHITECTURES\nThis section discusses the diﬀerent architectural designs for multilingual\nTransformers. We divide multilingual transformers into (i) Natural Lan-\nguage Understanding (NLU) architectures and (ii) Natural Language\nGeneration (NLG) architectures. Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. \n\n 10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free). \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier. \n\n For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. \n\n GPT-3 had over 1000 times as many parameters as\nGPT-3 small. All models were trained on 300 billion tokens and used a\ncontext window of 2048 tokens. Note that the position-wise feedforward\nnetwork had dff= 4dmodel. Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets. \n\n 10■Transformers for Machine Learning: A Deep Dive\n•DeepLearningforCoderswithfastaiandPyTorchbySylvainGug-\nger and Jeremy Howard https://course.fast.ai/\n•Jay Alammar’s visual explanation of transformers and related ar-\nchitectures https://jalammar.github.io/\n1.3.4 Case Studies and Details\nAt the end of Chapters 2–7, we include a case study that allows the\nreader to see how one or more of the models and methods discussed in\nthe chapter can be applied, or how they stack up against one another\nwhen applied to the same problem. The aim is for the case study to\nprovide a small starting point in working with transformer models from\nwhich one can branch out further. Each case study has been chosen to\nrun within approximately one hour on GPUs at least as powerful as\nthe NVIDIA K80 (Google Colaboratory provides these for free). Case\nstudies are also available in the Github repository which accompanies\nthis book: https://github.com/CRCTransformers/deepdive-book. \n\n Larger models using larger batch sizes used\nsmaller learning rates.\nPerformance Several sizes of GPT-3 models were evaluated for zero-,\none-, and few-shot learning, which Brown et al. describe as diﬀerent\ntypes of in-context learning. With the few-shot type of learning, GPT-3\ngivesthemodelasmanyexamplesofthedesiredtaskthatwillﬁtintothe\ncontext window (10–100 examples). One-shot learning then provides one\nexample and a task description, while zero-shot provides no examples,\nonly a task description. There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets.\nResults showed that as the number of model parameters increases,\nthe model needs fewer demonstrations to learn how to perform a task\n(and hence to reach a given accuracy target). When few-shot is used,\nmodel performance increases more quickly with model size, suggesting\nlarger models are better at in-context learning. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n •AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. Adversarial defense is about making the model robust \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600. \n\n \n La question est What type of guarantee does the insurance provide for legal defense in criminal cases?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:06:37.015848",
              "duration": 2.757073,
              "cost": 0.0021799999999999996
            },
            "meta": {},
            "text": "The insurance typically provides coverage for legal defense in criminal cases, which means that the policyholder will have financial assistance to hire a lawyer and cover legal fees associated with defending against criminal charges. This coverage can vary depending on the specific terms and conditions of the insurance policy, so it is important to review the policy documents to understand the extent of coverage provided for legal defense in criminal cases.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The insurance typically provides coverage for legal defense in criminal cases, which means that the policyholder will have financial assistance to hire a lawyer and cover legal fees associated with defending against criminal charges. This coverage can vary depending on the specific terms and conditions of the insurance policy, so it is important to review the policy documents to understand the extent of coverage provided for legal defense in criminal cases.\n\n1. 1. The text does not specify the type of guarantee provided by the insurance for legal defense.\n2. 2. There is no information about the insurance coverage for legal defense in criminal cases.\n3. 3. The document lacks details on the insurance's guarantee for legal defense in criminal situations.\n4. 4. The insurance's provision for legal defense in criminal cases is not clarified in the text.\n5. 5. The text does not elaborate on the guarantee offered by the insurance for legal defense in criminal cases.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:46.174268",
                "duration": 0.778057,
                "cost": 0.008669999999999999
              },
              "meta": {},
              "text": "Validated: ['2'] - Not validated: ['1', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The exclusion does not apply when oils are being transported.\n2. Damages caused by transporting hazardous materials are usually excluded.\n3. The vehicle's transportation of oils is an exception to the exclusion.\n4. The exclusion for damages is not applicable when the vehicle carries oils.\n5. The rule of excluding damages from transporting hazardous materials has an exception for oils.",
          "prompt": {
            "meta": {},
            "user": "The exclusion for damages caused by transporting hazardous materials does not apply when the vehicle is transporting oils.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:22:58.522129",
          "duration": 7.128248,
          "cost": 0.008400000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The exclusion does not apply when oils are being transported."
          },
          {
            "meta": {},
            "text": "2. Damages caused by transporting hazardous materials are usually excluded."
          },
          {
            "meta": {},
            "text": "3. The vehicle's transportation of oils is an exception to the exclusion."
          },
          {
            "meta": {},
            "text": "4. The exclusion for damages is not applicable when the vehicle carries oils."
          },
          {
            "meta": {},
            "text": "5. The rule of excluding damages from transporting hazardous materials has an exception for oils."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.811104604730202,
              "Node id": "0c1dbf3c-f46f-46a8-9774-def66cf09e36"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 8.966806835876644,
              "Node id": "58c52e91-de4b-4fc1-8e03-aebf4a521732"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 8.8162430353312,
              "Node id": "f9727f5f-d488-46a1-99de-19206790b166"
            },
            "text": "As spatial resolution increases and the videos get longer, divided\nspace-timeattentionscalesbetterthanthejointspace-timeattentionbe-\ncause the spatial and temporal components are separated in the divided\ncase. Note that, for a single patch, the spatial and temporal attention\nare both linear. Spatial is O(N)and temporal is O(F), so the divided\nscales asO(N+F). The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets?"
          },
          {
            "meta": {
              "score": 7.522048380124359,
              "Node id": "b7f2d95a-0a28-4696-a5db-5d9457a24340"
            },
            "text": "TaBERT’s representations may then be ﬁne-tuned further\nusing the training data for that job.\nTaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities."
          },
          {
            "meta": {
              "score": 7.452522904278682,
              "Node id": "e0414862-564b-46f5-96ba-296a7d9939ad"
            },
            "text": "Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes,"
          },
          {
            "meta": {
              "score": 7.419050909281848,
              "Node id": "f8de472d-7d55-4620-959b-98b51a5612f0"
            },
            "text": "son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ;"
          },
          {
            "meta": {
              "score": 7.385878239749204,
              "Node id": "b7d6cabb-9523-4583-8d8b-990d9d8eec5a"
            },
            "text": "Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; toutefois la non-garantie ne saurait être invoquée du chef de transports d’huiles,"
          },
          {
            "meta": {
              "score": 7.288116564330007,
              "Node id": "69573aec-90d2-4ac8-bba4-050b7dc57b08"
            },
            "text": "le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule assuré faisait l’ob- jet d’une location lors de la survenance du dom- mage.Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,"
          },
          {
            "meta": {
              "score": 7.279022868178898,
              "Node id": "754180dc-d8e3-4145-8a0b-0e43b2dce00c"
            },
            "text": "Toutefois, si \nl’aide consiste en une opération de remorquage, \nla garantie n’est acquise que si le remorquage \nest effectué en conformité avec les dispositions \nlégales ou réglementaires  ;\n • le remboursement des frais de nettoyage et remise  \nen état des garnitures intérieures du véhicule \nassuré, des vêtements du conducteur et des pas-\nsagers à la suite du transport bénévole d’urgence \nd’un blessé de la route.\nOUTRE LES EXCLUSIONS PRÉVUES  \nAU CHAPITRE 5 « CE QUE LE CONTRAT  \nNE GARANTIT JAMAIS  », NOTRE GARANTIE \nNE S’APPLIQUE PAS  :\n • lorsque vous êtes mis en cause pour des \ndommages dus à l’amiante ou à tout matériau \ncontenant de l’amiante sous quelque forme  \net en quelque quantité que ce soit,\n • aux dommages résultants, directement \nou indirectement, d’une contamination \nbiologique ou chimique provoquée par un acte \nde terrorisme de quelque nature qu’il soit."
          },
          {
            "meta": {
              "score": 7.237079173886373,
              "Node id": "112e6277-105c-4a73-bf4f-298a7ca2917a"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400."
          },
          {
            "meta": {
              "score": 0.7487410358035584,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7310884326719673,
              "Node id": "9b938fdf-d400-40fc-aa69-400bd77bb390"
            },
            "text": "[95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf."
          },
          {
            "meta": {
              "score": 0.7291178587469662,
              "Node id": "3ee2e47b-e7aa-40a0-ad29-46c4703dd219"
            },
            "text": "C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective."
          },
          {
            "meta": {
              "score": 0.7275769441145028,
              "Node id": "c4f1af7e-3e97-4c63-a5a3-4c51de9ef39c"
            },
            "text": "Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion."
          },
          {
            "meta": {
              "score": 0.7275667128577444,
              "Node id": "b9d76e2b-1c4a-4168-99e7-334f028d7a1b"
            },
            "text": "C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]."
          },
          {
            "meta": {
              "score": 0.7271488682363729,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7268040278020966,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7251909150047672,
              "Node id": "cd221c3b-36c2-4216-a212-bea04249287e"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe."
          },
          {
            "meta": {
              "score": 0.7251112832380238,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7247942664973241,
              "Node id": "2e5e2d89-a6b2-42ca-98a7-b0ab51c9df32"
            },
            "text": "), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The exclusion for damages caused by transporting hazardous materials does not apply when the vehicle is transporting oils.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?",
                "system": "Contexte :  First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n As spatial resolution increases and the videos get longer, divided\nspace-timeattentionscalesbetterthanthejointspace-timeattentionbe-\ncause the spatial and temporal components are separated in the divided\ncase. Note that, for a single patch, the spatial and temporal attention\nare both linear. Spatial is O(N)and temporal is O(F), so the divided\nscales asO(N+F). The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? \n\n TaBERT’s representations may then be ﬁne-tuned further\nusing the training data for that job.\nTaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. \n\n Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, \n\n son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; \n\n Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; toutefois la non-garantie ne saurait être invoquée du chef de transports d’huiles, \n\n le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule assuré faisait l’ob- jet d’une location lors de la survenance du dom- mage.Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*, \n\n Toutefois, si \nl’aide consiste en une opération de remorquage, \nla garantie n’est acquise que si le remorquage \nest effectué en conformité avec les dispositions \nlégales ou réglementaires  ;\n • le remboursement des frais de nettoyage et remise  \nen état des garnitures intérieures du véhicule \nassuré, des vêtements du conducteur et des pas-\nsagers à la suite du transport bénévole d’urgence \nd’un blessé de la route.\nOUTRE LES EXCLUSIONS PRÉVUES  \nAU CHAPITRE 5 « CE QUE LE CONTRAT  \nNE GARANTIT JAMAIS  », NOTRE GARANTIE \nNE S’APPLIQUE PAS  :\n • lorsque vous êtes mis en cause pour des \ndommages dus à l’amiante ou à tout matériau \ncontenant de l’amiante sous quelque forme  \net en quelque quantité que ce soit,\n • aux dommages résultants, directement \nou indirectement, d’une contamination \nbiologique ou chimique provoquée par un acte \nde terrorisme de quelque nature qu’il soit. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n [95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n \n La question est Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:06:39.258613",
              "duration": 2.238732,
              "cost": 0.12789
            },
            "meta": {},
            "text": "The exclusion for damages caused by transporting hazardous materials does not apply when the vehicle is transporting oils.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The exclusion for damages caused by transporting hazardous materials does not apply when the vehicle is transporting oils.\n\n1. 1. The exclusion does not apply when oils are being transported.\n2. 2. Damages caused by transporting hazardous materials are usually excluded.\n3. 3. The vehicle's transportation of oils is an exception to the exclusion.\n4. 4. The exclusion for damages is not applicable when the vehicle carries oils.\n5. 5. The rule of excluding damages from transporting hazardous materials has an exception for oils.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:48.077423",
                "duration": 1.899989,
                "cost": 0.007050000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The exclusion for damages caused by transporting hazardous materials does not apply under the following circumstances:\n\n- When the sums paid are solely for the reimbursement of travel expenses.\n- When the vehicle insured was rented at the time of the damage.\n- When the aid consists of a towing operation, the guarantee is only acquired if the towing is carried out in accordance with legal or regulatory provisions.\n- For the reimbursement of cleaning and restoration costs of the insured vehicle's interior trim, the driver's clothing, and the passengers' clothing following the voluntary emergency transport of a road accident victim.\n\nThese are the exceptions to the exclusion clause for damages caused by transporting hazardous materials.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?",
                "system": "Contexte :  First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n As spatial resolution increases and the videos get longer, divided\nspace-timeattentionscalesbetterthanthejointspace-timeattentionbe-\ncause the spatial and temporal components are separated in the divided\ncase. Note that, for a single patch, the spatial and temporal attention\nare both linear. Spatial is O(N)and temporal is O(F), so the divided\nscales asO(N+F). The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? \n\n TaBERT’s representations may then be ﬁne-tuned further\nusing the training data for that job.\nTaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. \n\n Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, \n\n son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; \n\n Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*,\n  le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des matières inflammables, explosives, corrosives ou comburantes, ayant provoqué ou aggravé le sinistre* ; toutefois la non-garantie ne saurait être invoquée du chef de transports d’huiles, \n\n le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule assuré faisait l’ob- jet d’une location lors de la survenance du dom- mage.Exclusions\nLES DOMMAGES SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsqu’il s’agit d’un transport de per- sonnes à titre onéreux. Cette exclusion ne s’ap- plique pas lors que les sommes versées ont pour seul objet l’indemnisation des frais de trajet.\nExclusions\nLES DOMMAGES CAUSÉS OU SUBIS PAR :\n     le véhicule, son contenu, son conducteur et ses \nayants droit lorsque le véhicule transporte des sources de rayonnements ionisants destinées à être utilisées hors d’une installation nucléaire, dès lors que lesdites sources auraient provoqué ou aggravé le sinistre*, \n\n Toutefois, si \nl’aide consiste en une opération de remorquage, \nla garantie n’est acquise que si le remorquage \nest effectué en conformité avec les dispositions \nlégales ou réglementaires  ;\n • le remboursement des frais de nettoyage et remise  \nen état des garnitures intérieures du véhicule \nassuré, des vêtements du conducteur et des pas-\nsagers à la suite du transport bénévole d’urgence \nd’un blessé de la route.\nOUTRE LES EXCLUSIONS PRÉVUES  \nAU CHAPITRE 5 « CE QUE LE CONTRAT  \nNE GARANTIT JAMAIS  », NOTRE GARANTIE \nNE S’APPLIQUE PAS  :\n • lorsque vous êtes mis en cause pour des \ndommages dus à l’amiante ou à tout matériau \ncontenant de l’amiante sous quelque forme  \net en quelque quantité que ce soit,\n • aux dommages résultants, directement \nou indirectement, d’une contamination \nbiologique ou chimique provoquée par un acte \nde terrorisme de quelque nature qu’il soit. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n [95]I. J. Goodfellow, J. Shlens, and C. Szegedy ,Ex-\nplaining and harnessing adversarial examples , arXiv preprint\narXiv:1412.6572, (2014).\n[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n \n La question est Under what circumstances does the exclusion for damages caused by transporting hazardous materials not apply?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:06:43.193146",
              "duration": 3.9316050000000002,
              "cost": 0.0023044999999999997
            },
            "meta": {},
            "text": "The exclusion for damages caused by transporting hazardous materials does not apply under the following circumstances:\n\n- When the sums paid are solely for the reimbursement of travel expenses.\n- When the vehicle insured was rented at the time of the damage.\n- When the aid consists of a towing operation, the guarantee is only acquired if the towing is carried out in accordance with legal or regulatory provisions.\n- For the reimbursement of cleaning and restoration costs of the insured vehicle's interior trim, the driver's clothing, and the passengers' clothing following the voluntary emergency transport of a road accident victim.\n\nThese are the exceptions to the exclusion clause for damages caused by transporting hazardous materials.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The exclusion for damages caused by transporting hazardous materials does not apply under the following circumstances:\n\n- When the sums paid are solely for the reimbursement of travel expenses.\n- When the vehicle insured was rented at the time of the damage.\n- When the aid consists of a towing operation, the guarantee is only acquired if the towing is carried out in accordance with legal or regulatory provisions.\n- For the reimbursement of cleaning and restoration costs of the insured vehicle's interior trim, the driver's clothing, and the passengers' clothing following the voluntary emergency transport of a road accident victim.\n\nThese are the exceptions to the exclusion clause for damages caused by transporting hazardous materials.\n\n1. 1. The exclusion does not apply when oils are being transported.\n2. 2. Damages caused by transporting hazardous materials are usually excluded.\n3. 3. The vehicle's transportation of oils is an exception to the exclusion.\n4. 4. The exclusion for damages is not applicable when the vehicle carries oils.\n5. 5. The rule of excluding damages from transporting hazardous materials has an exception for oils.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:49.042516",
                "duration": 0.962202,
                "cost": 0.0096
              },
              "meta": {},
              "text": "Validated: ['2'] - Not validated: ['1', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The refund for the unused part of the premium or contribution is calculated when the risk did not occur.\n2. The refund amount can differ based on the terms and conditions of the insurance policy or agreement.\n3. The refund is usually calculated on a pro-rata basis, which means it is proportional to the unused coverage period.\n4. If the risk did not occur for half of the coverage period, then half of the premium or contribution might be refunded.\n5. Some insurance policies may have specific conditions or fees that could influence the refund amount.\n6. It is advisable to review the policy documents or speak with the insurance provider for precise information.",
          "prompt": {
            "meta": {},
            "user": "The refund calculation for the unused portion of the premium or contribution during the period when the risk did not occur can vary based on the terms and conditions of the insurance policy or agreement. Typically, it is calculated on a pro-rata basis, meaning the refund amount is proportional to the unused period of coverage. For instance, if the risk did not occur for half of the coverage period, then half of the premium or contribution may be refunded. However, some policies may have specific conditions or fees that could affect the refund amount. It's always recommended to review the policy documents or consult with the insurance provider for accurate information.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:08.121872",
          "duration": 9.595912,
          "cost": 0.015030000000000002
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The refund for the unused part of the premium or contribution is calculated when the risk did not occur."
          },
          {
            "meta": {},
            "text": "2. The refund amount can differ based on the terms and conditions of the insurance policy or agreement."
          },
          {
            "meta": {},
            "text": "3. The refund is usually calculated on a pro-rata basis, which means it is proportional to the unused coverage period."
          },
          {
            "meta": {},
            "text": "4. If the risk did not occur for half of the coverage period, then half of the premium or contribution might be refunded."
          },
          {
            "meta": {},
            "text": "5. Some insurance policies may have specific conditions or fees that could influence the refund amount."
          },
          {
            "meta": {},
            "text": "6. It is advisable to review the policy documents or speak with the insurance provider for precise information."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.382056699021692,
              "Node id": "99a17286-3305-483a-8988-b7c803780e17"
            },
            "text": "(5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω."
          },
          {
            "meta": {
              "score": 13.330009850876845,
              "Node id": "b7f62ee6-4cff-41a8-b86d-3a50e6d8d373"
            },
            "text": "(5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling."
          },
          {
            "meta": {
              "score": 12.901050176888905,
              "Node id": "bcfa7ed8-94c3-4cf7-8088-e6f2bbe509a8"
            },
            "text": "If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling. It is also proven to be Turing\ncomplete and a universal sequence function approximator [292].\nAn Attention mechanism is a directed graph Big Bird describes\nthe attention mechanism as a directed graph. The vertices represent the\nLpositions in the input sequence. The directed edges represent the pre-\nsoftmax attention weights. In other words, the directed edges are the\ninner products between the query and key vectors, where the ithvertex"
          },
          {
            "meta": {
              "score": 11.825358591908872,
              "Node id": "5b3179ff-2477-4406-9b04-9142c534b4fc"
            },
            "text": "xviii■Foreword\nfocus on AI explainability, an increasingly important theme in practice\nimposed by greater focus on issues such as ethical AI and trustable AI.\n– Dr. Yogesh Malhotra\nFounding Chairman and CEO\nU.S. Venture Capital and Private Equity Firm\nGlobal Risk Management Network LLC\nscientist\nwww.yogeshmalhotra.com"
          },
          {
            "meta": {
              "score": 11.189891064960868,
              "Node id": "93e13967-b7c0-4a40-be3e-0119cb4938de"
            },
            "text": "Finally, the ﬁne-tuned model is then used to perform inference\non the same task from multiple diﬀerent target languages. Transformers\nhas shown remarkable progress in cross-lingual transfer learning, espe-\ncially when it comes to zero-shot learning. The details of how trans-\nformers perform cross-lingual transfer learning is still an active research\narea. This section discusses the active areas of research to understand\nthe main factors aﬀecting cross-lingual transfer learning [74]. The dis-\ncussed factors include: (i) data related factors, (ii) model architecture\nfactors, and (iii) training tasks factors.\n4.3.1.1 Data factors\nShared Vocabulary Prior to training a mLM, text from all languages\nis tokenized using a WordPiece [220] or SentencePiece [144] models. The\nmain concept is to tokenize each word into its most often occurring sub-\nwords."
          },
          {
            "meta": {
              "score": 11.189891064960868,
              "Node id": "b307ec09-395a-4c3e-a986-e5f353f104ed"
            },
            "text": "First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space."
          },
          {
            "meta": {
              "score": 10.46537698838579,
              "Node id": "c16e075a-4662-4ce5-a92f-ea6ba7f18647"
            },
            "text": "Multilingual transformers\nis certainly no exception but even more intriguing as they appear to use\nand share knowledge across multiple languages; i.e., cross-lingual trans-\nfer learning. This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]."
          },
          {
            "meta": {
              "score": 10.46537698838579,
              "Node id": "d456d013-cc33-42cc-847e-3c5cf14e316c"
            },
            "text": "This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization."
          },
          {
            "meta": {
              "score": 10.330795387376355,
              "Node id": "4f9f7503-84a6-47ff-b1c3-7dca74d818f2"
            },
            "text": "This is\ndespite the fact that it does not use parallel corpora for pre-training.\nXLM-R follows the architectural design of XLM without using the lan-\nguage embeddings which allows for better handling of code-switching.\nThe main contributions of XLM-R are achieving state-of-the-art perfor-\nmance with monolingual data and proving deep insights into the value\nof cleaner and larger training data, the size of the vocabulary, as well as\nthe right level of hyperparameters tuning.\n4.1.2.4 Alternating Language Model (ALM)\nAlternating Language Model (ALM) ALM [282] is a pre-training\ntask for cross-lingual representations that works in the same way as the\nTLMpre-trainingtaskexceptforthenatureoftheinputdata.TLMuses\nparallel sentences for input, however ALM uses code-switched sentences\ngenerated from parallel sentences. In code-switched sentences, some"
          },
          {
            "meta": {
              "score": 10.134980485703334,
              "Node id": "1086faf9-7508-4695-ba05-40b940f5eef8"
            },
            "text": "Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]."
          },
          {
            "meta": {
              "score": 0.747561536451914,
              "Node id": "0e0fbb72-d00b-4ffa-8e2e-0167c0498623"
            },
            "text": "It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al."
          },
          {
            "meta": {
              "score": 0.7404986932137408,
              "Node id": "493a412f-2c99-43ee-a4d5-e115341ece95"
            },
            "text": "The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule."
          },
          {
            "meta": {
              "score": 0.7394170896606421,
              "Node id": "87414ab0-3426-4230-906b-f033b27cba48"
            },
            "text": "The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper."
          },
          {
            "meta": {
              "score": 0.73909359439094,
              "Node id": "351b65ec-99cc-407c-8c5c-3d86474f63f3"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al."
          },
          {
            "meta": {
              "score": 0.7383659386301512,
              "Node id": "37d64be3-8b89-4fb1-b1fd-4e371d5dadf3"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi."
          },
          {
            "meta": {
              "score": 0.7368691787873407,
              "Node id": "8e666c52-3b9f-4ce1-8c30-bb0ca634f7b6"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to."
          },
          {
            "meta": {
              "score": 0.7366502567097412,
              "Node id": "410faf9a-e253-4228-acf1-f43e0baf1ab4"
            },
            "text": "The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers."
          },
          {
            "meta": {
              "score": 0.7364255112269361,
              "Node id": "d7d336e0-17b8-4036-93a3-f1217f9d34e0"
            },
            "text": "The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12)"
          },
          {
            "meta": {
              "score": 0.7360409917219103,
              "Node id": "3717f92d-e7f5-4240-a356-a4166eeaafca"
            },
            "text": "The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]."
          },
          {
            "meta": {
              "score": 0.7358915200242546,
              "Node id": "33aa42ff-d542-4052-8bd8-e72e62015741"
            },
            "text": "BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The refund calculation for the unused portion of the premium or contribution during the period when the risk did not occur can vary based on the terms and conditions of the insurance policy or agreement. Typically, it is calculated on a pro-rata basis, meaning the refund amount is proportional to the unused period of coverage. For instance, if the risk did not occur for half of the coverage period, then half of the premium or contribution may be refunded. However, some policies may have specific conditions or fees that could affect the refund amount. It's always recommended to review the policy documents or consult with the insurance provider for accurate information.",
              "prompt": {
                "meta": {},
                "user": "How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?",
                "system": "Contexte :  (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω. \n\n (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling. \n\n If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling. It is also proven to be Turing\ncomplete and a universal sequence function approximator [292].\nAn Attention mechanism is a directed graph Big Bird describes\nthe attention mechanism as a directed graph. The vertices represent the\nLpositions in the input sequence. The directed edges represent the pre-\nsoftmax attention weights. In other words, the directed edges are the\ninner products between the query and key vectors, where the ithvertex \n\n xviii■Foreword\nfocus on AI explainability, an increasingly important theme in practice\nimposed by greater focus on issues such as ethical AI and trustable AI.\n– Dr. Yogesh Malhotra\nFounding Chairman and CEO\nU.S. Venture Capital and Private Equity Firm\nGlobal Risk Management Network LLC\nscientist\nwww.yogeshmalhotra.com \n\n Finally, the ﬁne-tuned model is then used to perform inference\non the same task from multiple diﬀerent target languages. Transformers\nhas shown remarkable progress in cross-lingual transfer learning, espe-\ncially when it comes to zero-shot learning. The details of how trans-\nformers perform cross-lingual transfer learning is still an active research\narea. This section discusses the active areas of research to understand\nthe main factors aﬀecting cross-lingual transfer learning [74]. The dis-\ncussed factors include: (i) data related factors, (ii) model architecture\nfactors, and (iii) training tasks factors.\n4.3.1.1 Data factors\nShared Vocabulary Prior to training a mLM, text from all languages\nis tokenized using a WordPiece [220] or SentencePiece [144] models. The\nmain concept is to tokenize each word into its most often occurring sub-\nwords. \n\n First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space. \n\n Multilingual transformers\nis certainly no exception but even more intriguing as they appear to use\nand share knowledge across multiple languages; i.e., cross-lingual trans-\nfer learning. This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. \n\n This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. \n\n This is\ndespite the fact that it does not use parallel corpora for pre-training.\nXLM-R follows the architectural design of XLM without using the lan-\nguage embeddings which allows for better handling of code-switching.\nThe main contributions of XLM-R are achieving state-of-the-art perfor-\nmance with monolingual data and proving deep insights into the value\nof cleaner and larger training data, the size of the vocabulary, as well as\nthe right level of hyperparameters tuning.\n4.1.2.4 Alternating Language Model (ALM)\nAlternating Language Model (ALM) ALM [282] is a pre-training\ntask for cross-lingual representations that works in the same way as the\nTLMpre-trainingtaskexceptforthenatureoftheinputdata.TLMuses\nparallel sentences for input, however ALM uses code-switched sentences\ngenerated from parallel sentences. In code-switched sentences, some \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n \n La question est How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:06:52.451210",
              "duration": 9.255254,
              "cost": 0.13278
            },
            "meta": {},
            "text": "The refund calculation for the unused portion of the premium or contribution during the period when the risk did not occur can vary based on the terms and conditions of the insurance policy or agreement. Typically, it is calculated on a pro-rata basis, meaning the refund amount is proportional to the unused period of coverage. For instance, if the risk did not occur for half of the coverage period, then half of the premium or contribution may be refunded. However, some policies may have specific conditions or fees that could affect the refund amount. It's always recommended to review the policy documents or consult with the insurance provider for accurate information.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The refund calculation for the unused portion of the premium or contribution during the period when the risk did not occur can vary based on the terms and conditions of the insurance policy or agreement. Typically, it is calculated on a pro-rata basis, meaning the refund amount is proportional to the unused period of coverage. For instance, if the risk did not occur for half of the coverage period, then half of the premium or contribution may be refunded. However, some policies may have specific conditions or fees that could affect the refund amount. It's always recommended to review the policy documents or consult with the insurance provider for accurate information.\n\n1. 1. The refund for the unused part of the premium or contribution is calculated when the risk did not occur.\n2. 2. The refund amount can differ based on the terms and conditions of the insurance policy or agreement.\n3. 3. The refund is usually calculated on a pro-rata basis, which means it is proportional to the unused coverage period.\n4. 4. If the risk did not occur for half of the coverage period, then half of the premium or contribution might be refunded.\n5. 5. Some insurance policies may have specific conditions or fees that could influence the refund amount.\n6. 6. It is advisable to review the policy documents or speak with the insurance provider for precise information.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:51.297645",
                "duration": 2.251945,
                "cost": 0.01221
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The refund for the portion of the premium or contribution not used during the period when the risk did not occur is typically calculated based on the insurance company's refund policy or the terms of the contribution agreement. \n\nHere are some common methods used to calculate refunds in such situations:\n\n1. Pro-rata Refund: This method involves calculating the unused portion of the premium or contribution on a pro-rata basis. The refund amount is determined by dividing the total premium or contribution by the total coverage period and then multiplying that by the number of days the risk did not occur.\n\n2. Short-Rate Cancellation: In some cases, insurance policies or contribution agreements may have a short-rate cancellation provision. This means that if the policyholder cancels before the end of the coverage period, the refund amount will be less than the pro-rata amount to account for administrative costs or fees.\n\n3. Flat Fee Deduction: Some insurance policies or contribution agreements may have a flat fee deduction for cancellations or early terminations. In this case, the refund amount is reduced by a predetermined flat fee.\n\nIt's important to review the specific terms and conditions of the insurance policy or contribution agreement to understand how refunds are calculated in the event that the risk does not occur during the coverage period.",
              "prompt": {
                "meta": {},
                "user": "How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?",
                "system": "Contexte :  (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω. \n\n (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling. \n\n If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω.\n5.2.2.4 Big Bird\nBig Bird is another eﬀort to give the Transformer a sparse attention\nmechanism that allows for linear scaling. It is also proven to be Turing\ncomplete and a universal sequence function approximator [292].\nAn Attention mechanism is a directed graph Big Bird describes\nthe attention mechanism as a directed graph. The vertices represent the\nLpositions in the input sequence. The directed edges represent the pre-\nsoftmax attention weights. In other words, the directed edges are the\ninner products between the query and key vectors, where the ithvertex \n\n xviii■Foreword\nfocus on AI explainability, an increasingly important theme in practice\nimposed by greater focus on issues such as ethical AI and trustable AI.\n– Dr. Yogesh Malhotra\nFounding Chairman and CEO\nU.S. Venture Capital and Private Equity Firm\nGlobal Risk Management Network LLC\nscientist\nwww.yogeshmalhotra.com \n\n Finally, the ﬁne-tuned model is then used to perform inference\non the same task from multiple diﬀerent target languages. Transformers\nhas shown remarkable progress in cross-lingual transfer learning, espe-\ncially when it comes to zero-shot learning. The details of how trans-\nformers perform cross-lingual transfer learning is still an active research\narea. This section discusses the active areas of research to understand\nthe main factors aﬀecting cross-lingual transfer learning [74]. The dis-\ncussed factors include: (i) data related factors, (ii) model architecture\nfactors, and (iii) training tasks factors.\n4.3.1.1 Data factors\nShared Vocabulary Prior to training a mLM, text from all languages\nis tokenized using a WordPiece [220] or SentencePiece [144] models. The\nmain concept is to tokenize each word into its most often occurring sub-\nwords. \n\n First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52].\nParallel Corpora Representations Another way to approach the\nlanguage-agnostic representation hypothesis is to use parallel corpora in\ndiﬀerent languages and examine their parallel representations for align-\nment in the model embedding space. \n\n Multilingual transformers\nis certainly no exception but even more intriguing as they appear to use\nand share knowledge across multiple languages; i.e., cross-lingual trans-\nfer learning. This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. \n\n This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. \n\n This is\ndespite the fact that it does not use parallel corpora for pre-training.\nXLM-R follows the architectural design of XLM without using the lan-\nguage embeddings which allows for better handling of code-switching.\nThe main contributions of XLM-R are achieving state-of-the-art perfor-\nmance with monolingual data and proving deep insights into the value\nof cleaner and larger training data, the size of the vocabulary, as well as\nthe right level of hyperparameters tuning.\n4.1.2.4 Alternating Language Model (ALM)\nAlternating Language Model (ALM) ALM [282] is a pre-training\ntask for cross-lingual representations that works in the same way as the\nTLMpre-trainingtaskexceptforthenatureoftheinputdata.TLMuses\nparallel sentences for input, however ALM uses code-switched sentences\ngenerated from parallel sentences. In code-switched sentences, some \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n \n La question est How is the refund calculated for the portion of the premium or contribution not used during the period when the risk did not occur?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:06:58.950584",
              "duration": 6.495598,
              "cost": 0.0024699999999999995
            },
            "meta": {},
            "text": "The refund for the portion of the premium or contribution not used during the period when the risk did not occur is typically calculated based on the insurance company's refund policy or the terms of the contribution agreement. \n\nHere are some common methods used to calculate refunds in such situations:\n\n1. Pro-rata Refund: This method involves calculating the unused portion of the premium or contribution on a pro-rata basis. The refund amount is determined by dividing the total premium or contribution by the total coverage period and then multiplying that by the number of days the risk did not occur.\n\n2. Short-Rate Cancellation: In some cases, insurance policies or contribution agreements may have a short-rate cancellation provision. This means that if the policyholder cancels before the end of the coverage period, the refund amount will be less than the pro-rata amount to account for administrative costs or fees.\n\n3. Flat Fee Deduction: Some insurance policies or contribution agreements may have a flat fee deduction for cancellations or early terminations. In this case, the refund amount is reduced by a predetermined flat fee.\n\nIt's important to review the specific terms and conditions of the insurance policy or contribution agreement to understand how refunds are calculated in the event that the risk does not occur during the coverage period.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 5, 6",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The refund for the portion of the premium or contribution not used during the period when the risk did not occur is typically calculated based on the insurance company's refund policy or the terms of the contribution agreement. \n\nHere are some common methods used to calculate refunds in such situations:\n\n1. Pro-rata Refund: This method involves calculating the unused portion of the premium or contribution on a pro-rata basis. The refund amount is determined by dividing the total premium or contribution by the total coverage period and then multiplying that by the number of days the risk did not occur.\n\n2. Short-Rate Cancellation: In some cases, insurance policies or contribution agreements may have a short-rate cancellation provision. This means that if the policyholder cancels before the end of the coverage period, the refund amount will be less than the pro-rata amount to account for administrative costs or fees.\n\n3. Flat Fee Deduction: Some insurance policies or contribution agreements may have a flat fee deduction for cancellations or early terminations. In this case, the refund amount is reduced by a predetermined flat fee.\n\nIt's important to review the specific terms and conditions of the insurance policy or contribution agreement to understand how refunds are calculated in the event that the risk does not occur during the coverage period.\n\n1. 1. The refund for the unused part of the premium or contribution is calculated when the risk did not occur.\n2. 2. The refund amount can differ based on the terms and conditions of the insurance policy or agreement.\n3. 3. The refund is usually calculated on a pro-rata basis, which means it is proportional to the unused coverage period.\n4. 4. If the risk did not occur for half of the coverage period, then half of the premium or contribution might be refunded.\n5. 5. Some insurance policies may have specific conditions or fees that could influence the refund amount.\n6. 6. It is advisable to review the policy documents or speak with the insurance provider for precise information.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:53.307923",
                "duration": 2.007542,
                "cost": 0.01593
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '5', '6'] - Not validated: ['4']",
              "human": 1.0,
              "auto": 0.8333333333333334
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Define \"Préjudice d’affection\" as described in the document."
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The term \"Préjudice d’affection\" is not specifically defined in the document.\n2. Compensation for \"Préjudice d’affection\" is given in the event of death.\n3. The spouse and children of the deceased insured are eligible for this compensation.\n4. If the spouse and children are not present, the deceased's parents can receive the compensation.\n5. The maximum total compensation for \"Préjudice d’affection\" is 50,000 €.\n6. Each beneficiary can receive up to 10,000 € as compensation.",
          "prompt": {
            "meta": {},
            "user": "The document does not provide a specific definition for \"Préjudice d’affection\". However, it mentions that in the event of death, compensation for \"Préjudice d’affection\" is provided to the spouse and children of the deceased insured, or in their absence, to the deceased's father and/or mother. The maximum compensation is set at 50,000 €, with a limit of 10,000 € per beneficiary.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:16.196217",
          "duration": 8.07133,
          "cost": 0.01341
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The term \"Préjudice d’affection\" is not specifically defined in the document."
          },
          {
            "meta": {},
            "text": "2. Compensation for \"Préjudice d’affection\" is given in the event of death."
          },
          {
            "meta": {},
            "text": "3. The spouse and children of the deceased insured are eligible for this compensation."
          },
          {
            "meta": {},
            "text": "4. If the spouse and children are not present, the deceased's parents can receive the compensation."
          },
          {
            "meta": {},
            "text": "5. The maximum total compensation for \"Préjudice d’affection\" is 50,000 €."
          },
          {
            "meta": {},
            "text": "6. Each beneficiary can receive up to 10,000 € as compensation."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.517119601695377,
              "Node id": "4462d9ef-1ab7-4aa3-8402-933f54c091ed"
            },
            "text": "Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3."
          },
          {
            "meta": {
              "score": 9.44522501531532,
              "Node id": "09895a00-9794-44b1-a59e-d0f5f5cac5db"
            },
            "text": "Pertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le"
          },
          {
            "meta": {
              "score": 9.44522501531532,
              "Node id": "81ae1983-c36b-4c2c-9ada-1f0759349c58"
            },
            "text": "- pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident , \n› aux personnes dont l’assuré ou son conjoint a la tutelle ou la curatelle et qui sont économiquement à la charge  de l’assuré \ndécédé au jour de l’accident ."
          },
          {
            "meta": {
              "score": 9.304645994936163,
              "Node id": "75a165d2-7991-414b-baac-70fda11a42fc"
            },
            "text": "• en cas de décès :  \n- pour la participation aux frais d’obsèques : à la personne ayant exposé les frais,  \n- pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident ,"
          },
          {
            "meta": {
              "score": 9.235914135222377,
              "Node id": "3fe55227-4adc-4082-83a8-a807b14b6068"
            },
            "text": "supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités"
          },
          {
            "meta": {
              "score": 9.235914135222377,
              "Node id": "a5ba3bf6-7443-47de-a807-b3e4f026ae53"
            },
            "text": "(article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au"
          },
          {
            "meta": {
              "score": 9.235914135222377,
              "Node id": "951a9567-ed53-4fdb-a041-db100821c7a9"
            },
            "text": ": 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3"
          },
          {
            "meta": {
              "score": 9.235914135222377,
              "Node id": "4e9934db-8538-4ee9-8a17-4abda4de5510"
            },
            "text": "),  \n le préjudice esthétique permanent  (P.E .P.), \n le préjudice d’agrément  (P.A.). \n les frais  de logement  adapté s, \n les frais de  véhicu le adaptés  \n \nEn cas de décès :  \n \n les pertes de revenus des ayants droit  consécutives au décès  du conducteur, que ce décès su rvienne imm édiatement \nou dans le délai d ’un a n des suites de l’accident garan ti (P.R.),  \n le préjudice d’affection  (P.A.F.), \n les frais d’obsè ques (F. O.)."
          },
          {
            "meta": {
              "score": 9.101452330843005,
              "Node id": "f59213b3-6eba-4f66-9608-7816966bf62f"
            },
            "text": "taux d’incapacité permanente  supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la"
          },
          {
            "meta": {
              "score": 9.101452330843005,
              "Node id": "4eed6964-00df-4598-bcf0-a006bc92cf07"
            },
            "text": "Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. \nPrestations \nd’accompagnement \npersonnalisé  \n(article 27 -3 E) Plafond  : 5 000 €, soit 250 unités de valeur "
          },
          {
            "meta": {
              "score": 0.7400914349610209,
              "Node id": "60bd3b32-8484-41a7-a0f3-4cbd6db53424"
            },
            "text": "[146]G. Lample and A. Conneau ,Cross-lingual language model\npretraining , Advances in Neural Information Processing Systems\n(NeurIPS), (2019).\n[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp."
          },
          {
            "meta": {
              "score": 0.7380289020368007,
              "Node id": "cc114184-b680-4833-978b-f380b8388f55"
            },
            "text": "228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067."
          },
          {
            "meta": {
              "score": 0.7366927075485387,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7359825193999907,
              "Node id": "8528a867-3990-4efe-b3d4-8fee9179c3e6"
            },
            "text": "8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018."
          },
          {
            "meta": {
              "score": 0.7356045117827777,
              "Node id": "5cc5c7ac-a675-4549-8624-7809fe1becce"
            },
            "text": "[154]G. Letarte, F. Paradis, P. Giguère, and F. Laviolette ,\nImportance of self-attention for sentiment analysis , in Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, 2018, pp. 267–275.\n[155]M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, and L. Zettlemoyer ,\nBART: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension , in Proceed-\nings of the 58th Annual Meeting of the Association for Compu-\ntational Linguistics, Online, July 2020, Association for Computa-\ntional Linguistics, pp. 7871–7880.\n[156]P. S. H. Lewis, B. Oguz, R. Rinott, S. Riedel, and\nH. Schwenk ,MLQA: evaluating cross-lingual extractive question\nanswering , in Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020, Online, July\n5–10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault,\neds., Association for Computational Linguistics, 2020, pp."
          },
          {
            "meta": {
              "score": 0.7340967905654354,
              "Node id": "ea60f0c6-b524-4065-949a-f71285cfb01c"
            },
            "text": "Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots."
          },
          {
            "meta": {
              "score": 0.7330262653782945,
              "Node id": "073a1436-4257-431e-ad87-c83e58b41f38"
            },
            "text": "337–346.\n[5]D. Amodei, C. Olah, J. Steinhardt, P. Christiano,\nJ. Schulman, and D. Mané ,Concrete problems in ai safety ,\narXiv preprint arXiv:1606.06565, (2016).\n[6]A. Andoni, P. Indyk, T. Laarhoven, I. P. Razenshteyn,\nand L. Schmidt ,Practical and optimal lsh for angular distance ,\nin NIPS, 2015.\n[7]D. Araci ,Finbert: Financial sentiment analysis with pre-trained\nlanguage models , ArXiv, abs/1908.10063 (2019).\n[8]E. Arkhangelskaia and S. Dutta ,Whatcha lookin’at?\ndeeplifting bert’s attention in question answering , arXiv preprint\narXiv:1910.06431, (2019)."
          },
          {
            "meta": {
              "score": 0.7326128311381126,
              "Node id": "fe78f1b2-4301-4e47-85a2-e52abf0be2ef"
            },
            "text": "Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]."
          },
          {
            "meta": {
              "score": 0.7324914559171848,
              "Node id": "d1331472-353b-450d-ac4f-0d074ce47b88"
            },
            "text": "[75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314."
          },
          {
            "meta": {
              "score": 0.732206217823004,
              "Node id": "c194b57c-f2ab-4d62-adce-5d3482fec02d"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The document does not provide a specific definition for \"Préjudice d’affection\". However, it mentions that in the event of death, compensation for \"Préjudice d’affection\" is provided to the spouse and children of the deceased insured, or in their absence, to the deceased's father and/or mother. The maximum compensation is set at 50,000 €, with a limit of 10,000 € per beneficiary.",
              "prompt": {
                "meta": {},
                "user": "Define \"Préjudice d’affection\" as described in the document.",
                "system": "Contexte :  Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. \n\n Pertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le \n\n - pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident , \n› aux personnes dont l’assuré ou son conjoint a la tutelle ou la curatelle et qui sont économiquement à la charge  de l’assuré \ndécédé au jour de l’accident . \n\n • en cas de décès :  \n- pour la participation aux frais d’obsèques : à la personne ayant exposé les frais,  \n- pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident , \n\n supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités \n\n (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au \n\n : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 \n\n ),  \n le préjudice esthétique permanent  (P.E .P.), \n le préjudice d’agrément  (P.A.). \n les frais  de logement  adapté s, \n les frais de  véhicu le adaptés  \n \nEn cas de décès :  \n \n les pertes de revenus des ayants droit  consécutives au décès  du conducteur, que ce décès su rvienne imm édiatement \nou dans le délai d ’un a n des suites de l’accident garan ti (P.R.),  \n le préjudice d’affection  (P.A.F.), \n les frais d’obsè ques (F. O.). \n\n taux d’incapacité permanente  supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la \n\n Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. \nPrestations \nd’accompagnement \npersonnalisé  \n(article 27 -3 E) Plafond  : 5 000 €, soit 250 unités de valeur  \n\n [146]G. Lample and A. Conneau ,Cross-lingual language model\npretraining , Advances in Neural Information Processing Systems\n(NeurIPS), (2019).\n[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. \n\n 228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018. \n\n [154]G. Letarte, F. Paradis, P. Giguère, and F. Laviolette ,\nImportance of self-attention for sentiment analysis , in Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, 2018, pp. 267–275.\n[155]M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, and L. Zettlemoyer ,\nBART: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension , in Proceed-\nings of the 58th Annual Meeting of the Association for Compu-\ntational Linguistics, Online, July 2020, Association for Computa-\ntional Linguistics, pp. 7871–7880.\n[156]P. S. H. Lewis, B. Oguz, R. Rinott, S. Riedel, and\nH. Schwenk ,MLQA: evaluating cross-lingual extractive question\nanswering , in Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020, Online, July\n5–10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault,\neds., Association for Computational Linguistics, 2020, pp. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 337–346.\n[5]D. Amodei, C. Olah, J. Steinhardt, P. Christiano,\nJ. Schulman, and D. Mané ,Concrete problems in ai safety ,\narXiv preprint arXiv:1606.06565, (2016).\n[6]A. Andoni, P. Indyk, T. Laarhoven, I. P. Razenshteyn,\nand L. Schmidt ,Practical and optimal lsh for angular distance ,\nin NIPS, 2015.\n[7]D. Araci ,Finbert: Financial sentiment analysis with pre-trained\nlanguage models , ArXiv, abs/1908.10063 (2019).\n[8]E. Arkhangelskaia and S. Dutta ,Whatcha lookin’at?\ndeeplifting bert’s attention in question answering , arXiv preprint\narXiv:1910.06431, (2019). \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n [75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n \n La question est Define \"Préjudice d’affection\" as described in the document."
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:05.747132",
              "duration": 6.791423,
              "cost": 0.13611
            },
            "meta": {},
            "text": "The document does not provide a specific definition for \"Préjudice d’affection\". However, it mentions that in the event of death, compensation for \"Préjudice d’affection\" is provided to the spouse and children of the deceased insured, or in their absence, to the deceased's father and/or mother. The maximum compensation is set at 50,000 €, with a limit of 10,000 € per beneficiary.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The document does not provide a specific definition for \"Préjudice d’affection\". However, it mentions that in the event of death, compensation for \"Préjudice d’affection\" is provided to the spouse and children of the deceased insured, or in their absence, to the deceased's father and/or mother. The maximum compensation is set at 50,000 €, with a limit of 10,000 € per beneficiary.\n\n1. 1. The term \"Préjudice d’affection\" is not specifically defined in the document.\n2. 2. Compensation for \"Préjudice d’affection\" is given in the event of death.\n3. 3. The spouse and children of the deceased insured are eligible for this compensation.\n4. 4. If the spouse and children are not present, the deceased's parents can receive the compensation.\n5. 5. The maximum total compensation for \"Préjudice d’affection\" is 50,000 €.\n6. 6. Each beneficiary can receive up to 10,000 € as compensation.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:55.004856",
                "duration": 1.693297,
                "cost": 0.010920000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Le \"préjudice d’affection\" tel que décrit dans le document fait référence à une indemnisation accordée aux proches de l'assuré décédé en raison de la perte affective subie suite au décès de l'assuré. Cette indemnisation est destinée au conjoint, aux enfants, et en l'absence de ceux-ci, aux parents de l'assuré décédé. Il s'agit d'une compensation financière pour le préjudice émotionnel et affectif causé par la perte de l'être cher.",
              "prompt": {
                "meta": {},
                "user": "Define \"Préjudice d’affection\" as described in the document.",
                "system": "Contexte :  Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. \n\n Pertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le \n\n - pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident , \n› aux personnes dont l’assuré ou son conjoint a la tutelle ou la curatelle et qui sont économiquement à la charge  de l’assuré \ndécédé au jour de l’accident . \n\n • en cas de décès :  \n- pour la participation aux frais d’obsèques : à la personne ayant exposé les frais,  \n- pour le préjudice d’affection des proches :  \n› au conjoint  et aux enfants de l’assuré décédé,  \n› à défaut au père et/ou à la mère de l’assuré décédé,  \n- pour les pertes de revenus des proches , les services à la personne et les prestations d’accompagnement personnalisé :  \n› au conjoint  de l’assuré décédé,  \n› aux enfants de l’assuré ou de son conjoint , âgés de moins de 25 ans et économiquement à charge  de l’assuré décédé au \njour de l’accident , \n\n supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités \n\n (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au \n\n : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 \n\n ),  \n le préjudice esthétique permanent  (P.E .P.), \n le préjudice d’agrément  (P.A.). \n les frais  de logement  adapté s, \n les frais de  véhicu le adaptés  \n \nEn cas de décès :  \n \n les pertes de revenus des ayants droit  consécutives au décès  du conducteur, que ce décès su rvienne imm édiatement \nou dans le délai d ’un a n des suites de l’accident garan ti (P.R.),  \n le préjudice d’affection  (P.A.F.), \n les frais d’obsè ques (F. O.). \n\n taux d’incapacité permanente  supérieur à 65 % (seuil de déclenchement)  \nPertes de gains \nprofessionnels \nfuturs (article 27 -\n2 E) Plafond  : 250  000 € \nEN CAS DE D ÉCÈS  \nParticipation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la \n\n Participation aux \nfrais d’obsèques \n(article 27 -3 A) Plafond  : 5 000 € \nPréjudice \nd’affection  \n(article 27 -3 B) Plafond  : 50 000 € dans la limite de 10 000 € par bénéficiaire  \nPertes de revenus \ndes proches  \n(article 27-3 C) Plafond  : 940 000 €  \nServices à la \npersonne (article \n27-3 D) Nombre d’unités de valeur  de services à la personne alloué dans la limite de  10 unités de valeur  (5) \n \n(5) Le  plafond est majoré de 50 % dans les situations visées au paragraphe B de l’article 27 -3 D-3. \nPrestations \nd’accompagnement \npersonnalisé  \n(article 27 -3 E) Plafond  : 5 000 €, soit 250 unités de valeur  \n\n [146]G. Lample and A. Conneau ,Cross-lingual language model\npretraining , Advances in Neural Information Processing Systems\n(NeurIPS), (2019).\n[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. \n\n 228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018. \n\n [154]G. Letarte, F. Paradis, P. Giguère, and F. Laviolette ,\nImportance of self-attention for sentiment analysis , in Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, 2018, pp. 267–275.\n[155]M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, and L. Zettlemoyer ,\nBART: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension , in Proceed-\nings of the 58th Annual Meeting of the Association for Compu-\ntational Linguistics, Online, July 2020, Association for Computa-\ntional Linguistics, pp. 7871–7880.\n[156]P. S. H. Lewis, B. Oguz, R. Rinott, S. Riedel, and\nH. Schwenk ,MLQA: evaluating cross-lingual extractive question\nanswering , in Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020, Online, July\n5–10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault,\neds., Association for Computational Linguistics, 2020, pp. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 337–346.\n[5]D. Amodei, C. Olah, J. Steinhardt, P. Christiano,\nJ. Schulman, and D. Mané ,Concrete problems in ai safety ,\narXiv preprint arXiv:1606.06565, (2016).\n[6]A. Andoni, P. Indyk, T. Laarhoven, I. P. Razenshteyn,\nand L. Schmidt ,Practical and optimal lsh for angular distance ,\nin NIPS, 2015.\n[7]D. Araci ,Finbert: Financial sentiment analysis with pre-trained\nlanguage models , ArXiv, abs/1908.10063 (2019).\n[8]E. Arkhangelskaia and S. Dutta ,Whatcha lookin’at?\ndeeplifting bert’s attention in question answering , arXiv preprint\narXiv:1910.06431, (2019). \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n [75]W. B. Dolan and C. Brockett ,Automatically constructing a\ncorpus of sentential paraphrases , in IJCNLP, 2005.\n[76]Y.Dong,F.Liao,T.Pang,H.Su,J.Zhu,X.Hu,andJ.Li ,\nBoosting adversarial attacks with momentum ,inProceedingsofthe\nIEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 9185–9193.\n[77]Y. Dong, H. Su, J. Zhu, and B. Zhang ,Improving inter-\npretability of deep neural networks with semantic information , in\nProceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2017, pp. 4306–4314. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n \n La question est Define \"Préjudice d’affection\" as described in the document."
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:09.643403",
              "duration": 3.893219,
              "cost": 0.0023655
            },
            "meta": {},
            "text": "Le \"préjudice d’affection\" tel que décrit dans le document fait référence à une indemnisation accordée aux proches de l'assuré décédé en raison de la perte affective subie suite au décès de l'assuré. Cette indemnisation est destinée au conjoint, aux enfants, et en l'absence de ceux-ci, aux parents de l'assuré décédé. Il s'agit d'une compensation financière pour le préjudice émotionnel et affectif causé par la perte de l'être cher.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 3, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Le \"préjudice d’affection\" tel que décrit dans le document fait référence à une indemnisation accordée aux proches de l'assuré décédé en raison de la perte affective subie suite au décès de l'assuré. Cette indemnisation est destinée au conjoint, aux enfants, et en l'absence de ceux-ci, aux parents de l'assuré décédé. Il s'agit d'une compensation financière pour le préjudice émotionnel et affectif causé par la perte de l'être cher.\n\n1. 1. The term \"Préjudice d’affection\" is not specifically defined in the document.\n2. 2. Compensation for \"Préjudice d’affection\" is given in the event of death.\n3. 3. The spouse and children of the deceased insured are eligible for this compensation.\n4. 4. If the spouse and children are not present, the deceased's parents can receive the compensation.\n5. 5. The maximum total compensation for \"Préjudice d’affection\" is 50,000 €.\n6. 6. Each beneficiary can receive up to 10,000 € as compensation.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:56.238555",
                "duration": 1.230833,
                "cost": 0.0114
              },
              "meta": {},
              "text": "Validated: ['2', '3', '4'] - Not validated: ['1', '5', '6']",
              "human": 1.0,
              "auto": 0.5
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not clarify when insurance covers occasional towing.\n2. There is no information on whether insurance covers voluntary vehicle assistance.\n3. The circumstances for insurance coverage of occasional and voluntary towing are not specified.\n4. The text leaves out details on insurance coverage for voluntary vehicle towing.\n5. It is unclear from the text if insurance covers occasional vehicle assistance.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the circumstances under which insurance covers occasional and voluntary towing or assistance of a vehicle.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:21.397010",
          "duration": 5.197454,
          "cost": 0.00852
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not clarify when insurance covers occasional towing."
          },
          {
            "meta": {},
            "text": "2. There is no information on whether insurance covers voluntary vehicle assistance."
          },
          {
            "meta": {},
            "text": "3. The circumstances for insurance coverage of occasional and voluntary towing are not specified."
          },
          {
            "meta": {},
            "text": "4. The text leaves out details on insurance coverage for voluntary vehicle towing."
          },
          {
            "meta": {},
            "text": "5. It is unclear from the text if insurance covers occasional vehicle assistance."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.130782338341737,
              "Node id": "d85ab61f-2026-4419-90d4-b3321ef36a75"
            },
            "text": "These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43"
          },
          {
            "meta": {
              "score": 8.71436737939289,
              "Node id": "53a29409-2210-4f7b-8a35-f5fdff3344cc"
            },
            "text": "The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention."
          },
          {
            "meta": {
              "score": 8.685673413525214,
              "Node id": "bc5c5b0a-035b-4d19-a9d0-4c51d5a48160"
            },
            "text": "Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output."
          },
          {
            "meta": {
              "score": 8.591953969588662,
              "Node id": "9f5d9ddf-d872-4c28-868e-77927717e007"
            },
            "text": "BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones."
          },
          {
            "meta": {
              "score": 8.550477695665586,
              "Node id": "94bb0b0f-c28a-44ed-a903-a3d6c3ef0377"
            },
            "text": "In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input."
          },
          {
            "meta": {
              "score": 8.37156749786076,
              "Node id": "7f9ba7a0-0741-427b-a314-d064f98906db"
            },
            "text": "In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3."
          },
          {
            "meta": {
              "score": 8.352533725704655,
              "Node id": "a53b68ed-38fa-45f7-8fce-cbe4899e1ae0"
            },
            "text": "BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]."
          },
          {
            "meta": {
              "score": 8.352533725704655,
              "Node id": "619c73c2-6012-46fb-bab0-b3e8b846eaa3"
            },
            "text": "C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]."
          },
          {
            "meta": {
              "score": 8.144350961980205,
              "Node id": "0707183f-0274-4e7d-bb0c-b02ee1df8bb9"
            },
            "text": "Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig."
          },
          {
            "meta": {
              "score": 8.035306007364472,
              "Node id": "d03973d1-c44c-4a4a-b683-a74d7e01732b"
            },
            "text": "The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue."
          },
          {
            "meta": {
              "score": 0.7442042714459717,
              "Node id": "9f797128-a745-4618-a86d-4a80f1c255e9"
            },
            "text": "Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig."
          },
          {
            "meta": {
              "score": 0.7415185631199818,
              "Node id": "c0022481-9f99-42be-897d-829a707cb8fa"
            },
            "text": "In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input."
          },
          {
            "meta": {
              "score": 0.7411428208943519,
              "Node id": "86b6331f-7b20-46a1-af02-99b0657764d0"
            },
            "text": "For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig."
          },
          {
            "meta": {
              "score": 0.7382720208728141,
              "Node id": "3eb400d7-ed6f-40b4-881d-8ca571ad1a07"
            },
            "text": "The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue."
          },
          {
            "meta": {
              "score": 0.7373436422939064,
              "Node id": "ea74c81c-cbe2-4090-9416-0e5c92a03200"
            },
            "text": "In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3."
          },
          {
            "meta": {
              "score": 0.7351238617180994,
              "Node id": "24a4d5c0-f074-4452-826d-38ce0ee55a4f"
            },
            "text": "Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues."
          },
          {
            "meta": {
              "score": 0.7340522248897847,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7305751091614239,
              "Node id": "56772b37-3c36-4ebf-8b7a-f5991cec46d1"
            },
            "text": "In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv."
          },
          {
            "meta": {
              "score": 0.7243665120056979,
              "Node id": "2133fdfd-a446-4a02-8e70-6ac58a825b02"
            },
            "text": "In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]."
          },
          {
            "meta": {
              "score": 0.7242122033949556,
              "Node id": "173dddf4-c179-4905-af71-765ba3abaaef"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...!"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the circumstances under which insurance covers occasional and voluntary towing or assistance of a vehicle.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?",
                "system": "Contexte :  These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n \n La question est Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:12.606610",
              "duration": 2.958866,
              "cost": 0.12993
            },
            "meta": {},
            "text": "The text does not provide information on the circumstances under which insurance covers occasional and voluntary towing or assistance of a vehicle.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the circumstances under which insurance covers occasional and voluntary towing or assistance of a vehicle.\n\n1. 1. The text does not clarify when insurance covers occasional towing.\n2. 2. There is no information on whether insurance covers voluntary vehicle assistance.\n3. 3. The circumstances for insurance coverage of occasional and voluntary towing are not specified.\n4. 4. The text leaves out details on insurance coverage for voluntary vehicle towing.\n5. 5. It is unclear from the text if insurance covers occasional vehicle assistance.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:57.739501",
                "duration": 1.497654,
                "cost": 0.00717
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Insurance coverage for occasional and voluntary towing or assistance of a vehicle can vary depending on the specific insurance policy. In general, insurance policies may provide coverage for towing or assistance in the following circumstances:\n\n1. **Roadside Assistance**: Many insurance policies offer roadside assistance coverage, which can include services like towing, jump-starts, tire changes, and fuel delivery in case of a breakdown or mechanical failure.\n\n2. **Accidents**: If your vehicle is involved in an accident and needs to be towed to a repair shop, the towing costs may be covered by your insurance policy.\n\n3. **Mechanical Breakdown**: If your vehicle experiences a mechanical breakdown and needs to be towed to a repair facility, the towing costs may be covered by your insurance.\n\n4. **Stuck in a Ditch or Snow**: If your vehicle gets stuck in a ditch, snow, or mud and requires towing assistance to get back on the road, some insurance policies may cover the towing costs.\n\n5. **Voluntary Towing**: Some insurance policies may also cover voluntary towing, which means you can request towing services even if your vehicle is not in an emergency situation. This could include towing to a repair shop, storage facility, or another location.\n\nIt's important to review your insurance policy or contact your insurance provider to understand the specific coverage and limitations related to towing and assistance services for your vehicle.",
              "prompt": {
                "meta": {},
                "user": "Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?",
                "system": "Contexte :  These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n The bidirectional\nattention of the transformer encoder forces the [MASK]prediction task to\nusethecontextprovidedbytheothernon-maskedtokensinthesequence.\nBERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n BERT is pre-trained with a 15% mask-out rate. This simple MLM task\nhas a downside where a mismatch is created between pre-training and\nﬁne-tuning tasks as the special [MASK]token does not appear during\nﬁne-tuning. To overcome this issue, every token in the 15% masked-out\ntokens is subjected to the following heuristic:\n•With a probability of 80%, the token is replaced with the special\n[MASK]token.\n•With a probability of 10%, the token is replaced with a random\ntoken.\n•With a probability of 10%, the token is left unchanged.\nThe occasional insertion of random tokens (i.e., noise) pushes BERT to\nbe less biased towards the masked token, especially when the masked\ntoken is left unchanged, in its bidirectional context attention. The MLM\ntask uses cross-entropy loss only over the masked tokens and ignores the\nprediction of all non-masked ones. \n\n In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. \n\n Thenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. \n\n In this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n The “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. \n\n In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3. \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n \n La question est Under what circumstances does the insurance cover occasional and voluntary towing or assistance of a vehicle?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:20.140375",
              "duration": 7.529464,
              "cost": 0.002558
            },
            "meta": {},
            "text": "Insurance coverage for occasional and voluntary towing or assistance of a vehicle can vary depending on the specific insurance policy. In general, insurance policies may provide coverage for towing or assistance in the following circumstances:\n\n1. **Roadside Assistance**: Many insurance policies offer roadside assistance coverage, which can include services like towing, jump-starts, tire changes, and fuel delivery in case of a breakdown or mechanical failure.\n\n2. **Accidents**: If your vehicle is involved in an accident and needs to be towed to a repair shop, the towing costs may be covered by your insurance policy.\n\n3. **Mechanical Breakdown**: If your vehicle experiences a mechanical breakdown and needs to be towed to a repair facility, the towing costs may be covered by your insurance.\n\n4. **Stuck in a Ditch or Snow**: If your vehicle gets stuck in a ditch, snow, or mud and requires towing assistance to get back on the road, some insurance policies may cover the towing costs.\n\n5. **Voluntary Towing**: Some insurance policies may also cover voluntary towing, which means you can request towing services even if your vehicle is not in an emergency situation. This could include towing to a repair shop, storage facility, or another location.\n\nIt's important to review your insurance policy or contact your insurance provider to understand the specific coverage and limitations related to towing and assistance services for your vehicle.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Insurance coverage for occasional and voluntary towing or assistance of a vehicle can vary depending on the specific insurance policy. In general, insurance policies may provide coverage for towing or assistance in the following circumstances:\n\n1. **Roadside Assistance**: Many insurance policies offer roadside assistance coverage, which can include services like towing, jump-starts, tire changes, and fuel delivery in case of a breakdown or mechanical failure.\n\n2. **Accidents**: If your vehicle is involved in an accident and needs to be towed to a repair shop, the towing costs may be covered by your insurance policy.\n\n3. **Mechanical Breakdown**: If your vehicle experiences a mechanical breakdown and needs to be towed to a repair facility, the towing costs may be covered by your insurance.\n\n4. **Stuck in a Ditch or Snow**: If your vehicle gets stuck in a ditch, snow, or mud and requires towing assistance to get back on the road, some insurance policies may cover the towing costs.\n\n5. **Voluntary Towing**: Some insurance policies may also cover voluntary towing, which means you can request towing services even if your vehicle is not in an emergency situation. This could include towing to a repair shop, storage facility, or another location.\n\nIt's important to review your insurance policy or contact your insurance provider to understand the specific coverage and limitations related to towing and assistance services for your vehicle.\n\n1. 1. The text does not clarify when insurance covers occasional towing.\n2. 2. There is no information on whether insurance covers voluntary vehicle assistance.\n3. 3. The circumstances for insurance coverage of occasional and voluntary towing are not specified.\n4. 4. The text leaves out details on insurance coverage for voluntary vehicle towing.\n5. 5. It is unclear from the text if insurance covers occasional vehicle assistance.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:27:59.128714",
                "duration": 1.386622,
                "cost": 0.01425
              },
              "meta": {},
              "text": "Validated: ['2', '4'] - Not validated: ['1', '3', '5']",
              "human": 1.0,
              "auto": 0.4
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not mention the need for insurance during test drives.\n2. There is no information about maintaining insurance coverage for test drives in the text.\n3. The document lacks details on insurance requirements for test drives.\n4. The text does not specify insurance obligations during test drives of a vehicle for sale.\n5. There is no reference to insurance coverage during test drives in the provided text.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:27.901085",
          "duration": 6.500889,
          "cost": 0.00897
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not mention the need for insurance during test drives."
          },
          {
            "meta": {},
            "text": "2. There is no information about maintaining insurance coverage for test drives in the text."
          },
          {
            "meta": {},
            "text": "3. The document lacks details on insurance requirements for test drives."
          },
          {
            "meta": {},
            "text": "4. The text does not specify insurance obligations during test drives of a vehicle for sale."
          },
          {
            "meta": {},
            "text": "5. There is no reference to insurance coverage during test drives in the provided text."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.919560312032086,
              "Node id": "b3ab183f-bdcf-432c-bd72-afc4481e9e1a"
            },
            "text": "(CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara."
          },
          {
            "meta": {
              "score": 11.782060790099958,
              "Node id": "17bcfd80-d5a2-4a44-bb7f-b68075164839"
            },
            "text": "For permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author."
          },
          {
            "meta": {
              "score": 8.86154855048396,
              "Node id": "26896112-48ac-4e5c-ae42-65476c2a7523"
            },
            "text": "Multilingual Transformer Architectures ■85\nXLCO is sentence-based, HICTL provides both sentence and word-level\ncross-lingual representations.\nSentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation."
          },
          {
            "meta": {
              "score": 8.7176306375789,
              "Node id": "d45a0869-fd72-444a-ad74-7e995823e635"
            },
            "text": "andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),"
          },
          {
            "meta": {
              "score": 8.407120683866086,
              "Node id": "8c187805-b390-4a31-874c-fbf50d5e3f7c"
            },
            "text": "then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7."
          },
          {
            "meta": {
              "score": 8.17419778262001,
              "Node id": "8c5a905f-498e-4179-8da2-b1444328542e"
            },
            "text": "Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7."
          },
          {
            "meta": {
              "score": 8.117969819631286,
              "Node id": "c81640bd-aeeb-4805-b1d8-7e251b017898"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used."
          },
          {
            "meta": {
              "score": 7.900586346181768,
              "Node id": "c576768f-5841-44de-b5a9-bd4e7bae251c"
            },
            "text": "i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),"
          },
          {
            "meta": {
              "score": 7.6945414621998856,
              "Node id": "faea14e8-dc72-4780-bd52-1289052ccd73"
            },
            "text": "14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process."
          },
          {
            "meta": {
              "score": 7.508463496402751,
              "Node id": "d38601fb-7281-4379-9cd6-416e8166ee2a"
            },
            "text": "Sentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation.\n4.1.3 Dual-Encoder Multilingual NLU\n4.1.3.1 Language-agnostic BERT Sentence Embedding (LaBSE)\nLanguage-agnostic BERT Sentence Embedding (LaBSE) [88] is an ar-\nchitecture for training cross-lingual sentence representations which com-\nbines Masked Language Model (MLM) and Translation Language Model\n(TLM) pre-training tasks from XLM [146] with a translation ranking\ntask using bi-directional dual-encoders with additive margin softmax\nloss[283]."
          },
          {
            "meta": {
              "score": 0.7542022308864127,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7491551138633025,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7489164918474962,
              "Node id": "20010ae9-c968-4c58-8312-204c37d847a5"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 0.7458375910711673,
              "Node id": "78b973d1-bc09-46b8-b151-630ed535c4a7"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 0.7413103689979188,
              "Node id": "0dca327f-4762-4975-82a4-0a9a1a50e6cf"
            },
            "text": "3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]."
          },
          {
            "meta": {
              "score": 0.7320962195702042,
              "Node id": "d380ad8c-1a90-4606-a113-6ff7104777c0"
            },
            "text": "As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]."
          },
          {
            "meta": {
              "score": 0.7316000598764192,
              "Node id": "7dad7d13-821f-4bbf-901b-ae5ef729a61a"
            },
            "text": "2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training."
          },
          {
            "meta": {
              "score": 0.7249578258572056,
              "Node id": "6ae11509-009f-423f-a3a1-4d8e649b3d90"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?"
          },
          {
            "meta": {
              "score": 0.7245893306013836,
              "Node id": "c4f1af7e-3e97-4c63-a5a3-4c51de9ef39c"
            },
            "text": "Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion."
          },
          {
            "meta": {
              "score": 0.7235785505050721,
              "Node id": "d3f527c5-3391-4393-a503-936de230a06a"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale.",
              "prompt": {
                "meta": {},
                "user": "What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?",
                "system": "Contexte :  (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. \n\n For permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \n\n Multilingual Transformer Architectures ■85\nXLCO is sentence-based, HICTL provides both sentence and word-level\ncross-lingual representations.\nSentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation. \n\n andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14), \n\n then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n Sentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation.\n4.1.3 Dual-Encoder Multilingual NLU\n4.1.3.1 Language-agnostic BERT Sentence Embedding (LaBSE)\nLanguage-agnostic BERT Sentence Embedding (LaBSE) [88] is an ar-\nchitecture for training cross-lingual sentence representations which com-\nbines Masked Language Model (MLM) and Translation Language Model\n(TLM) pre-training tasks from XLM [146] with a translation ranking\ntask using bi-directional dual-encoders with additive margin softmax\nloss[283]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n \n La question est What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:22.591160",
              "duration": 2.446288,
              "cost": 0.13248
            },
            "meta": {},
            "text": "The text does not provide information on the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale.\n\n1. 1. The text does not mention the need for insurance during test drives.\n2. 2. There is no information about maintaining insurance coverage for test drives in the text.\n3. 3. The document lacks details on insurance requirements for test drives.\n4. 4. The text does not specify insurance obligations during test drives of a vehicle for sale.\n5. 5. There is no reference to insurance coverage during test drives in the provided text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:00.609121",
                "duration": 1.477435,
                "cost": 0.007410000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Maintaining insurance coverage during test drives for a vehicle being prepared for sale is crucial to protect both the seller and potential buyers. Here are some common requirements:\n\n1. **Existing Insurance Policy**: The vehicle should be covered under the seller's existing insurance policy. It's important to inform the insurance company that the vehicle is being used for test drives during the selling process.\n\n2. **Liability Coverage**: The insurance policy should include liability coverage to protect against any accidents or damages that may occur during test drives. This coverage typically helps pay for the other party's medical expenses or property damage if the seller is found at fault.\n\n3. **Comprehensive and Collision Coverage**: Comprehensive coverage protects against non-collision incidents like theft or vandalism, while collision coverage helps pay for damages to the vehicle in case of an accident during a test drive.\n\n4. **Permission for Test Drives**: Some insurance policies may require explicit permission for test drives. Make sure to check with your insurance provider if this is a requirement.\n\n5. **Driver Requirements**: Ensure that anyone test driving the vehicle meets the driver requirements specified in your insurance policy. This may include having a valid driver's license and meeting age restrictions.\n\n6. **Notification of Changes**: Inform your insurance company about any changes in the vehicle's status, such as it being used for test drives. Failure to disclose this information could lead to coverage issues.\n\n7. **Temporary Coverage**: If necessary, you may need to obtain temporary coverage or a special policy for test drives if your existing policy does not adequately cover this activity.\n\nIt's essential to review your insurance policy and discuss any specific requirements or concerns with your insurance provider to ensure that you have appropriate coverage during test drives for a vehicle being prepared for sale.",
              "prompt": {
                "meta": {},
                "user": "What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?",
                "system": "Contexte :  (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. \n\n For permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. For works that are not available on CCC please contact mpkbookspermis-sions@tandf.co.uk\nTrademark notice: Product or corporate names may be trademarks or registered trademarks and are \nused only for identification and explanation without intent to infringe.\nISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \n\n Multilingual Transformer Architectures ■85\nXLCO is sentence-based, HICTL provides both sentence and word-level\ncross-lingual representations.\nSentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation. \n\n andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14), \n\n then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n Sentence level representations are constructed in the same manner\nas XCLO except for the negative sampling where instead of collecting\nsamples from{yj}N\nj=1,j̸=ismoothed linear interpolation [31, 300] between\nsentence representations is used to construct hard negative samples.\nFor world-level representations, the contrastive loss similarity score\nis computed between a parallel sentence (x i,yi)[CLS]token represen-\ntation and other words representations. For each parallel sentence pair\ninput (xi,yi), a bag of wordsWis maintained, where all words in Ware\nconsidered positive samples and all other words in the vocabulary are\nnegative samples. For eﬃcient negative words sampling, HICTL does\nnot sample from the entire vocabulary but rather constructs a set of\nnegative words that very close to parallel sentence ( xi,yi)[CLS]token\nrepresentation.\n4.1.3 Dual-Encoder Multilingual NLU\n4.1.3.1 Language-agnostic BERT Sentence Embedding (LaBSE)\nLanguage-agnostic BERT Sentence Embedding (LaBSE) [88] is an ar-\nchitecture for training cross-lingual sentence representations which com-\nbines Masked Language Model (MLM) and Translation Language Model\n(TLM) pre-training tasks from XLM [146] with a translation ranking\ntask using bi-directional dual-encoders with additive margin softmax\nloss[283]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n \n La question est What are the requirements for maintaining insurance coverage during test drives for a vehicle being prepared for sale?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:31.588921",
              "duration": 8.993428,
              "cost": 0.0027059999999999996
            },
            "meta": {},
            "text": "Maintaining insurance coverage during test drives for a vehicle being prepared for sale is crucial to protect both the seller and potential buyers. Here are some common requirements:\n\n1. **Existing Insurance Policy**: The vehicle should be covered under the seller's existing insurance policy. It's important to inform the insurance company that the vehicle is being used for test drives during the selling process.\n\n2. **Liability Coverage**: The insurance policy should include liability coverage to protect against any accidents or damages that may occur during test drives. This coverage typically helps pay for the other party's medical expenses or property damage if the seller is found at fault.\n\n3. **Comprehensive and Collision Coverage**: Comprehensive coverage protects against non-collision incidents like theft or vandalism, while collision coverage helps pay for damages to the vehicle in case of an accident during a test drive.\n\n4. **Permission for Test Drives**: Some insurance policies may require explicit permission for test drives. Make sure to check with your insurance provider if this is a requirement.\n\n5. **Driver Requirements**: Ensure that anyone test driving the vehicle meets the driver requirements specified in your insurance policy. This may include having a valid driver's license and meeting age restrictions.\n\n6. **Notification of Changes**: Inform your insurance company about any changes in the vehicle's status, such as it being used for test drives. Failure to disclose this information could lead to coverage issues.\n\n7. **Temporary Coverage**: If necessary, you may need to obtain temporary coverage or a special policy for test drives if your existing policy does not adequately cover this activity.\n\nIt's essential to review your insurance policy and discuss any specific requirements or concerns with your insurance provider to ensure that you have appropriate coverage during test drives for a vehicle being prepared for sale.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Maintaining insurance coverage during test drives for a vehicle being prepared for sale is crucial to protect both the seller and potential buyers. Here are some common requirements:\n\n1. **Existing Insurance Policy**: The vehicle should be covered under the seller's existing insurance policy. It's important to inform the insurance company that the vehicle is being used for test drives during the selling process.\n\n2. **Liability Coverage**: The insurance policy should include liability coverage to protect against any accidents or damages that may occur during test drives. This coverage typically helps pay for the other party's medical expenses or property damage if the seller is found at fault.\n\n3. **Comprehensive and Collision Coverage**: Comprehensive coverage protects against non-collision incidents like theft or vandalism, while collision coverage helps pay for damages to the vehicle in case of an accident during a test drive.\n\n4. **Permission for Test Drives**: Some insurance policies may require explicit permission for test drives. Make sure to check with your insurance provider if this is a requirement.\n\n5. **Driver Requirements**: Ensure that anyone test driving the vehicle meets the driver requirements specified in your insurance policy. This may include having a valid driver's license and meeting age restrictions.\n\n6. **Notification of Changes**: Inform your insurance company about any changes in the vehicle's status, such as it being used for test drives. Failure to disclose this information could lead to coverage issues.\n\n7. **Temporary Coverage**: If necessary, you may need to obtain temporary coverage or a special policy for test drives if your existing policy does not adequately cover this activity.\n\nIt's essential to review your insurance policy and discuss any specific requirements or concerns with your insurance provider to ensure that you have appropriate coverage during test drives for a vehicle being prepared for sale.\n\n1. 1. The text does not mention the need for insurance during test drives.\n2. 2. There is no information about maintaining insurance coverage for test drives in the text.\n3. 3. The document lacks details on insurance requirements for test drives.\n4. 4. The text does not specify insurance obligations during test drives of a vehicle for sale.\n5. 5. There is no reference to insurance coverage during test drives in the provided text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:02.170969",
                "duration": 1.559699,
                "cost": 0.01713
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" in case a driver is not designated?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The special deductible applies to the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when no driver has been designated.\n2. This deductible is applicable to the civil liability and all accident damage Eco guarantees.\n3. The all accident damage Eco guarantee is the primary application of this deductible when it is subscribed.\n4. The specific amount of the deductible is stated in the specific conditions of the insurance contract.",
          "prompt": {
            "meta": {},
            "user": "The special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when a driver has not been designated applies to the civil liability and all accident damage Eco guarantees. It is applied primarily to the all accident damage Eco guarantee when it has been subscribed. The exact amount of the deductible is indicated in the specific conditions of the insurance contract.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:33.802416",
          "duration": 5.898059,
          "cost": 0.0108
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The special deductible applies to the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when no driver has been designated."
          },
          {
            "meta": {},
            "text": "2. This deductible is applicable to the civil liability and all accident damage Eco guarantees."
          },
          {
            "meta": {},
            "text": "3. The all accident damage Eco guarantee is the primary application of this deductible when it is subscribed."
          },
          {
            "meta": {},
            "text": "4. The specific amount of the deductible is stated in the specific conditions of the insurance contract."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 17.162503828734508,
              "Node id": "b35b0b94-73c9-4af2-9877-9a8620bd79e3"
            },
            "text": "Formules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*.\nLorsque plusieurs de ces garanties sont mises en jeu à \nl’occasion d’un même sinistre*, l’assuré* conserve à sa charge les franchises correspondantes.\nSi le responsable des dommages n’est pas une personne \ncouverte par la garantie Responsabilité civile, nous lui réclamerons le remboursement du montant de la franchise* proportionnellement à sa responsabilité."
          },
          {
            "meta": {
              "score": 16.60301274236419,
              "Node id": "5def1c68-526c-4960-8776-beb32b691964"
            },
            "text": "Cette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents."
          },
          {
            "meta": {
              "score": 16.60301274236419,
              "Node id": "eb973734-4488-41a6-9332-a6ece74b0471"
            },
            "text": "Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents. Elle s’applique en priorité sur la garantie Dommages tous accidents lorsqu’elle a été souscrite."
          },
          {
            "meta": {
              "score": 16.4954635767085,
              "Node id": "a993748c-b39f-4900-9dda-3f4175d88cae"
            },
            "text": "15.4 Franchise\n Lorsque mention en est faite sur vos Conditions particulières l’assuré* conserve à sa charge une franchise* pour tout sinistre* mettant en jeu les garanties :\n  DOMMAGES AU VÉHICULE ASSURÉ,\n  BRIS DE PARE-BRISE,\n  BRIS DE GLACES sauf pour la réparation, sans \nremplacement du pare-brise,\n  INDEMNISA TION RENFORCÉE, pour les formules T ous \nRisques Confort et T ous Risques Confort +,\n  RESPONSABILITÉ CIVILE\nFormules Tiers Essentiel et Essentiel + et formules \nTous Risques Confort et Confort + : franchise spéciale lorsqu’un conducteur novice n’a pas été désigné.\nFormules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*."
          },
          {
            "meta": {
              "score": 16.492033067240964,
              "Node id": "aa07b54c-d680-4933-be98-2bd11f305b26"
            },
            "text": "3   T ABLEAU DES FORMULES DE GARANTIES\nPagesTIERS\nECOTIERS\nESSENTIELTIERS\nESSENTIEL+TOUS \nRISQUES\nECOTOUS \nRISQUES\nCONFORTTOUS \nRISQUES\nCONFORT+\nLES GARANTIES \nResponsabilité civile/ Défense 11\nRecours 14\nAssistance aux personnes 26\nDommages corporels du conducteur Eco 17\nDommages corporels du conducteur 19\nCatastrophes naturelles 21\nÉvénements climatiques 21\nBris de pare-brise 21\nBris de glaces 21\nIncendie, Explosion,"
          },
          {
            "meta": {
              "score": 15.779940656485913,
              "Node id": "99fa1de5-ca90-442c-b171-17a55d45e6e7"
            },
            "text": "17.3  Dispositions particulières\n    FORMULE TIERS Eco ET FORMULE TOUS RISQUES Eco\nPrêt de votre véhicule (prêt de volant*) :  lorsque le véhicule \nassuré est conduit par une personne non désignée sur les Conditions particulières, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières ; sauf s’il s’agit d’un associé ou d’un préposé* du souscripteur agissant exclusivement dans le cadre de l’activité professionnelle.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite."
          },
          {
            "meta": {
              "score": 14.650357503552545,
              "Node id": "fa445917-cb6e-4b17-bd3b-b0c640523046"
            },
            "text": "To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token. The\nhidden vector of this token in the last BERT layer will be used as an\naggregate representation for the entire input sequence and usually used\nfor classiﬁcation tasks. For NLP tasks with paired sentences, BERT con-\ncatenates the sentences into one sequence with a separator token [SEP]\nin between, which serves as one way BERT uses to distinguish the two\nsentences. BERT also uses a learned segment embedding to indicate\nto which sentence a token belongs."
          },
          {
            "meta": {
              "score": 14.437926575241804,
              "Node id": "6f98382f-9b7a-475d-bbca-ac6b767401b1"
            },
            "text": "QUI EST LE BÉNÉFICIAIRE DE L’INDEMNITÉ ? \nLe propriétaire du véhicule assuré.\n DISPOSITIONS PARTICULIÈRES \n     VALEUR D’ACHA T 12 MOIS POUR LES FORMULES TIERS ESSENTIEL ET ESSENTIEL + ET LES FORMULES TOUS RISQUES CONFORT ET CONFORT +\nSi le sinistre* survient dans les 12 mois suivant la date de \npremière mise en circulation du véhicule,\n-  lorsque celui-ci est détruit (le montant des réparations \ndépasse la valeur de remplacement*),\n-  ou lorsqu’il est volé et non retrouvé,l’indemnité est égale à la valeur d’acquisition* du véhicule, \nsur présentation de la facture d’achat. GARANTIES VOL Eco ET VOL\n-  Si votre véhicule est retrouvé dans un délai de 20 jours à \ncompter de la réception de votre déclaration, vous vous engagez à en reprendre possession et l’indemnité que nous verserons, sera égale à celle indiquée ci-dessus augmentée des frais que vous aurez engagés en accord avec nous pour la récupération du véhicule."
          },
          {
            "meta": {
              "score": 14.107959740613607,
              "Node id": "f4c0c066-f6ec-43b9-97c0-980dfd0cb5bc"
            },
            "text": "FORMULES DE GARANTIES ET OPTIONS    \nTIERS  TIERS - \nVOL - \nINCENDIE  TOUS RISQUES  TOUS RISQUES \nPLUS (1) \nGARANTIES MOBILITÉ  \nAssistance au véhicule et aux \npersonnes transportées  23 ● ● ● ● \nAssistance panne  0 kilomètre /  \nrétention administrative du \npermis  de conduire  24  OPTION  OPTION   OPTION  ●  \nVéhicule de \nremplacement\n(5)(6) Niveau 1  \n25 OPTION  OPTION  OPTION   \nNiveau 2    OPTION  ● \nPanne mécanique(5)(7) 26   OPTION  ● \n \n(1) Formule de  garanties"
          },
          {
            "meta": {
              "score": 14.034930683429888,
              "Node id": "dd3263b6-52cf-48a2-8520-30914e53ed15"
            },
            "text": "44■Transformers for Machine Learning: A Deep Dive\nnumber of transformer layers L, (ii) the size of hidden representations\nH, and the number of bidirectional self-attention heads A.\nInput and output representations Given the simplicity of the\nBERT’s core architecture, just a stack of bidirectional transformer en-\ncoders, the ingenuity of BERT lies in the design of input and output\nrepresentations that can be used to train many downstream NLP tasks\nwith the same architecture. To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token."
          },
          {
            "meta": {
              "score": 0.7416761092934456,
              "Node id": "e6fa195f-abb2-4d27-8161-b48023814c1a"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other."
          },
          {
            "meta": {
              "score": 0.7395392533243254,
              "Node id": "410faf9a-e253-4228-acf1-f43e0baf1ab4"
            },
            "text": "The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers."
          },
          {
            "meta": {
              "score": 0.7390019183898983,
              "Node id": "e2e70800-ae76-45ed-9b95-67a9382671f4"
            },
            "text": "The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not."
          },
          {
            "meta": {
              "score": 0.7382597932842215,
              "Node id": "2436ee17-46ed-4ffd-8494-0c49400e40e3"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to."
          },
          {
            "meta": {
              "score": 0.7371270058930822,
              "Node id": "8e666c52-3b9f-4ce1-8c30-bb0ca634f7b6"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to."
          },
          {
            "meta": {
              "score": 0.736131446101412,
              "Node id": "b9c34789-6d83-4615-b032-030eb4446371"
            },
            "text": "Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp."
          },
          {
            "meta": {
              "score": 0.7355315852414424,
              "Node id": "d33a403a-4bad-43ce-9ee5-f3951e20e7be"
            },
            "text": "128■Transformers for Machine Learning: A Deep Dive\nwhereSiis the set of key positions that query iattends to and Z(i,Si) =\n∑\nr∈Siexp(\nqikT\nr√dk(\nis the softmax normalization term.\nWithout a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum."
          },
          {
            "meta": {
              "score": 0.7346677486814511,
              "Node id": "1ae8e2be-dc9c-4fb8-a7d4-2cd0e17aa4de"
            },
            "text": "j−logZ(i,S(r)\ni)(\nvj (5.39)\nm(r)\ni,j=⌋\n}\n∞ j /∈S(r)\ni\n105ifi=j\nlogNi,jotherwise(5.40)\nNi,j=⏐⏐⏐⏐{r′:j∈S(r′)\ni}⏐⏐⏐⏐ (5.41)\nNote that in the multi-round LSH case, Reformer modiﬁes the mask-\ning term to include (and downweight) the case where query at position\niattends to a key at position i. This case is added because, while the\nstandard transformer allows a position to attend to itself, this is unhelp-\nful when Q=K, as is the case in LSH attention."
          },
          {
            "meta": {
              "score": 0.7338556012093169,
              "Node id": "3dd0d1e4-e8e1-44f9-aaf2-c38de9c60069"
            },
            "text": "In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder. Both\ngIandgRare implemented using transformers and are trained with the\nlog-likelihood loss function,˜P(sR\ni|sI\ni), for each task.\nAs illustrated in Fig. 4.7, mUSE uses a single shared encoder sup-\nporting multiple downstream tasks. The training tasks include: QA pre-\ndiction task, NLI task, and translation ranking."
          },
          {
            "meta": {
              "score": 0.7323765233650782,
              "Node id": "88379bb7-5eea-4dbe-ac8f-f5253f787d2a"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when a driver has not been designated applies to the civil liability and all accident damage Eco guarantees. It is applied primarily to the all accident damage Eco guarantee when it has been subscribed. The exact amount of the deductible is indicated in the specific conditions of the insurance contract.",
              "prompt": {
                "meta": {},
                "user": "What is the special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" in case a driver is not designated?",
                "system": "Contexte :  Formules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*.\nLorsque plusieurs de ces garanties sont mises en jeu à \nl’occasion d’un même sinistre*, l’assuré* conserve à sa charge les franchises correspondantes.\nSi le responsable des dommages n’est pas une personne \ncouverte par la garantie Responsabilité civile, nous lui réclamerons le remboursement du montant de la franchise* proportionnellement à sa responsabilité. \n\n Cette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents. \n\n Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents. Elle s’applique en priorité sur la garantie Dommages tous accidents lorsqu’elle a été souscrite. \n\n 15.4 Franchise\n Lorsque mention en est faite sur vos Conditions particulières l’assuré* conserve à sa charge une franchise* pour tout sinistre* mettant en jeu les garanties :\n  DOMMAGES AU VÉHICULE ASSURÉ,\n  BRIS DE PARE-BRISE,\n  BRIS DE GLACES sauf pour la réparation, sans \nremplacement du pare-brise,\n  INDEMNISA TION RENFORCÉE, pour les formules T ous \nRisques Confort et T ous Risques Confort +,\n  RESPONSABILITÉ CIVILE\nFormules Tiers Essentiel et Essentiel + et formules \nTous Risques Confort et Confort + : franchise spéciale lorsqu’un conducteur novice n’a pas été désigné.\nFormules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*. \n\n 3   T ABLEAU DES FORMULES DE GARANTIES\nPagesTIERS\nECOTIERS\nESSENTIELTIERS\nESSENTIEL+TOUS \nRISQUES\nECOTOUS \nRISQUES\nCONFORTTOUS \nRISQUES\nCONFORT+\nLES GARANTIES \nResponsabilité civile/ Défense 11\nRecours 14\nAssistance aux personnes 26\nDommages corporels du conducteur Eco 17\nDommages corporels du conducteur 19\nCatastrophes naturelles 21\nÉvénements climatiques 21\nBris de pare-brise 21\nBris de glaces 21\nIncendie, Explosion, \n\n 17.3  Dispositions particulières\n    FORMULE TIERS Eco ET FORMULE TOUS RISQUES Eco\nPrêt de votre véhicule (prêt de volant*) :  lorsque le véhicule \nassuré est conduit par une personne non désignée sur les Conditions particulières, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières ; sauf s’il s’agit d’un associé ou d’un préposé* du souscripteur agissant exclusivement dans le cadre de l’activité professionnelle.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite. \n\n To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token. The\nhidden vector of this token in the last BERT layer will be used as an\naggregate representation for the entire input sequence and usually used\nfor classiﬁcation tasks. For NLP tasks with paired sentences, BERT con-\ncatenates the sentences into one sequence with a separator token [SEP]\nin between, which serves as one way BERT uses to distinguish the two\nsentences. BERT also uses a learned segment embedding to indicate\nto which sentence a token belongs. \n\n QUI EST LE BÉNÉFICIAIRE DE L’INDEMNITÉ ? \nLe propriétaire du véhicule assuré.\n DISPOSITIONS PARTICULIÈRES \n     VALEUR D’ACHA T 12 MOIS POUR LES FORMULES TIERS ESSENTIEL ET ESSENTIEL + ET LES FORMULES TOUS RISQUES CONFORT ET CONFORT +\nSi le sinistre* survient dans les 12 mois suivant la date de \npremière mise en circulation du véhicule,\n-  lorsque celui-ci est détruit (le montant des réparations \ndépasse la valeur de remplacement*),\n-  ou lorsqu’il est volé et non retrouvé,l’indemnité est égale à la valeur d’acquisition* du véhicule, \nsur présentation de la facture d’achat. GARANTIES VOL Eco ET VOL\n-  Si votre véhicule est retrouvé dans un délai de 20 jours à \ncompter de la réception de votre déclaration, vous vous engagez à en reprendre possession et l’indemnité que nous verserons, sera égale à celle indiquée ci-dessus augmentée des frais que vous aurez engagés en accord avec nous pour la récupération du véhicule. \n\n FORMULES DE GARANTIES ET OPTIONS    \nTIERS  TIERS - \nVOL - \nINCENDIE  TOUS RISQUES  TOUS RISQUES \nPLUS (1) \nGARANTIES MOBILITÉ  \nAssistance au véhicule et aux \npersonnes transportées  23 ● ● ● ● \nAssistance panne  0 kilomètre /  \nrétention administrative du \npermis  de conduire  24  OPTION  OPTION   OPTION  ●  \nVéhicule de \nremplacement\n(5)(6) Niveau 1  \n25 OPTION  OPTION  OPTION   \nNiveau 2    OPTION  ● \nPanne mécanique(5)(7) 26   OPTION  ● \n \n(1) Formule de  garanties \n\n 44■Transformers for Machine Learning: A Deep Dive\nnumber of transformer layers L, (ii) the size of hidden representations\nH, and the number of bidirectional self-attention heads A.\nInput and output representations Given the simplicity of the\nBERT’s core architecture, just a stack of bidirectional transformer en-\ncoders, the ingenuity of BERT lies in the design of input and output\nrepresentations that can be used to train many downstream NLP tasks\nwith the same architecture. To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token. \n\n Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other. \n\n The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers. \n\n The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n 128■Transformers for Machine Learning: A Deep Dive\nwhereSiis the set of key positions that query iattends to and Z(i,Si) =\n∑\nr∈Siexp(\nqikT\nr√dk(\nis the softmax normalization term.\nWithout a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum. \n\n j−logZ(i,S(r)\ni)(\nvj (5.39)\nm(r)\ni,j=⌋\n}\n∞ j /∈S(r)\ni\n105ifi=j\nlogNi,jotherwise(5.40)\nNi,j=⏐⏐⏐⏐{r′:j∈S(r′)\ni}⏐⏐⏐⏐ (5.41)\nNote that in the multi-round LSH case, Reformer modiﬁes the mask-\ning term to include (and downweight) the case where query at position\niattends to a key at position i. This case is added because, while the\nstandard transformer allows a position to attend to itself, this is unhelp-\nful when Q=K, as is the case in LSH attention. \n\n In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder. Both\ngIandgRare implemented using transformers and are trained with the\nlog-likelihood loss function,˜P(sR\ni|sI\ni), for each task.\nAs illustrated in Fig. 4.7, mUSE uses a single shared encoder sup-\nporting multiple downstream tasks. The training tasks include: QA pre-\ndiction task, NLI task, and translation ranking. \n\n Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. \n\n \n La question est What is the special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" in case a driver is not designated?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:37.366664",
              "duration": 5.773368,
              "cost": 0.13107
            },
            "meta": {},
            "text": "The special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when a driver has not been designated applies to the civil liability and all accident damage Eco guarantees. It is applied primarily to the all accident damage Eco guarantee when it has been subscribed. The exact amount of the deductible is indicated in the specific conditions of the insurance contract.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when a driver has not been designated applies to the civil liability and all accident damage Eco guarantees. It is applied primarily to the all accident damage Eco guarantee when it has been subscribed. The exact amount of the deductible is indicated in the specific conditions of the insurance contract.\n\n1. 1. The special deductible applies to the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when no driver has been designated.\n2. 2. This deductible is applicable to the civil liability and all accident damage Eco guarantees.\n3. 3. The all accident damage Eco guarantee is the primary application of this deductible when it is subscribed.\n4. 4. The specific amount of the deductible is stated in the specific conditions of the insurance contract.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:03.872613",
                "duration": 1.698966,
                "cost": 0.00879
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the \"Formules Tiers Eco\" and \"Tous Risques Eco\" insurance formulas, if a driver is not designated and multiple guarantees are involved in a single claim, the insured will bear the corresponding deductibles. Specifically, the special deductible applies to the following guarantees:\n\n- Mechanical breakdown\n- Damage to bicycles\n\nThis deductible affects the Liability and All Accidents Damage Eco guarantees. It is prioritized on the All Accidents Damage Eco guarantee if it was subscribed to.",
              "prompt": {
                "meta": {},
                "user": "What is the special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" in case a driver is not designated?",
                "system": "Contexte :  Formules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*.\nLorsque plusieurs de ces garanties sont mises en jeu à \nl’occasion d’un même sinistre*, l’assuré* conserve à sa charge les franchises correspondantes.\nSi le responsable des dommages n’est pas une personne \ncouverte par la garantie Responsabilité civile, nous lui réclamerons le remboursement du montant de la franchise* proportionnellement à sa responsabilité. \n\n Cette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents. \n\n Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite.\n    FORMULES TIERS ESSENTIEL ET ESSENTIEL  + ET FORMULES TOUS RISQUES CONFORT ET CONFORT +\nConduite occasionnelle du véhicule par un conducteur novice non désigné :  lorsque le véhicule assuré est occasionnellement \nconduit par un conducteur novice et que vous ne l’avez pas préalablement désigné, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents. Elle s’applique en priorité sur la garantie Dommages tous accidents lorsqu’elle a été souscrite. \n\n 15.4 Franchise\n Lorsque mention en est faite sur vos Conditions particulières l’assuré* conserve à sa charge une franchise* pour tout sinistre* mettant en jeu les garanties :\n  DOMMAGES AU VÉHICULE ASSURÉ,\n  BRIS DE PARE-BRISE,\n  BRIS DE GLACES sauf pour la réparation, sans \nremplacement du pare-brise,\n  INDEMNISA TION RENFORCÉE, pour les formules T ous \nRisques Confort et T ous Risques Confort +,\n  RESPONSABILITÉ CIVILE\nFormules Tiers Essentiel et Essentiel + et formules \nTous Risques Confort et Confort + : franchise spéciale lorsqu’un conducteur novice n’a pas été désigné.\nFormules Tiers Eco et Tous Risques Eco : franchise \nspéciale lorsqu’un conducteur n’a pas été désigné,\n  PANNE MÉCANIQUE, \n  DOMMAGES VÉLO*. \n\n 3   T ABLEAU DES FORMULES DE GARANTIES\nPagesTIERS\nECOTIERS\nESSENTIELTIERS\nESSENTIEL+TOUS \nRISQUES\nECOTOUS \nRISQUES\nCONFORTTOUS \nRISQUES\nCONFORT+\nLES GARANTIES \nResponsabilité civile/ Défense 11\nRecours 14\nAssistance aux personnes 26\nDommages corporels du conducteur Eco 17\nDommages corporels du conducteur 19\nCatastrophes naturelles 21\nÉvénements climatiques 21\nBris de pare-brise 21\nBris de glaces 21\nIncendie, Explosion, \n\n 17.3  Dispositions particulières\n    FORMULE TIERS Eco ET FORMULE TOUS RISQUES Eco\nPrêt de votre véhicule (prêt de volant*) :  lorsque le véhicule \nassuré est conduit par une personne non désignée sur les Conditions particulières, l’assuré* supporte, en cas de sinistre* dont la responsabilité n’est pas entièrement imputable à  un tiers*, une franchise dont le montant est indiqué sur vos Conditions particulières ; sauf s’il s’agit d’un associé ou d’un préposé* du souscripteur agissant exclusivement dans le cadre de l’activité professionnelle.\nCette franchise affecte les garanties Responsabilité civile et \nDommages tous accidents Eco. Elle s’applique en priorité sur la garantie Dommages tous accidents Eco lorsqu’elle a été souscrite. \n\n To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token. The\nhidden vector of this token in the last BERT layer will be used as an\naggregate representation for the entire input sequence and usually used\nfor classiﬁcation tasks. For NLP tasks with paired sentences, BERT con-\ncatenates the sentences into one sequence with a separator token [SEP]\nin between, which serves as one way BERT uses to distinguish the two\nsentences. BERT also uses a learned segment embedding to indicate\nto which sentence a token belongs. \n\n QUI EST LE BÉNÉFICIAIRE DE L’INDEMNITÉ ? \nLe propriétaire du véhicule assuré.\n DISPOSITIONS PARTICULIÈRES \n     VALEUR D’ACHA T 12 MOIS POUR LES FORMULES TIERS ESSENTIEL ET ESSENTIEL + ET LES FORMULES TOUS RISQUES CONFORT ET CONFORT +\nSi le sinistre* survient dans les 12 mois suivant la date de \npremière mise en circulation du véhicule,\n-  lorsque celui-ci est détruit (le montant des réparations \ndépasse la valeur de remplacement*),\n-  ou lorsqu’il est volé et non retrouvé,l’indemnité est égale à la valeur d’acquisition* du véhicule, \nsur présentation de la facture d’achat. GARANTIES VOL Eco ET VOL\n-  Si votre véhicule est retrouvé dans un délai de 20 jours à \ncompter de la réception de votre déclaration, vous vous engagez à en reprendre possession et l’indemnité que nous verserons, sera égale à celle indiquée ci-dessus augmentée des frais que vous aurez engagés en accord avec nous pour la récupération du véhicule. \n\n FORMULES DE GARANTIES ET OPTIONS    \nTIERS  TIERS - \nVOL - \nINCENDIE  TOUS RISQUES  TOUS RISQUES \nPLUS (1) \nGARANTIES MOBILITÉ  \nAssistance au véhicule et aux \npersonnes transportées  23 ● ● ● ● \nAssistance panne  0 kilomètre /  \nrétention administrative du \npermis  de conduire  24  OPTION  OPTION   OPTION  ●  \nVéhicule de \nremplacement\n(5)(6) Niveau 1  \n25 OPTION  OPTION  OPTION   \nNiveau 2    OPTION  ● \nPanne mécanique(5)(7) 26   OPTION  ● \n \n(1) Formule de  garanties \n\n 44■Transformers for Machine Learning: A Deep Dive\nnumber of transformer layers L, (ii) the size of hidden representations\nH, and the number of bidirectional self-attention heads A.\nInput and output representations Given the simplicity of the\nBERT’s core architecture, just a stack of bidirectional transformer en-\ncoders, the ingenuity of BERT lies in the design of input and output\nrepresentations that can be used to train many downstream NLP tasks\nwith the same architecture. To that end, BERT’s input is designed to\ndistinctly represent NLP downstream tasks involving a single or a pair\nof sentences using the same input representation design. Most major\nNLP downstream tasks can be covered using a single sentence (e.g., text\nclassiﬁcation, sequence tagging, summarization, etc.) or paired sentences\n(e.g., question answering, natural language inference, etc.).\nFor any input sequence, BERT preﬁxes a special [CLS]token. \n\n Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other. \n\n The choice for\nthe root point is such that for it to be insigniﬁcantly diﬀerent from\nthe input xfor whichfgenerates a diﬀerent prediction [187].\nf(x) =f(ˆx) +∇ˆxf·(x−ˆx) +ϵ (7.1)\nf(x) =N\ni∂f\n∂xi( ˆxi)·(xi−ˆxi) +ϵ (7.2)\nwhereϵsums up all second order and higher terms in the Taylor\nexpansion. The relevance score for the inputs can be derived from\nthe above equation as\nri=∂f\n∂xi( ˆxi)·(xi−ˆxi) (7.3)\nFor a deep network with multiple layers, the Deep Taylor Decom-\nposition assumes decomposition of the relevance scores, starting\nfrom the output to the input through the intermediate layers. \n\n The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n 128■Transformers for Machine Learning: A Deep Dive\nwhereSiis the set of key positions that query iattends to and Z(i,Si) =\n∑\nr∈Siexp(\nqikT\nr√dk(\nis the softmax normalization term.\nWithout a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum. \n\n j−logZ(i,S(r)\ni)(\nvj (5.39)\nm(r)\ni,j=⌋\n}\n∞ j /∈S(r)\ni\n105ifi=j\nlogNi,jotherwise(5.40)\nNi,j=⏐⏐⏐⏐{r′:j∈S(r′)\ni}⏐⏐⏐⏐ (5.41)\nNote that in the multi-round LSH case, Reformer modiﬁes the mask-\ning term to include (and downweight) the case where query at position\niattends to a key at position i. This case is added because, while the\nstandard transformer allows a position to attend to itself, this is unhelp-\nful when Q=K, as is the case in LSH attention. \n\n In other words, the conditional probability P(sR\ni|sI\ni)\nshould be deﬁned as:\nP(\nsR\ni|sI\ni)\n=eφ(sI\ni,sR\ni)\n∑\nsR\nj∈SReφ(sR\ni,sR\nj)\nφ(\nsI\ni,sR\nj)\n=gI(\nsI\ni)⊤gR(\nsR\nj)(4.17)\nwheregIis the input sentence encoding function and gRis response\nsentence encoding function, gIandgRmake up the dual-encoder. Both\ngIandgRare implemented using transformers and are trained with the\nlog-likelihood loss function,˜P(sR\ni|sI\ni), for each task.\nAs illustrated in Fig. 4.7, mUSE uses a single shared encoder sup-\nporting multiple downstream tasks. The training tasks include: QA pre-\ndiction task, NLI task, and translation ranking. \n\n Pre-trained and Application-Speciﬁc Transformers ■179\nFigure 6.6 Decision Transformer architecture for oﬄine RL. The ﬁrst two\ntime steps are shown. ER,ES, andEArepresent the combined embed-\nding layer positional encoding, and layer norm for the returns-to-go,\nstates, and actions, respectively. The layer after GPT-2 contains three\nlinear submodules for predicting the next element of the sequence. Next\nstate, action, and return-to-go are predicted using diﬀerent submodules.\nThese ﬁxed trajectories are used for autoregressive training. The pre-\nviousKstepsarepassedtotheDecisionTransformer.Eachstepcontains\nthree “tokens”: the state, action, and return-to-go. Each type of token\nhas its own linear embedding layer, which is followed up by a Layer-\nNorm. \n\n \n La question est What is the special deductible for the \"Formules Tiers Eco\" and \"Tous Risques Eco\" in case a driver is not designated?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:43.672588",
              "duration": 6.302783,
              "cost": 0.0022555
            },
            "meta": {},
            "text": "In the \"Formules Tiers Eco\" and \"Tous Risques Eco\" insurance formulas, if a driver is not designated and multiple guarantees are involved in a single claim, the insured will bear the corresponding deductibles. Specifically, the special deductible applies to the following guarantees:\n\n- Mechanical breakdown\n- Damage to bicycles\n\nThis deductible affects the Liability and All Accidents Damage Eco guarantees. It is prioritized on the All Accidents Damage Eco guarantee if it was subscribed to.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the \"Formules Tiers Eco\" and \"Tous Risques Eco\" insurance formulas, if a driver is not designated and multiple guarantees are involved in a single claim, the insured will bear the corresponding deductibles. Specifically, the special deductible applies to the following guarantees:\n\n- Mechanical breakdown\n- Damage to bicycles\n\nThis deductible affects the Liability and All Accidents Damage Eco guarantees. It is prioritized on the All Accidents Damage Eco guarantee if it was subscribed to.\n\n1. 1. The special deductible applies to the \"Formules Tiers Eco\" and \"Tous Risques Eco\" when no driver has been designated.\n2. 2. This deductible is applicable to the civil liability and all accident damage Eco guarantees.\n3. 3. The all accident damage Eco guarantee is the primary application of this deductible when it is subscribed.\n4. 4. The specific amount of the deductible is stated in the specific conditions of the insurance contract.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:05.075485",
                "duration": 1.199345,
                "cost": 0.009300000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3'] - Not validated: ['4']",
              "human": 1.0,
              "auto": 0.75
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the guarantee included in all four formulas of guarantees?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text lacks information on the guarantee included in all four formulas.\n2. There is no mention of the guarantee in the four formulas.\n3. The four formulas of guarantees are not explained in the text.\n4. The text does not detail the guarantee present in each of the four formulas.\n5. The guarantee aspect of the four formulas is missing from the text.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the guarantee included in all four formulas of guarantees.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:39.486727",
          "duration": 5.681236,
          "cost": 0.00846
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text lacks information on the guarantee included in all four formulas."
          },
          {
            "meta": {},
            "text": "2. There is no mention of the guarantee in the four formulas."
          },
          {
            "meta": {},
            "text": "3. The four formulas of guarantees are not explained in the text."
          },
          {
            "meta": {},
            "text": "4. The text does not detail the guarantee present in each of the four formulas."
          },
          {
            "meta": {},
            "text": "5. The guarantee aspect of the four formulas is missing from the text."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 22.070553535797863,
              "Node id": "e4c82045-0596-49c0-9e23-cde6f8925da1"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to."
          },
          {
            "meta": {
              "score": 19.439614742107807,
              "Node id": "891a8f0c-35eb-4421-bed5-80d23712a55c"
            },
            "text": "148■Transformers for Machine Learning: A Deep Dive\nlargest component of 5.75: p(x) = (p1,...,pN), call itpj, and is used to\ncomputetheupdatedtokenrepresentation.Thus,thoughtheparameters\nhave increased four times because of four feed-forward networks, hard\nrouting guarantees that the computational cost remains the same. Thus\nthe switching feed-forward network enables the scaling of experts to any\nnumber (bound by memory capacity) without increasing computational\ncost and not needing any transfer helps in the sharding process.\nModel parallelism shards the model across the devices (cores/ma-\nchines) while consuming the same data in the batch, resulting in a large\nmodelwithslowerprocessingduetosequentialﬂow;thebottleneckintro-\nduced by communication. On the other hand, data parallelism keeps the\nmodel weights constant and shards the data across, allowing improved\ncomputational speed but a lower capacity model due to size."
          },
          {
            "meta": {
              "score": 16.686752023732446,
              "Node id": "66ceef81-1a75-4c0b-8dab-6c01b9a153df"
            },
            "text": "As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2."
          },
          {
            "meta": {
              "score": 15.44966354119203,
              "Node id": "364c75ef-106d-4b6d-9f5c-ea3206aa0af2"
            },
            "text": "As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block."
          },
          {
            "meta": {
              "score": 14.474158347772747,
              "Node id": "a0250264-acb3-4a24-8b27-afb790524d1d"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 14.293655578727721,
              "Node id": "d4667095-1eda-47ce-8a4e-4d81a81ec024"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 14.117599347118563,
              "Node id": "8a83916f-aaaa-418b-a13a-66ee35c49b0d"
            },
            "text": "Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size."
          },
          {
            "meta": {
              "score": 14.117599347118563,
              "Node id": "27aafed6-2dab-4ddb-9087-5dd4c50608b2"
            },
            "text": "3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]."
          },
          {
            "meta": {
              "score": 13.945827346887985,
              "Node id": "65219b96-2ac5-4f46-8dd3-3ecb1b24a90b"
            },
            "text": "2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training."
          },
          {
            "meta": {
              "score": 13.861499359788965,
              "Node id": "379bf1b9-6d81-4afd-ad55-dfb5ad3a955c"
            },
            "text": "), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al."
          },
          {
            "meta": {
              "score": 0.7613936471246465,
              "Node id": "78b973d1-bc09-46b8-b151-630ed535c4a7"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 0.7501149575935632,
              "Node id": "d380ad8c-1a90-4606-a113-6ff7104777c0"
            },
            "text": "As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]."
          },
          {
            "meta": {
              "score": 0.749057348357148,
              "Node id": "20010ae9-c968-4c58-8312-204c37d847a5"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 0.7490453431509362,
              "Node id": "7dad7d13-821f-4bbf-901b-ae5ef729a61a"
            },
            "text": "2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training."
          },
          {
            "meta": {
              "score": 0.7470910809469912,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7457555023436552,
              "Node id": "c896d16f-d55c-44b8-ab5a-a80d51f0ccca"
            },
            "text": "188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe."
          },
          {
            "meta": {
              "score": 0.7445253535181555,
              "Node id": "0dca327f-4762-4975-82a4-0a9a1a50e6cf"
            },
            "text": "3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]."
          },
          {
            "meta": {
              "score": 0.7440386231356076,
              "Node id": "da38881c-51f6-4a28-a672-39fe549188bf"
            },
            "text": "found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token."
          },
          {
            "meta": {
              "score": 0.7432208072299915,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7424346393068281,
              "Node id": "2e5e2d89-a6b2-42ca-98a7-b0ab51c9df32"
            },
            "text": "), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the guarantee included in all four formulas of guarantees.",
              "prompt": {
                "meta": {},
                "user": "What is the guarantee included in all four formulas of guarantees?",
                "system": "Contexte :  Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n 148■Transformers for Machine Learning: A Deep Dive\nlargest component of 5.75: p(x) = (p1,...,pN), call itpj, and is used to\ncomputetheupdatedtokenrepresentation.Thus,thoughtheparameters\nhave increased four times because of four feed-forward networks, hard\nrouting guarantees that the computational cost remains the same. Thus\nthe switching feed-forward network enables the scaling of experts to any\nnumber (bound by memory capacity) without increasing computational\ncost and not needing any transfer helps in the sharding process.\nModel parallelism shards the model across the devices (cores/ma-\nchines) while consuming the same data in the batch, resulting in a large\nmodelwithslowerprocessingduetosequentialﬂow;thebottleneckintro-\nduced by communication. On the other hand, data parallelism keeps the\nmodel weights constant and shards the data across, allowing improved\ncomputational speed but a lower capacity model due to size. \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n \n La question est What is the guarantee included in all four formulas of guarantees?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:45.599794",
              "duration": 1.922298,
              "cost": 0.12764999999999999
            },
            "meta": {},
            "text": "The text does not provide information on the guarantee included in all four formulas of guarantees.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the guarantee included in all four formulas of guarantees.\n\n1. 1. The text lacks information on the guarantee included in all four formulas.\n2. 2. There is no mention of the guarantee in the four formulas.\n3. 3. The four formulas of guarantees are not explained in the text.\n4. 4. The text does not detail the guarantee present in each of the four formulas.\n5. 5. The guarantee aspect of the four formulas is missing from the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:06.146554",
                "duration": 1.068904,
                "cost": 0.006690000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '4', '5'] - Not validated: ['2', '3']",
              "human": 1.0,
              "auto": 0.6
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The guarantee included in all four formulas of guarantees is the assurance of trustworthiness in the transformer-based models. This trustworthiness is assessed through criteria such as satisfactory testing, experience, safety, and ethics, ensuring that the models can perform reliably, consistently, and ethically in various applications without compromising societal well-being.",
              "prompt": {
                "meta": {},
                "user": "What is the guarantee included in all four formulas of guarantees?",
                "system": "Contexte :  Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n 148■Transformers for Machine Learning: A Deep Dive\nlargest component of 5.75: p(x) = (p1,...,pN), call itpj, and is used to\ncomputetheupdatedtokenrepresentation.Thus,thoughtheparameters\nhave increased four times because of four feed-forward networks, hard\nrouting guarantees that the computational cost remains the same. Thus\nthe switching feed-forward network enables the scaling of experts to any\nnumber (bound by memory capacity) without increasing computational\ncost and not needing any transfer helps in the sharding process.\nModel parallelism shards the model across the devices (cores/ma-\nchines) while consuming the same data in the batch, resulting in a large\nmodelwithslowerprocessingduetosequentialﬂow;thebottleneckintro-\nduced by communication. On the other hand, data parallelism keeps the\nmodel weights constant and shards the data across, allowing improved\ncomputational speed but a lower capacity model due to size. \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Table 4.1 [74] provides a summary of\nthe impactful multilingual transformer models available at the time of\npublishing this book.\n4.1.1 Basic Multilingual Transformer\nMultilingual transformer models are typically based on the mBERT ar-\nchitecture [72]. In this section, we describe the basic components of a\nmultilingual transformer architecture.\nInput Layer A series of tokens is provided as input to the multilin-\ngual transformer. The token input is derived from a one-shot represen-\ntation of a limited vocabulary, often a subword vocabulary. Typically,\nthis vocabulary is learned by concatenating monolingual data from sev-\neral languages using algorithms such as BPE [223], WordPiece [273], or\nSentencePiece [144]. To guarantee that diverse languages and scripts are\nwell represented in the vocabulary, data may be sampled using exponen-\ntial weighted smoothing or distinct vocabularies for clusters of languages\ncan be learned [55] by dividing the vocabulary size. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n 2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. \n\n 3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5]. \n\n found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n ), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. \n\n \n La question est What is the guarantee included in all four formulas of guarantees?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:47.887052",
              "duration": 2.280528,
              "cost": 0.0022065
            },
            "meta": {},
            "text": "The guarantee included in all four formulas of guarantees is the assurance of trustworthiness in the transformer-based models. This trustworthiness is assessed through criteria such as satisfactory testing, experience, safety, and ethics, ensuring that the models can perform reliably, consistently, and ethically in various applications without compromising societal well-being.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The guarantee included in all four formulas of guarantees is the assurance of trustworthiness in the transformer-based models. This trustworthiness is assessed through criteria such as satisfactory testing, experience, safety, and ethics, ensuring that the models can perform reliably, consistently, and ethically in various applications without compromising societal well-being.\n\n1. 1. The text lacks information on the guarantee included in all four formulas.\n2. 2. There is no mention of the guarantee in the four formulas.\n3. 3. The four formulas of guarantees are not explained in the text.\n4. 4. The text does not detail the guarantee present in each of the four formulas.\n5. 5. The guarantee aspect of the four formulas is missing from the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:06.858689",
                "duration": 0.70945,
                "cost": 0.00768
              },
              "meta": {},
              "text": "Validated: [] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The default coefficient used in premium calculation for the insured is 1.\n2. Article 1 mentions the use of a default coefficient in premium calculation.\n3. The insured's premium is calculated using a default coefficient of 1.\n4. The coefficient of 1 is the standard used in calculating the insured's premium.\n5. The premium for the insured is calculated using a coefficient, which by default is 1.",
          "prompt": {
            "meta": {},
            "user": "The default coefficient used in the calculation of the premium for the insured, as mentioned in Article 1, is 1.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:45.226631",
          "duration": 5.736658,
          "cost": 0.00936
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The default coefficient used in premium calculation for the insured is 1."
          },
          {
            "meta": {},
            "text": "2. Article 1 mentions the use of a default coefficient in premium calculation."
          },
          {
            "meta": {},
            "text": "3. The insured's premium is calculated using a default coefficient of 1."
          },
          {
            "meta": {},
            "text": "4. The coefficient of 1 is the standard used in calculating the insured's premium."
          },
          {
            "meta": {},
            "text": "5. The premium for the insured is calculated using a coefficient, which by default is 1."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.363360681059351,
              "Node id": "6c1e5200-73da-4193-8757-01b83135ddbf"
            },
            "text": "5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3."
          },
          {
            "meta": {
              "score": 11.177735716734475,
              "Node id": "cce42fe8-b707-440a-ad57-101b96f414c9"
            },
            "text": "Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1."
          },
          {
            "meta": {
              "score": 10.899914905819195,
              "Node id": "081c34f9-379d-47ad-9023-8d4470edc16a"
            },
            "text": "5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3.\nArticle 1 : réduction et majoration des primes\nLors de chaque échéance annuelle du contrat, la prime due par l’assuré  \nest déterminée en multipliant le montant de la prime de référence, telle \nqu’elle est définie à l’article 2, par un coefficient dit « coefficient de \nréduction-majoration » fixé conformément aux articles 4 et 5 suivants."
          },
          {
            "meta": {
              "score": 10.612289862073172,
              "Node id": "b974317b-1b45-4375-bf11-d612f0720bcc"
            },
            "text": "6819.  VOTRE COEFFICIENT BONUS/MALUS\n 19.1  Signification du coefficient bonus/\nmalus\nC’est l’instrument de mesure de votre Bonus ou de votre \nMalus.\n      Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler."
          },
          {
            "meta": {
              "score": 10.320155520953136,
              "Node id": "18176ec0-21d8-4c48-b206-53d6f789e6c4"
            },
            "text": "Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?"
          },
          {
            "meta": {
              "score": 10.171262766741577,
              "Node id": "2c413731-d4a4-4cc8-899a-adbe75d04ccc"
            },
            "text": "Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1.\n     si la responsabilité de l’assuré* n’est que partiellement \nengagée, la majoration est réduite de moitié : le coefficient est multiplié par 1,125."
          },
          {
            "meta": {
              "score": 9.954572889443767,
              "Node id": "1ea77042-0123-4d87-8cbd-abccfffe162d"
            },
            "text": "Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur)."
          },
          {
            "meta": {
              "score": 9.838400596860861,
              "Node id": "a8c33880-609b-489e-95ae-2548027cd290"
            },
            "text": "Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours.\nLe coefficient d’origine, c’est-à-dire celui affecté à un \nconducteur lors de sa première année d’assurance*, est égal à 1."
          },
          {
            "meta": {
              "score": 9.781325370958974,
              "Node id": "0019db6a-f644-4a61-b7d9-cde787933643"
            },
            "text": "Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours."
          },
          {
            "meta": {
              "score": 9.765429613566202,
              "Node id": "423a2996-eb32-4599-9e2f-c511c2e72ee0"
            },
            "text": "Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991."
          },
          {
            "meta": {
              "score": 0.7458154017002883,
              "Node id": "c1f0536b-7f70-4b41-a217-33f4231f7699"
            },
            "text": "Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages."
          },
          {
            "meta": {
              "score": 0.7424101608268857,
              "Node id": "50610582-b112-4f31-a319-d19106cdf74d"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used."
          },
          {
            "meta": {
              "score": 0.7417334546132411,
              "Node id": "6739a344-7e2c-49a9-93de-145dc7dfdd1c"
            },
            "text": "i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),"
          },
          {
            "meta": {
              "score": 0.7417202365247069,
              "Node id": "845269f1-acb6-47f7-908c-905f8cee14f1"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 0.7406039246078336,
              "Node id": "bcb8550a-88e6-43a8-8207-35352fdef7c1"
            },
            "text": "Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7."
          },
          {
            "meta": {
              "score": 0.73623977389054,
              "Node id": "0e0fbb72-d00b-4ffa-8e2e-0167c0498623"
            },
            "text": "It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al."
          },
          {
            "meta": {
              "score": 0.735581550702049,
              "Node id": "87414ab0-3426-4230-906b-f033b27cba48"
            },
            "text": "The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper."
          },
          {
            "meta": {
              "score": 0.7355548415683821,
              "Node id": "83370bec-582f-43dc-ad30-4499b2f357ea"
            },
            "text": "This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. Therefore, low-resource languages will be over sampled while\nhigh-resource languages will be under sampled. Such process guaran-\ntees the use of a reasonable set of vocabulary (while training the Word-\nPiece [220] or SentencePiece model [144]) in the low resource languages.\nTable 4.1 summarizes the number of languages supported by diﬀerent\nmLMs and the total vocabulary used by them. Typically, mLMs which\nsupport more languages have a larger vocabulary."
          },
          {
            "meta": {
              "score": 0.7354886967150064,
              "Node id": "afbc28c8-5b99-4c09-8cfb-d756e77b7751"
            },
            "text": "(5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω."
          },
          {
            "meta": {
              "score": 0.7349130002238344,
              "Node id": "3717f92d-e7f5-4240-a356-a4166eeaafca"
            },
            "text": "The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The default coefficient used in the calculation of the premium for the insured, as mentioned in Article 1, is 1.",
              "prompt": {
                "meta": {},
                "user": "What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?",
                "system": "Contexte :  5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3. \n\n Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1. \n\n 5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3.\nArticle 1 : réduction et majoration des primes\nLors de chaque échéance annuelle du contrat, la prime due par l’assuré  \nest déterminée en multipliant le montant de la prime de référence, telle \nqu’elle est définie à l’article 2, par un coefficient dit « coefficient de \nréduction-majoration » fixé conformément aux articles 4 et 5 suivants. \n\n 6819.  VOTRE COEFFICIENT BONUS/MALUS\n 19.1  Signification du coefficient bonus/\nmalus\nC’est l’instrument de mesure de votre Bonus ou de votre \nMalus.\n      Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler. \n\n Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ? \n\n Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1.\n     si la responsabilité de l’assuré* n’est que partiellement \nengagée, la majoration est réduite de moitié : le coefficient est multiplié par 1,125. \n\n Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur). \n\n Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours.\nLe coefficient d’origine, c’est-à-dire celui affecté à un \nconducteur lors de sa première année d’assurance*, est égal à 1. \n\n Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours. \n\n Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991. \n\n Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. Therefore, low-resource languages will be over sampled while\nhigh-resource languages will be under sampled. Such process guaran-\ntees the use of a reasonable set of vocabulary (while training the Word-\nPiece [220] or SentencePiece model [144]) in the low resource languages.\nTable 4.1 summarizes the number of languages supported by diﬀerent\nmLMs and the total vocabulary used by them. Typically, mLMs which\nsupport more languages have a larger vocabulary. \n\n (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω. \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n \n La question est What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:50.416416",
              "duration": 2.524335,
              "cost": 0.13113
            },
            "meta": {},
            "text": "The default coefficient used in the calculation of the premium for the insured, as mentioned in Article 1, is 1.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The default coefficient used in the calculation of the premium for the insured, as mentioned in Article 1, is 1.\n\n1. 1. The default coefficient used in premium calculation for the insured is 1.\n2. 2. Article 1 mentions the use of a default coefficient in premium calculation.\n3. 3. The insured's premium is calculated using a default coefficient of 1.\n4. 4. The coefficient of 1 is the standard used in calculating the insured's premium.\n5. 5. The premium for the insured is calculated using a coefficient, which by default is 1.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:08.381340",
                "duration": 1.519569,
                "cost": 0.007620000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The default coefficient used in the calculation of the premium for the insured as mentioned in Article 1 is 1. This coefficient is applied to the prime de référence to determine the premium due by the insured at each annual renewal of the contract.",
              "prompt": {
                "meta": {},
                "user": "What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?",
                "system": "Contexte :  5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3. \n\n Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1. \n\n 5112 •  LE CADRE RÉGLEMENTAIRE  \nDE VOTRE CONTRAT \n12.1  LA CLAUSE DE RÉDUCTION - MAJORATION  \n(BONUS-MALUS)\nL’annexe de l’article A.121-1 du Code des assurances, dont le texte est \nreproduit ci-dessous concerne les modalités de calcul de votre coefficient de \nréduction-majoration (Bonus-Malus) applicable à votre prime d’assurance.\nCette clause ne s’applique pas aux véhicules dont la cylindrée est inférieure \nà 80 cm3.\nArticle 1 : réduction et majoration des primes\nLors de chaque échéance annuelle du contrat, la prime due par l’assuré  \nest déterminée en multipliant le montant de la prime de référence, telle \nqu’elle est définie à l’article 2, par un coefficient dit « coefficient de \nréduction-majoration » fixé conformément aux articles 4 et 5 suivants. \n\n 6819.  VOTRE COEFFICIENT BONUS/MALUS\n 19.1  Signification du coefficient bonus/\nmalus\nC’est l’instrument de mesure de votre Bonus ou de votre \nMalus.\n      Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler. \n\n Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ? \n\n Ex. : Au 01/01/N-1 : coefficient Bonus/Malus = 0,80 (soit 20 % \nde Bonus).\nSi vous êtes entièrement responsable d’un sinistre* au cours \nde l’année N-1 :\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 = 1 (ni \nBonus ni Malus).\nSi vous êtes entièrement responsable de 2 sinistres* au cours \nde l’année N-1:\nau 01/01/N : coefficient Bonus/Malus = 0,80 x 1,25 x 1,25= \n1,25 (soit 25 % de Malus).\nLes sinistres* pris en compte pour le calcul de votre coefficient \nau 01/01/N sont ceux survenus du 01/11/N-2 au 31/10/N-1.\n     si la responsabilité de l’assuré* n’est que partiellement \nengagée, la majoration est réduite de moitié : le coefficient est multiplié par 1,125. \n\n Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur). \n\n Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours.\nLe coefficient d’origine, c’est-à-dire celui affecté à un \nconducteur lors de sa première année d’assurance*, est égal à 1. \n\n Ex. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991.\nPar dérogation aux dispositions légales (Article A 121-1  \nalinéa 2 du Code des assurances*), nous appliquons également le coefficient Bonus/Malus au mini-véhicule (quadricycle à moteur).\n 19.2    Comment évolue le coefficient bonus/\nmalus ?\n   Dans ce présent paragraphe, N correspond à l’année en \ncours. \n\n Lorsqu’il est inférieur à 1, il vous donne par différence \nvotre taux de Bonus.\nEx. : coefficient 0,80 = 20 % de Bonus.\n  Lorsqu’il est supérieur à 1, il vous donne par différence \nvotre taux de Malus.\nEx. : coefficient 1,25 = 25 % de Malus.\nLors de la souscription de votre contrat il est mentionné \nsur vos Conditions particulières. Affecté à la cotisation de référence, il est un des éléments de calcul de la cotisation que vous devez régler.\nLes conditions d’application et d’évolution de ce coefficient \nBonus/Malus sont fixées par un arrêté ministériel qui s’impose à toutes les Sociétés d’Assurance. Cet arrêté a été modifié le 22/11/1991. \n\n Multilingual Transformer Architectures ■91\nis obvious that the number of available articles in English is much higher\nthan other languages such as Persian or Urdu. Similarly, the amount of\nparallel data available for pairs of languages diﬀer highly depending of\nthe popularity of such languages. To overcome these challenges, most\nmLMs use exponentially smoothed weighting of the data while creat-\ning the pre-training data. This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. refer to the model distillation category as a post-training\nmethod where the encoded knowledge in the model is distilled into a rep-\nresentation agreeable for an explanation by a user, as shown in Fig. 7.3.\nXie et al. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n This weighting prevents the low resource lan-\nguagesfrombeingunderrepresented.Morespeciﬁcally,if m%ofthetotal\npre-training data belongs to language i, then the probability of this lan-\nguage ispi=k\n100. Each probability is then modulated by an exponen-\ntial factor α, then the resulting values are normalized to provide the\nﬁnal probability distribution over the languages. This probability dis-\ntribution is used to the sample the pre-training data from the diﬀerent\nlanguages. Therefore, low-resource languages will be over sampled while\nhigh-resource languages will be under sampled. Such process guaran-\ntees the use of a reasonable set of vocabulary (while training the Word-\nPiece [220] or SentencePiece model [144]) in the low resource languages.\nTable 4.1 summarizes the number of languages supported by diﬀerent\nmLMs and the total vocabulary used by them. Typically, mLMs which\nsupport more languages have a larger vocabulary. \n\n (5.53)\nWhen the kernel functions φ(x)are as deﬁned in [53], the attention\nweightsAij= exp(\nqikT\nj)\ncan be approximated by\nexp(qikT\nj) = Λ⟨cosh( ωT(qi+kj))⟩ω, (5.54)\nwhere Λ = exp(−(∥qi∥2+∥kj∥2)/2)andωis sampled from the dk-\ndimensional standard normal distribution. If ωin (5.54) is replaced with√\ndω\n∥ω∥, thenωis any point on the surface of the dk-dimensional sphere\nof radius√dk.\nThis kernel approximation of the softmax calculation reduces the\nquadratic complexity to one that is nearly linear in sequence length, and\nthe approximation error can be decreased by periodically resampling ω. \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n \n La question est What is the default coefficient used in the calculation of the premium for the insured as mentioned in Article 1?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:52.427457",
              "duration": 2.000403,
              "cost": 0.002234
            },
            "meta": {},
            "text": "The default coefficient used in the calculation of the premium for the insured as mentioned in Article 1 is 1. This coefficient is applied to the prime de référence to determine the premium due by the insured at each annual renewal of the contract.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The default coefficient used in the calculation of the premium for the insured as mentioned in Article 1 is 1. This coefficient is applied to the prime de référence to determine the premium due by the insured at each annual renewal of the contract.\n\n1. 1. The default coefficient used in premium calculation for the insured is 1.\n2. 2. Article 1 mentions the use of a default coefficient in premium calculation.\n3. 3. The insured's premium is calculated using a default coefficient of 1.\n4. 4. The coefficient of 1 is the standard used in calculating the insured's premium.\n5. 5. The premium for the insured is calculated using a coefficient, which by default is 1.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:10.558264",
                "duration": 2.174405,
                "cost": 0.00834
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What are the different options proposed by the medical team for transport or medical repatriation?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text lacks details on various transport options suggested by the medical team.\n2. There is no mention of medical repatriation proposals in the text.\n3. The medical team's proposed options for transport are not discussed in the text.\n4. The text does not elaborate on any medical repatriation plans put forth by the medical team.\n5. There is an absence of information on the medical team's transport or medical repatriation suggestions in the text.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the different options proposed by the medical team for transport or medical repatriation.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:23:54.452366",
          "duration": 9.221112,
          "cost": 0.00978
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text lacks details on various transport options suggested by the medical team."
          },
          {
            "meta": {},
            "text": "2. There is no mention of medical repatriation proposals in the text."
          },
          {
            "meta": {},
            "text": "3. The medical team's proposed options for transport are not discussed in the text."
          },
          {
            "meta": {},
            "text": "4. The text does not elaborate on any medical repatriation plans put forth by the medical team."
          },
          {
            "meta": {},
            "text": "5. There is an absence of information on the medical team's transport or medical repatriation suggestions in the text."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 7.979483107730073,
              "Node id": "38d293dc-fd6b-44b3-b374-5efb8dd25fd8"
            },
            "text": "19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription."
          },
          {
            "meta": {
              "score": 7.478812879476664,
              "Node id": "abbb6d4d-549d-43c0-b450-aa47ab99b947"
            },
            "text": "• à vérifier régulièrement ses e -mails indésirables afin de s’ assurer que des \ne-mails liés à la gestion et à l’exécution de son contrat n’y figurent pas et \ns’ils y figurent à en prendre connaissance.\n19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations."
          },
          {
            "meta": {
              "score": 7.432179893935129,
              "Node id": "b0669ced-bf64-4faf-b3ba-e66acfae3a06"
            },
            "text": "■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même."
          },
          {
            "meta": {
              "score": 7.432179893935129,
              "Node id": "bf5337d6-771c-471a-bce3-0e0701d97f85"
            },
            "text": "30  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) Section V  - GARANTIES MOBILITÉ    \nLes garanties Mobilité vous sont acquises uniquement lorsque vous les avez souscrites et qu’elles figurent aux  conditions \nparticulières  et ce, qu’elles soient en inclusion ou en option de la formule de garanties choisie.  \n \nLes plafonds et modalités d’indemnisation applicables aux garanties Mobilité figurent aux articles 3 -1 et 34 des présentes conditions  \ngénérales . \n \nARTICLE  23    Assistance au véhicule et aux personnes transportées  \n \nMatmut  Assistance  propose un ensemble de prestations mis en œuvre  par Inter Mutuelles Assistance GIE (118 avenue de Paris, \n79000 Niort)."
          },
          {
            "meta": {
              "score": 7.164154021679057,
              "Node id": "dca9be1e-f788-45f5-8d70-542eababe57d"
            },
            "text": "19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées."
          },
          {
            "meta": {
              "score": 7.164154021679057,
              "Node id": "a5cb4ad4-1802-4c46-a9c1-fe7be20bd1c7"
            },
            "text": "• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même. A tout moment, l’assuré peut retourner sur \nl’écran de consultation précédent afin de corriger une information inexacte."
          },
          {
            "meta": {
              "score": 7.047684860887287,
              "Node id": "054a1a01-3d4e-4b6e-b5a5-deb53b91443f"
            },
            "text": "792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2."
          },
          {
            "meta": {
              "score": 6.902247850166777,
              "Node id": "489e4a90-45ef-4a73-b9e0-bcb2eb947955"
            },
            "text": "1.2.2 Pre-training Methods and Applications\nA large body of work has focused on how a transformer can be\npre-trained. There are encoder-only models, such as BERT [71], decoder-\nonly models like the famed generative pre-trained transformer models\nGPT-3 [32], and encoder-decoder models like T5 [205] and ByT5 [280].\nBERT is discussed in detail in Chapter 3, T5 in Chapter 5, and ByT5\ninChapter 6.\nThere have been many application and domain-speciﬁc transform-\ners made for speciﬁc data domains (e.g., ﬁnancial or medical text) and\nspeciﬁc kinds of data (e.g., images or video). We discuss several such\napplications in Chapter 6.\n1.3 RESOURCES\nIn this section, we will discuss some resources that can be useful for\nresearchers and practitioners."
          },
          {
            "meta": {
              "score": 6.855093626478503,
              "Node id": "73e992be-5123-455c-8a30-e7f7d8e50cea"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■213\nTABLE 7.1 Fine-tuned base BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0."
          },
          {
            "meta": {
              "score": 6.855093626478503,
              "Node id": "90c002d9-5730-47b6-8dee-ff80be6724cf"
            },
            "text": "612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2.The BIO_ClinincalBERT performs worse than the base model on\n39instances."
          },
          {
            "meta": {
              "score": 0.7436000345526245,
              "Node id": "eada2f44-1d6a-4882-a349-8816a3a160af"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model."
          },
          {
            "meta": {
              "score": 0.7413522330057666,
              "Node id": "1013254d-f04a-45d8-868b-43537d884680"
            },
            "text": "However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method."
          },
          {
            "meta": {
              "score": 0.7368999616417928,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7362617045573401,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7320977050408379,
              "Node id": "db77345e-39fb-4953-be00-c116ea012076"
            },
            "text": "Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package."
          },
          {
            "meta": {
              "score": 0.7319244652544253,
              "Node id": "11e1d6a4-5cd2-4f6e-be52-db875dcc1342"
            },
            "text": "Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package.\n7.6.3 Experiments, Results, and Analysis\n7.6.3.1 Exploratory data analysis\nWe perform some basic EDA to understand the data from distribution\nand corpus wise."
          },
          {
            "meta": {
              "score": 0.7286383948109912,
              "Node id": "4d65ee66-c6ab-42f3-adfc-f4c07a168587"
            },
            "text": "The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data."
          },
          {
            "meta": {
              "score": 0.726950346016842,
              "Node id": "ff1a2384-275a-45c5-b831-ff1e62d333bf"
            },
            "text": "),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT."
          },
          {
            "meta": {
              "score": 0.7264591296568713,
              "Node id": "1c0c1207-d0bc-4f2f-831f-6a4501ffdac0"
            },
            "text": "Figs. 7.7 to 7.10 shows interesting outputs from word cloud, docu-\nment length distribution, top words, and class distribution. The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves."
          },
          {
            "meta": {
              "score": 0.7252444885216325,
              "Node id": "4300b614-4c52-466f-9450-51ba2106ed79"
            },
            "text": "This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the different options proposed by the medical team for transport or medical repatriation.",
              "prompt": {
                "meta": {},
                "user": "What are the different options proposed by the medical team for transport or medical repatriation?",
                "system": "Contexte :  19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. \n\n • à vérifier régulièrement ses e -mails indésirables afin de s’ assurer que des \ne-mails liés à la gestion et à l’exécution de son contrat n’y figurent pas et \ns’ils y figurent à en prendre connaissance.\n19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations. \n\n ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même. \n\n 30  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) Section V  - GARANTIES MOBILITÉ    \nLes garanties Mobilité vous sont acquises uniquement lorsque vous les avez souscrites et qu’elles figurent aux  conditions \nparticulières  et ce, qu’elles soient en inclusion ou en option de la formule de garanties choisie.  \n \nLes plafonds et modalités d’indemnisation applicables aux garanties Mobilité figurent aux articles 3 -1 et 34 des présentes conditions  \ngénérales . \n \nARTICLE  23    Assistance au véhicule et aux personnes transportées  \n \nMatmut  Assistance  propose un ensemble de prestations mis en œuvre  par Inter Mutuelles Assistance GIE (118 avenue de Paris, \n79000 Niort). \n\n 19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées. \n\n • Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même. A tout moment, l’assuré peut retourner sur \nl’écran de consultation précédent afin de corriger une information inexacte. \n\n 792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2. \n\n 1.2.2 Pre-training Methods and Applications\nA large body of work has focused on how a transformer can be\npre-trained. There are encoder-only models, such as BERT [71], decoder-\nonly models like the famed generative pre-trained transformer models\nGPT-3 [32], and encoder-decoder models like T5 [205] and ByT5 [280].\nBERT is discussed in detail in Chapter 3, T5 in Chapter 5, and ByT5\ninChapter 6.\nThere have been many application and domain-speciﬁc transform-\ners made for speciﬁc data domains (e.g., ﬁnancial or medical text) and\nspeciﬁc kinds of data (e.g., images or video). We discuss several such\napplications in Chapter 6.\n1.3 RESOURCES\nIn this section, we will discuss some resources that can be useful for\nresearchers and practitioners. \n\n Interpretability and Explainability Techniques for Transformers ■213\nTABLE 7.1 Fine-tuned base BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0. \n\n 612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2.The BIO_ClinincalBERT performs worse than the base model on\n39instances. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package. \n\n Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package.\n7.6.3 Experiments, Results, and Analysis\n7.6.3.1 Exploratory data analysis\nWe perform some basic EDA to understand the data from distribution\nand corpus wise. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n ),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT. \n\n Figs. 7.7 to 7.10 shows interesting outputs from word cloud, docu-\nment length distribution, top words, and class distribution. The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. \n\n This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. \n\n \n La question est What are the different options proposed by the medical team for transport or medical repatriation?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:07:55.133693",
              "duration": 2.700816,
              "cost": 0.12705
            },
            "meta": {},
            "text": "The text does not provide information on the different options proposed by the medical team for transport or medical repatriation.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the different options proposed by the medical team for transport or medical repatriation.\n\n1. 1. The text lacks details on various transport options suggested by the medical team.\n2. 2. There is no mention of medical repatriation proposals in the text.\n3. 3. The medical team's proposed options for transport are not discussed in the text.\n4. 4. The text does not elaborate on any medical repatriation plans put forth by the medical team.\n5. 5. There is an absence of information on the medical team's transport or medical repatriation suggestions in the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:12.077541",
                "duration": 1.516056,
                "cost": 0.0078000000000000005
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The medical team proposes two options for transport or medical repatriation:\n\n1. **Assistance au véhicule et aux personnes transportées**: Matmut Assistance offers a range of services implemented by Inter Mutuelles Assistance GIE, located at 118 avenue de Paris, 79000 Niort. This service includes assistance for the vehicle and the individuals being transported.\n\n2. **Garanties Mobilité**: The Mobility Guarantees are available when subscribed to and listed in the specific conditions of the contract, whether they are included or optional in the chosen coverage formula. The ceilings and indemnification modalities applicable to the Mobility Guarantees are detailed in articles 3-1 and 34 of the general conditions.\n\nThese options provide support and assistance for transport or medical repatriation needs as part of the insurance coverage.",
              "prompt": {
                "meta": {},
                "user": "What are the different options proposed by the medical team for transport or medical repatriation?",
                "system": "Contexte :  19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. \n\n • à vérifier régulièrement ses e -mails indésirables afin de s’ assurer que des \ne-mails liés à la gestion et à l’exécution de son contrat n’y figurent pas et \ns’ils y figurent à en prendre connaissance.\n19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations. \n\n ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même. \n\n 30  CONDITIONS GÉNÉRALES   \nLe symbole renvoie à un terme  \ndéfini au lexi que (article 1 ) Section V  - GARANTIES MOBILITÉ    \nLes garanties Mobilité vous sont acquises uniquement lorsque vous les avez souscrites et qu’elles figurent aux  conditions \nparticulières  et ce, qu’elles soient en inclusion ou en option de la formule de garanties choisie.  \n \nLes plafonds et modalités d’indemnisation applicables aux garanties Mobilité figurent aux articles 3 -1 et 34 des présentes conditions  \ngénérales . \n \nARTICLE  23    Assistance au véhicule et aux personnes transportées  \n \nMatmut  Assistance  propose un ensemble de prestations mis en œuvre  par Inter Mutuelles Assistance GIE (118 avenue de Paris, \n79000 Niort). \n\n 19 BIS.1.4. Processus de souscription électronique\nL’assuré peut souscrire son contrat électroniquement :\n• soit dans le cadre d’une souscription auprès de l’un de nos conseillers ;\n• soit dans le cadre d’une souscription en ligne sur Internet.\n  ■Quelles sont les différentes étapes du processus de souscription \nélectronique ?\n• Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées. \n\n • Renseignements des informations concernant l’assuré\nAux fins de nous permettre de proposer à l’assuré le contrat et les options \nles plus adaptées, l’assuré doit renseigner un certain nombre d’informations.\nSi l’assuré souscrit son contrat par téléphone auprès de l’un de \nnos conseillers, ses réponses sont recueillies préalablement à toute \nsouscription. Lorsqu’une date et heure sont indiquées en bas d’un \ndocument (telles les Conditions Personnelles), par la signature électronique \nde ce document l’assuré reconnaît que ce document a bien été établi à la \ndate et heure indiquées.\nSi l’assuré souscrit son contrat en ligne sur Internet, il doit renseigner \nces informations lui-même. A tout moment, l’assuré peut retourner sur \nl’écran de consultation précédent afin de corriger une information inexacte. \n\n 792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2. \n\n 1.2.2 Pre-training Methods and Applications\nA large body of work has focused on how a transformer can be\npre-trained. There are encoder-only models, such as BERT [71], decoder-\nonly models like the famed generative pre-trained transformer models\nGPT-3 [32], and encoder-decoder models like T5 [205] and ByT5 [280].\nBERT is discussed in detail in Chapter 3, T5 in Chapter 5, and ByT5\ninChapter 6.\nThere have been many application and domain-speciﬁc transform-\ners made for speciﬁc data domains (e.g., ﬁnancial or medical text) and\nspeciﬁc kinds of data (e.g., images or video). We discuss several such\napplications in Chapter 6.\n1.3 RESOURCES\nIn this section, we will discuss some resources that can be useful for\nresearchers and practitioners. \n\n Interpretability and Explainability Techniques for Transformers ■213\nTABLE 7.1 Fine-tuned base BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.792 0.768 0.779\nMedical Records 0.612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0. \n\n 612 0.510 0.556\nInternal Medicine 0.588 0.642 0.614\nOther 0.587 0.670 0.626\naccuracy 0.67\nmacro avg 0.645 0.648 0.644\nweighted avg 0.673 0.670 0.670\nTABLE 7.2 Fine-tuned Bio_Clinical BERT on Test Data\nClass/Metrics Precision RecallF1\nSurgery 0.796 0.788 0.792\nMedical Records 0.605 0.676 0.639\nInternal Medicine 0.690 0.633 0.660\nOther 0.589 0.582 0.586\naccuracy 0.694\nmacro avg 0.670 0.670 0.669\nweighted avg 0.696 0.694 0.694\n2.The BIO_ClinincalBERT performs worse than the base model on\n39instances. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package. \n\n Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. We employ the Huggingface implementation of BERT\nand Bio_ClinicalBERT as our transformer implementations. Captum li-\nbrary is used to perform input attribution via the saliency method. For\nunderstanding and visualizing how the BERT layers and heads work we\nuse the exBERT visualization package.\n7.6.3 Experiments, Results, and Analysis\n7.6.3.1 Exploratory data analysis\nWe perform some basic EDA to understand the data from distribution\nand corpus wise. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n ),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT. \n\n Figs. 7.7 to 7.10 shows interesting outputs from word cloud, docu-\nment length distribution, top words, and class distribution. The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. \n\n This raises the question if multilingual transformers learn\nlanguage-agnostic cross-lingual representations. Several approaches have\nbeen proposed to answer this question, each with a diﬀerent angle of at-\ntack; (i) Ablation Studies, (ii) Task Probes, and (iii) Parallel Corpora\nRepresentations.\nAblation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. \n\n \n La question est What are the different options proposed by the medical team for transport or medical repatriation?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:07:59.457066",
              "duration": 4.320435,
              "cost": 0.0023435
            },
            "meta": {},
            "text": "The medical team proposes two options for transport or medical repatriation:\n\n1. **Assistance au véhicule et aux personnes transportées**: Matmut Assistance offers a range of services implemented by Inter Mutuelles Assistance GIE, located at 118 avenue de Paris, 79000 Niort. This service includes assistance for the vehicle and the individuals being transported.\n\n2. **Garanties Mobilité**: The Mobility Guarantees are available when subscribed to and listed in the specific conditions of the contract, whether they are included or optional in the chosen coverage formula. The ceilings and indemnification modalities applicable to the Mobility Guarantees are detailed in articles 3-1 and 34 of the general conditions.\n\nThese options provide support and assistance for transport or medical repatriation needs as part of the insurance coverage.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The medical team proposes two options for transport or medical repatriation:\n\n1. **Assistance au véhicule et aux personnes transportées**: Matmut Assistance offers a range of services implemented by Inter Mutuelles Assistance GIE, located at 118 avenue de Paris, 79000 Niort. This service includes assistance for the vehicle and the individuals being transported.\n\n2. **Garanties Mobilité**: The Mobility Guarantees are available when subscribed to and listed in the specific conditions of the contract, whether they are included or optional in the chosen coverage formula. The ceilings and indemnification modalities applicable to the Mobility Guarantees are detailed in articles 3-1 and 34 of the general conditions.\n\nThese options provide support and assistance for transport or medical repatriation needs as part of the insurance coverage.\n\n1. 1. The text lacks details on various transport options suggested by the medical team.\n2. 2. There is no mention of medical repatriation proposals in the text.\n3. 3. The medical team's proposed options for transport are not discussed in the text.\n4. 4. The text does not elaborate on any medical repatriation plans put forth by the medical team.\n5. 5. There is an absence of information on the medical team's transport or medical repatriation suggestions in the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:13.512645",
                "duration": 1.43287,
                "cost": 0.01191
              },
              "meta": {},
              "text": "Validated: ['2', '3', '4', '5'] - Not validated: ['1']",
              "human": 1.0,
              "auto": 0.8
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the coverage amount determined based on the level of permanent disability?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not explain how the coverage amount is calculated.\n2. The determination of the coverage amount based on the level of permanent disability is not clarified in the text.\n3. There is no information in the text about the correlation between the coverage amount and the level of permanent disability.\n4. The text lacks details on the method used to determine the coverage amount for permanent disability.\n5. The text does not provide a formula or method for determining the coverage amount based on permanent disability.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on how the coverage amount is determined based on the level of permanent disability.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:04.812177",
          "duration": 10.356609,
          "cost": 0.010020000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not explain how the coverage amount is calculated."
          },
          {
            "meta": {},
            "text": "2. The determination of the coverage amount based on the level of permanent disability is not clarified in the text."
          },
          {
            "meta": {},
            "text": "3. There is no information in the text about the correlation between the coverage amount and the level of permanent disability."
          },
          {
            "meta": {},
            "text": "4. The text lacks details on the method used to determine the coverage amount for permanent disability."
          },
          {
            "meta": {},
            "text": "5. The text does not provide a formula or method for determining the coverage amount based on permanent disability."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.230308676291898,
              "Node id": "8218fbbf-3aa7-4fa9-a5fe-413931870459"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi."
          },
          {
            "meta": {
              "score": 11.496514557918099,
              "Node id": "c81640bd-aeeb-4805-b1d8-7e251b017898"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used."
          },
          {
            "meta": {
              "score": 11.353161188739993,
              "Node id": "066f4cda-ff48-496d-a467-a0d84e932b98"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al."
          },
          {
            "meta": {
              "score": 10.89686336635127,
              "Node id": "faea14e8-dc72-4780-bd52-1289052ccd73"
            },
            "text": "14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process."
          },
          {
            "meta": {
              "score": 10.479620365496425,
              "Node id": "06879607-4884-4228-bf8c-17f83cda6894"
            },
            "text": "Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information."
          },
          {
            "meta": {
              "score": 9.951756844524528,
              "Node id": "a92d29a6-be7c-4b13-8243-3b5a17348d37"
            },
            "text": "Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers."
          },
          {
            "meta": {
              "score": 9.926101941222973,
              "Node id": "9accc4ad-b779-4ffb-8fa0-54df17731eb7"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 9.901960038635075,
              "Node id": "d60c06d9-b06b-48b1-9540-f2540c28c5b0"
            },
            "text": "For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture."
          },
          {
            "meta": {
              "score": 9.901960038635075,
              "Node id": "424bf425-8bbc-412e-8788-434af90f1331"
            },
            "text": "Vi-\nsualization of the attention layers demonstrates that the model selects\ninformative words and sentences for classiﬁcation.\nJesse Vig introduced a tool “BertViz” for visualizing attention in the\nTransformer at all levels, viz. at the whole model level, the attention-\nhead level, and the individual neuron level [255]. The model view pro-\nvides a single high-level view of the entire model across all the layers\nand heads for a given input. The model view helps in visualizing how\nattention patterns evolve in the attention heads across all layers. The\nattention-head view visualizes the attention patterns produced by one\nor more attention heads in a given layer and can help detect biases such\nas gender bias. The neuron view visualizes the individual neurons in the\nquery and key vectors and their interaction producing attention, thus\ngiving a detailed view of how the patterns are formed.\n7.3.3.2 Joint training\nIn a joint training-based approach, the explanation task is associated\nwith the actual learning task so that the model learns the explanation\nand the actual task jointly."
          },
          {
            "meta": {
              "score": 9.458046824607262,
              "Node id": "6593421a-e0b0-4d2f-bedb-6322ce36ccdf"
            },
            "text": "Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. Instead of a ﬁxed number of layers in the\ntransformers,UniversaltransformershaveaUniversaltransformerblock,\ni.e., a self-attention mechanism followed by a recurrent transformation\nthatprovidesarecurrentinductivebiasforeachinputsymbolinparallel.\nAs shown in Fig. 5.5, the Universal Transformer is a recurrent function\nnot in time but in depth that evolves the hidden states corresponding to\nevery input in parallel, based at each step on the sequence of previous\nhidden states.\nUT has many commonalities with the existing neural architectures,\nsuch as the Neural GPU [136] and the Neural Turing Machine [98]."
          },
          {
            "meta": {
              "score": 0.7451096510491757,
              "Node id": "845269f1-acb6-47f7-908c-905f8cee14f1"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 0.7402495247939889,
              "Node id": "50610582-b112-4f31-a319-d19106cdf74d"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used."
          },
          {
            "meta": {
              "score": 0.7398502598968723,
              "Node id": "9c4d9242-2223-41fc-b760-d4ce2373d989"
            },
            "text": "14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process."
          },
          {
            "meta": {
              "score": 0.7381827184460765,
              "Node id": "f4d2ab08-3665-4baf-9913-cdd36a2effd9"
            },
            "text": "provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]."
          },
          {
            "meta": {
              "score": 0.737467633688664,
              "Node id": "ac02af39-b35b-4b3b-9117-9296aca82f75"
            },
            "text": "Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance."
          },
          {
            "meta": {
              "score": 0.7369487638363872,
              "Node id": "3717f92d-e7f5-4240-a356-a4166eeaafca"
            },
            "text": "The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]."
          },
          {
            "meta": {
              "score": 0.7355601759086533,
              "Node id": "4d65ee66-c6ab-42f3-adfc-f4c07a168587"
            },
            "text": "The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data."
          },
          {
            "meta": {
              "score": 0.733816847065257,
              "Node id": "87414ab0-3426-4230-906b-f033b27cba48"
            },
            "text": "The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper."
          },
          {
            "meta": {
              "score": 0.7336267292784047,
              "Node id": "6739a344-7e2c-49a9-93de-145dc7dfdd1c"
            },
            "text": "i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),"
          },
          {
            "meta": {
              "score": 0.7335857111779727,
              "Node id": "55322a5c-15ba-412e-9352-b3eb4dcbfd3c"
            },
            "text": "Cumulative scoring is an evaluation set-dependent metric esti-\nmating how much higher a score one can obtain on a probing task with\nthe introduction of each layer.\nHewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on how the coverage amount is determined based on the level of permanent disability.",
              "prompt": {
                "meta": {},
                "user": "How is the coverage amount determined based on the level of permanent disability?",
                "system": "Contexte :  Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. \n\n Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. \n\n Vi-\nsualization of the attention layers demonstrates that the model selects\ninformative words and sentences for classiﬁcation.\nJesse Vig introduced a tool “BertViz” for visualizing attention in the\nTransformer at all levels, viz. at the whole model level, the attention-\nhead level, and the individual neuron level [255]. The model view pro-\nvides a single high-level view of the entire model across all the layers\nand heads for a given input. The model view helps in visualizing how\nattention patterns evolve in the attention heads across all layers. The\nattention-head view visualizes the attention patterns produced by one\nor more attention heads in a given layer and can help detect biases such\nas gender bias. The neuron view visualizes the individual neurons in the\nquery and key vectors and their interaction producing attention, thus\ngiving a detailed view of how the patterns are formed.\n7.3.3.2 Joint training\nIn a joint training-based approach, the explanation task is associated\nwith the actual learning task so that the model learns the explanation\nand the actual task jointly. \n\n Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. Instead of a ﬁxed number of layers in the\ntransformers,UniversaltransformershaveaUniversaltransformerblock,\ni.e., a self-attention mechanism followed by a recurrent transformation\nthatprovidesarecurrentinductivebiasforeachinputsymbolinparallel.\nAs shown in Fig. 5.5, the Universal Transformer is a recurrent function\nnot in time but in depth that evolves the hidden states corresponding to\nevery input in parallel, based at each step on the sequence of previous\nhidden states.\nUT has many commonalities with the existing neural architectures,\nsuch as the Neural GPU [136] and the Neural Turing Machine [98]. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. \n\n Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance. \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n Cumulative scoring is an evaluation set-dependent metric esti-\nmating how much higher a score one can obtain on a probing task with\nthe introduction of each layer.\nHewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. \n\n \n La question est How is the coverage amount determined based on the level of permanent disability?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:02.459991",
              "duration": 2.998589,
              "cost": 0.135
            },
            "meta": {},
            "text": "The text does not provide information on how the coverage amount is determined based on the level of permanent disability.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 3",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on how the coverage amount is determined based on the level of permanent disability.\n\n1. 1. The text does not explain how the coverage amount is calculated.\n2. 2. The determination of the coverage amount based on the level of permanent disability is not clarified in the text.\n3. 3. There is no information in the text about the correlation between the coverage amount and the level of permanent disability.\n4. 4. The text lacks details on the method used to determine the coverage amount for permanent disability.\n5. 5. The text does not provide a formula or method for determining the coverage amount based on permanent disability.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:14.824007",
                "duration": 1.309491,
                "cost": 0.00735
              },
              "meta": {},
              "text": "Validated: ['2', '3'] - Not validated: ['1', '4', '5']",
              "human": 1.0,
              "auto": 0.4
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The coverage amount based on the level of permanent disability is determined through a process that involves assessing the extent of impairment or disability that an individual has sustained. This assessment is typically done by medical professionals or experts in the field of disability evaluation.\n\nThe determination of the coverage amount may involve the following steps:\n\n1. **Medical Evaluation**: The individual's medical condition and functional limitations are assessed to determine the level of permanent disability.\n\n2. **Disability Rating**: A disability rating is assigned based on the severity of the impairment. This rating is often expressed as a percentage, with higher percentages indicating more severe disabilities.\n\n3. **Insurance Policy**: The coverage amount may be outlined in the individual's insurance policy or benefits plan. The policy will specify how much coverage is provided based on the disability rating.\n\n4. **Coverage Calculation**: The coverage amount is calculated based on the disability rating and the terms of the insurance policy. This calculation may take into account factors such as the individual's income, occupation, and the type of disability insurance coverage they have.\n\n5. **Legal Regulations**: In some cases, legal regulations or guidelines may dictate how the coverage amount is determined for individuals with permanent disabilities.\n\nOverall, the coverage amount for individuals with permanent disabilities is determined through a combination of medical assessment, insurance policy terms, and legal regulations to ensure that the individual receives appropriate financial support based on their level of disability.",
              "prompt": {
                "meta": {},
                "user": "How is the coverage amount determined based on the level of permanent disability?",
                "system": "Contexte :  Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n Interpretability and Explainability Techniques for Transformers ■195\n•Prediction Diﬀerence Analysis: Zintgraf et al. propose a\nmethod based on Robnik-Sikonja and Kononenko research, to\nmeasure the inﬂuence based on altering input information for\nprobabilistic-based classiﬁers [210, 304]. It evaluates the eﬀect of\nan input feature xiwith respect to class cby determining the dif-\nference between p(c|x−iandp(c|x )using the marginal probability\np(c|x−i) =\nxip(xi|x−i)p(c|x−i,xi) (7.9)\nwheremathbfx corresponds to all input features and x−icorre-\nsponds to all features except xi. The importance of the feature xi\nis measured using:\nDiﬀi(c|x) = log2(odds(c|x ))−log2(odds(c|x−i))(7.10)\n7.3.2 Model Distillation\nXie et al. \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. \n\n Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n For layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. It was shown that\ntheknowledgeoflinearwordorderdecreasesaroundlayer4intheBERT-\nbase. This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. \n\n Vi-\nsualization of the attention layers demonstrates that the model selects\ninformative words and sentences for classiﬁcation.\nJesse Vig introduced a tool “BertViz” for visualizing attention in the\nTransformer at all levels, viz. at the whole model level, the attention-\nhead level, and the individual neuron level [255]. The model view pro-\nvides a single high-level view of the entire model across all the layers\nand heads for a given input. The model view helps in visualizing how\nattention patterns evolve in the attention heads across all layers. The\nattention-head view visualizes the attention patterns produced by one\nor more attention heads in a given layer and can help detect biases such\nas gender bias. The neuron view visualizes the individual neurons in the\nquery and key vectors and their interaction producing attention, thus\ngiving a detailed view of how the patterns are formed.\n7.3.3.2 Joint training\nIn a joint training-based approach, the explanation task is associated\nwith the actual learning task so that the model learns the explanation\nand the actual task jointly. \n\n Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. Instead of a ﬁxed number of layers in the\ntransformers,UniversaltransformershaveaUniversaltransformerblock,\ni.e., a self-attention mechanism followed by a recurrent transformation\nthatprovidesarecurrentinductivebiasforeachinputsymbolinparallel.\nAs shown in Fig. 5.5, the Universal Transformer is a recurrent function\nnot in time but in depth that evolves the hidden states corresponding to\nevery input in parallel, based at each step on the sequence of previous\nhidden states.\nUT has many commonalities with the existing neural architectures,\nsuch as the Neural GPU [136] and the Neural Turing Machine [98]. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n 14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. Beam-search\ncan be used to improve upon this greedy search method by main-\ntaining a candidate set of rules during the iterative search process. \n\n provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. \n\n Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance. \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n The choice of\nthe reference input x′is domain speciﬁc. For example, in MNIST\nclassiﬁcation, input with all zeros representing the common back-\nground can be one of the choices for the reference input.\n∆t=f(x)−f(x′)is the diﬀerence in neuron output between input\nxand a reference input x′. The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n Cumulative scoring is an evaluation set-dependent metric esti-\nmating how much higher a score one can obtain on a probing task with\nthe introduction of each layer.\nHewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. \n\n \n La question est How is the coverage amount determined based on the level of permanent disability?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:09.697483",
              "duration": 7.234348,
              "cost": 0.0026445
            },
            "meta": {},
            "text": "The coverage amount based on the level of permanent disability is determined through a process that involves assessing the extent of impairment or disability that an individual has sustained. This assessment is typically done by medical professionals or experts in the field of disability evaluation.\n\nThe determination of the coverage amount may involve the following steps:\n\n1. **Medical Evaluation**: The individual's medical condition and functional limitations are assessed to determine the level of permanent disability.\n\n2. **Disability Rating**: A disability rating is assigned based on the severity of the impairment. This rating is often expressed as a percentage, with higher percentages indicating more severe disabilities.\n\n3. **Insurance Policy**: The coverage amount may be outlined in the individual's insurance policy or benefits plan. The policy will specify how much coverage is provided based on the disability rating.\n\n4. **Coverage Calculation**: The coverage amount is calculated based on the disability rating and the terms of the insurance policy. This calculation may take into account factors such as the individual's income, occupation, and the type of disability insurance coverage they have.\n\n5. **Legal Regulations**: In some cases, legal regulations or guidelines may dictate how the coverage amount is determined for individuals with permanent disabilities.\n\nOverall, the coverage amount for individuals with permanent disabilities is determined through a combination of medical assessment, insurance policy terms, and legal regulations to ensure that the individual receives appropriate financial support based on their level of disability.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The coverage amount based on the level of permanent disability is determined through a process that involves assessing the extent of impairment or disability that an individual has sustained. This assessment is typically done by medical professionals or experts in the field of disability evaluation.\n\nThe determination of the coverage amount may involve the following steps:\n\n1. **Medical Evaluation**: The individual's medical condition and functional limitations are assessed to determine the level of permanent disability.\n\n2. **Disability Rating**: A disability rating is assigned based on the severity of the impairment. This rating is often expressed as a percentage, with higher percentages indicating more severe disabilities.\n\n3. **Insurance Policy**: The coverage amount may be outlined in the individual's insurance policy or benefits plan. The policy will specify how much coverage is provided based on the disability rating.\n\n4. **Coverage Calculation**: The coverage amount is calculated based on the disability rating and the terms of the insurance policy. This calculation may take into account factors such as the individual's income, occupation, and the type of disability insurance coverage they have.\n\n5. **Legal Regulations**: In some cases, legal regulations or guidelines may dictate how the coverage amount is determined for individuals with permanent disabilities.\n\nOverall, the coverage amount for individuals with permanent disabilities is determined through a combination of medical assessment, insurance policy terms, and legal regulations to ensure that the individual receives appropriate financial support based on their level of disability.\n\n1. 1. The text does not explain how the coverage amount is calculated.\n2. 2. The determination of the coverage amount based on the level of permanent disability is not clarified in the text.\n3. 3. There is no information in the text about the correlation between the coverage amount and the level of permanent disability.\n4. 4. The text lacks details on the method used to determine the coverage amount for permanent disability.\n5. 5. The text does not provide a formula or method for determining the coverage amount based on permanent disability.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:16.365755",
                "duration": 1.539101,
                "cost": 0.01557
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How are the names and addresses of the members present or represented recorded during a general assembly?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not explain how to record the names of members at a general assembly.\n2. There is no information on how to document the addresses of attendees at a general assembly.\n3. The method of recording the presence or representation of members at a general assembly is not detailed in the text.\n4. The text lacks instructions on how to keep track of members who are represented at a general assembly.\n5. There is no mention of how to register the names and addresses of members during a general assembly in the text.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on how the names and addresses of the members present or represented are recorded during a general assembly.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:12.853321",
          "duration": 8.038066,
          "cost": 0.01056
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not explain how to record the names of members at a general assembly."
          },
          {
            "meta": {},
            "text": "2. There is no information on how to document the addresses of attendees at a general assembly."
          },
          {
            "meta": {},
            "text": "3. The method of recording the presence or representation of members at a general assembly is not detailed in the text."
          },
          {
            "meta": {},
            "text": "4. The text lacks instructions on how to keep track of members who are represented at a general assembly."
          },
          {
            "meta": {},
            "text": "5. There is no mention of how to register the names and addresses of members during a general assembly in the text."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.495386506695702,
              "Node id": "e3e8a5a1-ec07-468b-b421-10a340ee05b0"
            },
            "text": "BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it."
          },
          {
            "meta": {
              "score": 14.220811113253632,
              "Node id": "0ccf97f8-8c48-4899-8424-be8f48c76d7c"
            },
            "text": "3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree."
          },
          {
            "meta": {
              "score": 13.376224654523291,
              "Node id": "2618376e-acf6-4070-a6f9-bafac763284b"
            },
            "text": "However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles."
          },
          {
            "meta": {
              "score": 13.219204620591034,
              "Node id": "73f0fe48-b5cf-4822-9ed2-9adcb43be104"
            },
            "text": "52■Transformers for Machine Learning: A Deep Dive\nBERT sentence representation can also be obtained by pooling the\nindividual token representations. However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information."
          },
          {
            "meta": {
              "score": 13.1665402327788,
              "Node id": "9cf61317-fd3e-4c74-aa8b-b0257e2e087b"
            },
            "text": "),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT."
          },
          {
            "meta": {
              "score": 12.831888491792807,
              "Node id": "edbcb4a4-ecf7-4ef7-ad73-e55783de0028"
            },
            "text": "At a glance, we can see that the most signiﬁcant\nnegativeattributionsforthebaseBERTmodelcomefromkidney-related\nterms(kidney,renal,etc.),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc."
          },
          {
            "meta": {
              "score": 12.831888491792807,
              "Node id": "bdf90f8b-ca33-47c3-a435-8eac31045ffa"
            },
            "text": "The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT.\nWe can also visualize the self-attention weights to gain another view"
          },
          {
            "meta": {
              "score": 12.447065266863483,
              "Node id": "0ef7e427-adba-4269-b11d-8e37a62e2ff0"
            },
            "text": "[34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1."
          },
          {
            "meta": {
              "score": 11.916219235527702,
              "Node id": "90e27b9d-01bb-4437-bbc2-c5d95471d963"
            },
            "text": "[34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1.\n7.3.1 Visualization Methods\nVisualization methods reveal an explanation by highlighting the in-\nﬂuence between the inputs and the outputs in a black-box model as\nshown in Fig."
          },
          {
            "meta": {
              "score": 11.644879187899164,
              "Node id": "0e67488d-8de1-4aac-85a9-28155bda9dbf"
            },
            "text": "Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its"
          },
          {
            "meta": {
              "score": 0.7362575165632872,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7347751713552593,
              "Node id": "2d253c59-5b9a-4329-ac3f-00920de985be"
            },
            "text": "As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block."
          },
          {
            "meta": {
              "score": 0.7343581071362317,
              "Node id": "6c134297-b2fc-4687-8467-485529765d26"
            },
            "text": "For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows. The ultimate outputs of layers of such horizontal and vertical self-\nattention are distributed representations of utterance tokens and table\ncolumns,whichmaybeutilizedtocalculatethedatabasequerybydown-\nstream semantic parsers."
          },
          {
            "meta": {
              "score": 0.7336832308420272,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7322748805836786,
              "Node id": "e35aee43-c372-4d24-ac21-c7bbe2a150a6"
            },
            "text": ",\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R."
          },
          {
            "meta": {
              "score": 0.7322616018211048,
              "Node id": "55d9ed11-8f4a-4bbe-88d8-a374febff098"
            },
            "text": "To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above."
          },
          {
            "meta": {
              "score": 0.7318957496059983,
              "Node id": "691b39eb-9cc3-4b98-a9ab-3e6d9f87276a"
            },
            "text": "Bibliography ■249\ntional Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26–30, 2020, OpenReview.net, 2020.\n[264]A. Warstadt, A. Singh, and S. R. Bowman ,Neural network\nacceptability judgments , arXiv preprint arXiv:1805.12471, (2018).\n[265]D. Watts and S. Strogatz ,Collective dynamics of ‘small-\nworld’ networks , Nature, 393 (1998), pp. 440–442.\n[266]X. Wei, R. Weng, Y. Hu, L. Xing, H. Yu, and W. Luo ,\nOn learning universal representations across languages , in 9th In-\nternational Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3–7, 2021, OpenReview.net, 2021."
          },
          {
            "meta": {
              "score": 0.7310493691294992,
              "Node id": "417fe1e0-ac85-4c9c-84be-650b6d0505ab"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 0.7302349306240377,
              "Node id": "2436ee17-46ed-4ffd-8494-0c49400e40e3"
            },
            "text": "Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to."
          },
          {
            "meta": {
              "score": 0.7301162122667133,
              "Node id": "892b264e-cf0d-4939-8bb9-0b7bb1a0009a"
            },
            "text": "Transformer Modiﬁcations ■133\nwill only have directed edges with the vertices that correspond to the\nkey positions that it can attend to, as described by the set Siin (5.27).\nAdjacency matrix The graph representation casts the problem of\nreducing the complexity of the attention mechanism as a graph spar-\nsiﬁcation problem, which can be tackled with graph theory. Big Bird\ndescribes the attention graph with its L×Ladjacency matrix, A:\nA(i,j ) ={1, j∈Si\n0,otherwise(5.55)\nWhen the adjacency matrix is all ones, A(i,j ) = 1,∀(i,j), then we\nhave a fully connected graph where each vertex is connected to every\nother vertex and an attention mechanism with quadratic complexity.\nWheneverA(i,j ) = 0, it means that edge (i,j)does not exist and thus\nquery position i(qi) cannot attend to key position j(kj)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on how the names and addresses of the members present or represented are recorded during a general assembly.",
              "prompt": {
                "meta": {},
                "user": "How are the names and addresses of the members present or represented recorded during a general assembly?",
                "system": "Contexte :  BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. \n\n 3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. \n\n However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. \n\n 52■Transformers for Machine Learning: A Deep Dive\nBERT sentence representation can also be obtained by pooling the\nindividual token representations. However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. \n\n ),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT. \n\n At a glance, we can see that the most signiﬁcant\nnegativeattributionsforthebaseBERTmodelcomefromkidney-related\nterms(kidney,renal,etc.),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. \n\n The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT.\nWe can also visualize the self-attention weights to gain another view \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1. \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1.\n7.3.1 Visualization Methods\nVisualization methods reveal an explanation by highlighting the in-\nﬂuence between the inputs and the outputs in a black-box model as\nshown in Fig. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. \n\n For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows. The ultimate outputs of layers of such horizontal and vertical self-\nattention are distributed representations of utterance tokens and table\ncolumns,whichmaybeutilizedtocalculatethedatabasequerybydown-\nstream semantic parsers. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above. \n\n Bibliography ■249\ntional Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26–30, 2020, OpenReview.net, 2020.\n[264]A. Warstadt, A. Singh, and S. R. Bowman ,Neural network\nacceptability judgments , arXiv preprint arXiv:1805.12471, (2018).\n[265]D. Watts and S. Strogatz ,Collective dynamics of ‘small-\nworld’ networks , Nature, 393 (1998), pp. 440–442.\n[266]X. Wei, R. Weng, Y. Hu, L. Xing, H. Yu, and W. Luo ,\nOn learning universal representations across languages , in 9th In-\nternational Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3–7, 2021, OpenReview.net, 2021. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Transformer Modiﬁcations ■133\nwill only have directed edges with the vertices that correspond to the\nkey positions that it can attend to, as described by the set Siin (5.27).\nAdjacency matrix The graph representation casts the problem of\nreducing the complexity of the attention mechanism as a graph spar-\nsiﬁcation problem, which can be tackled with graph theory. Big Bird\ndescribes the attention graph with its L×Ladjacency matrix, A:\nA(i,j ) ={1, j∈Si\n0,otherwise(5.55)\nWhen the adjacency matrix is all ones, A(i,j ) = 1,∀(i,j), then we\nhave a fully connected graph where each vertex is connected to every\nother vertex and an attention mechanism with quadratic complexity.\nWheneverA(i,j ) = 0, it means that edge (i,j)does not exist and thus\nquery position i(qi) cannot attend to key position j(kj). \n\n \n La question est How are the names and addresses of the members present or represented recorded during a general assembly?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:12.216872",
              "duration": 2.513753,
              "cost": 0.13083
            },
            "meta": {},
            "text": "The text does not provide information on how the names and addresses of the members present or represented are recorded during a general assembly.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on how the names and addresses of the members present or represented are recorded during a general assembly.\n\n1. 1. The text does not explain how to record the names of members at a general assembly.\n2. 2. There is no information on how to document the addresses of attendees at a general assembly.\n3. 3. The method of recording the presence or representation of members at a general assembly is not detailed in the text.\n4. 4. The text lacks instructions on how to keep track of members who are represented at a general assembly.\n5. 5. There is no mention of how to register the names and addresses of members during a general assembly in the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:18.090201",
                "duration": 1.72112,
                "cost": 0.00822
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "During a general assembly, the names and addresses of the members present or represented are typically recorded in a sign-in sheet or attendance register. Each member is required to sign or mark their attendance along with their name and address to officially record their presence at the meeting. This record serves as a formal documentation of who participated in the general assembly and can be used for various purposes such as quorum verification, voting eligibility, and communication with the members.",
              "prompt": {
                "meta": {},
                "user": "How are the names and addresses of the members present or represented recorded during a general assembly?",
                "system": "Contexte :  BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. \n\n 3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. \n\n However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. \n\n 52■Transformers for Machine Learning: A Deep Dive\nBERT sentence representation can also be obtained by pooling the\nindividual token representations. However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. \n\n ),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT. \n\n At a glance, we can see that the most signiﬁcant\nnegativeattributionsforthebaseBERTmodelcomefromkidney-related\nterms(kidney,renal,etc.),possiblyindicatingthatthebaseBERTmodel\nassociates such terms with a class other than Surgery. The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. \n\n The domain-\nadapted Bio_Clinical BERT has positive and negative attributions for\nbladderas well as positive explanations for the descriptions of the pro-\ncedures (e.g., “placed in dorsal lithotomy position” and “prepped and\ndraped in the standard fashion”). This may give us some insight into\nhow the additional pre-training is helping Bio_Clinical BERT achieve a\nrelatively better performance.\nIn the example as shown in Figs. 7.14 and 7.12, we can see that\nthe BIO_ClinicalBERT correctly predicts Medical Records while\nthe base BERT predicts Internal Medicine. Highlights on context\nwords such as “past medical history”, “present”, “admitted”, etc. in the\nBIO_ClinicalBERTsaliencyvisualizationfurtherhighlightsreasonswhy\nMedical Records is the predicted class for the BIO_ClinicalBERT.\nWe can also visualize the self-attention weights to gain another view \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1. \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1.\n7.3.1 Visualization Methods\nVisualization methods reveal an explanation by highlighting the in-\nﬂuence between the inputs and the outputs in a black-box model as\nshown in Fig. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n As mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. \n\n For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows. The ultimate outputs of layers of such horizontal and vertical self-\nattention are distributed representations of utterance tokens and table\ncolumns,whichmaybeutilizedtocalculatethedatabasequerybydown-\nstream semantic parsers. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above. \n\n Bibliography ■249\ntional Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26–30, 2020, OpenReview.net, 2020.\n[264]A. Warstadt, A. Singh, and S. R. Bowman ,Neural network\nacceptability judgments , arXiv preprint arXiv:1805.12471, (2018).\n[265]D. Watts and S. Strogatz ,Collective dynamics of ‘small-\nworld’ networks , Nature, 393 (1998), pp. 440–442.\n[266]X. Wei, R. Weng, Y. Hu, L. Xing, H. Yu, and W. Luo ,\nOn learning universal representations across languages , in 9th In-\nternational Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3–7, 2021, OpenReview.net, 2021. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Without a loss in generality, we can rewrite (5.30) as a sum over an\nexpanded set of key positions, ˜Si⊇Si, that can include positions that\nqidoes not attend to:\nai=\nj∈˜Siexp(\nqikT\nj√dk−m(i,Si)−logZ(i,Si)(\nvj(5.31)\nm(i,Si) ={∞, j /∈Si\n0,otherwise(5.32)\nThe termm(i,Si)in (5.31) is a masking term that ensures that key\npositions that qidoesnotattend to does not contribute to the sum.\nAs mentioned above, the set Siis the set of key positions that query i\nattends to. Under the LSH scheme deﬁned above, Sishould only contain\nkey positions that hash into the same bucket as the query, or in other\nwords\nSi={j:h(qi) =h(kj)} (5.33)\nA priori, there are no guarantees that a query will have any keys to\nattend to. \n\n Transformer Modiﬁcations ■133\nwill only have directed edges with the vertices that correspond to the\nkey positions that it can attend to, as described by the set Siin (5.27).\nAdjacency matrix The graph representation casts the problem of\nreducing the complexity of the attention mechanism as a graph spar-\nsiﬁcation problem, which can be tackled with graph theory. Big Bird\ndescribes the attention graph with its L×Ladjacency matrix, A:\nA(i,j ) ={1, j∈Si\n0,otherwise(5.55)\nWhen the adjacency matrix is all ones, A(i,j ) = 1,∀(i,j), then we\nhave a fully connected graph where each vertex is connected to every\nother vertex and an attention mechanism with quadratic complexity.\nWheneverA(i,j ) = 0, it means that edge (i,j)does not exist and thus\nquery position i(qi) cannot attend to key position j(kj). \n\n \n La question est How are the names and addresses of the members present or represented recorded during a general assembly?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:15.248394",
              "duration": 3.029474,
              "cost": 0.0022875
            },
            "meta": {},
            "text": "During a general assembly, the names and addresses of the members present or represented are typically recorded in a sign-in sheet or attendance register. Each member is required to sign or mark their attendance along with their name and address to officially record their presence at the meeting. This record serves as a formal documentation of who participated in the general assembly and can be used for various purposes such as quorum verification, voting eligibility, and communication with the members.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: During a general assembly, the names and addresses of the members present or represented are typically recorded in a sign-in sheet or attendance register. Each member is required to sign or mark their attendance along with their name and address to officially record their presence at the meeting. This record serves as a formal documentation of who participated in the general assembly and can be used for various purposes such as quorum verification, voting eligibility, and communication with the members.\n\n1. 1. The text does not explain how to record the names of members at a general assembly.\n2. 2. There is no information on how to document the addresses of attendees at a general assembly.\n3. 3. The method of recording the presence or representation of members at a general assembly is not detailed in the text.\n4. 4. The text lacks instructions on how to keep track of members who are represented at a general assembly.\n5. 5. There is no mention of how to register the names and addresses of members during a general assembly in the text.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:19.785452",
                "duration": 1.692291,
                "cost": 0.01011
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not specify a maximum limit for the policyholder's responsibility for additional charges.\n2. The policyholder may be held responsible for increases in fiscal and similar burdens, according to the text.\n3. The text does not provide details on the policyholder's liability for charges beyond a certain limit.\n4. The text mentions that the policyholder could be liable for fiscal increases and similar burdens.\n5. The text lacks information on the upper limit of additional charges for which the policyholder can be held accountable.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:19.274675",
          "duration": 6.418215,
          "cost": 0.01071
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not specify a maximum limit for the policyholder's responsibility for additional charges."
          },
          {
            "meta": {},
            "text": "2. The policyholder may be held responsible for increases in fiscal and similar burdens, according to the text."
          },
          {
            "meta": {},
            "text": "3. The text does not provide details on the policyholder's liability for charges beyond a certain limit."
          },
          {
            "meta": {},
            "text": "4. The text mentions that the policyholder could be liable for fiscal increases and similar burdens."
          },
          {
            "meta": {},
            "text": "5. The text lacks information on the upper limit of additional charges for which the policyholder can be held accountable."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 18.274087880861522,
              "Node id": "b8c98a17-12b9-47dd-90c1-020e1c61db21"
            },
            "text": "This behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k. There are two key results:\n1. Layerwise parameter sharing performs the best.\n2. Performance when using layerwise sharing is nearly identical to\nthe performance of using no parameter sharing."
          },
          {
            "meta": {
              "score": 17.44006825668054,
              "Node id": "31a260be-bb68-4a3f-ac10-2c99e33e5cd6"
            },
            "text": "The Linformer speed remained largely constant and was\nsigniﬁcantly faster for long sequences. Wang et al. kept the sequence\nlength ﬁxed and varied the batch size to process the text sequences.\nThis behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k."
          },
          {
            "meta": {
              "score": 16.250467775351527,
              "Node id": "fedec7ab-4c78-43e4-9c34-76db4c401ae5"
            },
            "text": "Due to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B. When the sequence length is de-\ncreases but the depth is not increased, performance decreases on GLUE\ntext classiﬁcation datasets."
          },
          {
            "meta": {
              "score": 13.255934257036015,
              "Node id": "962beea3-b756-46f5-9176-292deb4bfd1b"
            },
            "text": "The expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x.\nPerformance DeLighT performed as well as or better than the stan-\ndard transformer on machine translation, despite having considerably\nfewer parameters. But, when DeLighT is given more parameters, it out-\nperforms the standard transformer. It also obtains similar or better qual-\nity to SOTA models on machine translation. Also, performance increases\nwith the number of network parameters in the DeLighT block."
          },
          {
            "meta": {
              "score": 13.119975019333122,
              "Node id": "d4667095-1eda-47ce-8a4e-4d81a81ec024"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 12.818996526254766,
              "Node id": "35156b1d-13d4-4501-9b55-a5ac06e4457c"
            },
            "text": "Regarding aspects of semantic knowledge, BERT has shown some\nknowledge for semantic roles. Also BERT encodes information about\nentity types, relations, semantic roles, and proto-roles. However, it was\nfound that BERT struggles with representations of numbers. Research\nprobing into BERT’s world knowledge capabilities showed that, for some\nrelation types, vanilla BERT is competitive with methods relying on\nknowledge bases. However, BERT cannot reason based on its world\nknowledge. Essentially BERT can “guess” the aﬀordances and properties\nof many objects, but does not have the information about their interac-\ntions (e.g., it “knows” that people can walk into houses, and that houses\nare big, but it cannot infer that houses are bigger than people.)\nAdditionally, BERTology is concerned with the localization of\nlinguistic information inside the BERT architecture, either at the"
          },
          {
            "meta": {
              "score": 12.647442096521726,
              "Node id": "0c1dbf3c-f46f-46a8-9774-def66cf09e36"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 12.083310348758808,
              "Node id": "deb0812f-346b-43c5-9122-1828440da556"
            },
            "text": "This acts as a\nresidual connection. They then add a few transformer layers on top of g\nfor it to learn how to best combine the features.\nScaling ThetimecomplexityofFunnel-Transformeris O(d·T2+T·d2).\nSinceTdecreases by half at successive encoder layers, the complexity\ndecreases by a factor of four for each layer. Since O(T·d2)has the large\nconstantd2, it tends to dominate, providing a linear speedup, instead of\na quadratic one.\nDue to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B."
          },
          {
            "meta": {
              "score": 11.904051188650161,
              "Node id": "56f57d30-7b37-4891-bcda-f0686319582c"
            },
            "text": "Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost."
          },
          {
            "meta": {
              "score": 11.730194510915888,
              "Node id": "a9ad2ace-ee42-4ac5-9765-cef4c7acf9e4"
            },
            "text": "The dovector is what is then pro-\njected in the query, key, and value spaces. DeLighT blocks closer to the\noutput of the model are made wider and deeper than those closer to the\nmodel input. DeLightT used do=d/2and also shuﬄes the features be-\ntween the groups in the GLT, using a mixer connection [182] to combine\nthe shuﬄed features with the input, similar to a residual connection.\nThe expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x."
          },
          {
            "meta": {
              "score": 0.733475909870215,
              "Node id": "845269f1-acb6-47f7-908c-905f8cee14f1"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 0.7315582150947759,
              "Node id": "6739a344-7e2c-49a9-93de-145dc7dfdd1c"
            },
            "text": "i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),"
          },
          {
            "meta": {
              "score": 0.7308952562115307,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7303514797981461,
              "Node id": "e6fa195f-abb2-4d27-8161-b48023814c1a"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other."
          },
          {
            "meta": {
              "score": 0.7292929757991804,
              "Node id": "50610582-b112-4f31-a319-d19106cdf74d"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used."
          },
          {
            "meta": {
              "score": 0.729082828365496,
              "Node id": "eb120220-ab1b-4060-81f8-e90714516c7e"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          },
          {
            "meta": {
              "score": 0.7287652142719345,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7279276550720208,
              "Node id": "4d65ee66-c6ab-42f3-adfc-f4c07a168587"
            },
            "text": "The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data."
          },
          {
            "meta": {
              "score": 0.7276956670996424,
              "Node id": "bcb8550a-88e6-43a8-8207-35352fdef7c1"
            },
            "text": "Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7."
          },
          {
            "meta": {
              "score": 0.7255866246092705,
              "Node id": "e0c3a866-0895-4808-b1f1-f9426598c3ba"
            },
            "text": "the edges as the attention weights; the maximum ﬂow algorithm can\ncompute the maximum attention ﬂow from any node of any layer to the\ninput nodes(tokens).\nFig. 7.6 shows attention ﬂow computation between an input token\n(source) and the output using the capacity (minimum weights).\nThe research shows that the two methods give complementary view-\npoints, and compared to draw attention, both yield higher correlations\nwith importance scores of input tokens using gradient-based and the\nablation method.\n7.6 CASE STUDY: TEXT CLASSIFICATION WITH EXPLAIN-\nABILITY\n7.6.1 Goal\nIn the healthcare domain, the transparency and interpretability of ma-\nchine learning models are imperative for their adoption. This section will\ngo through a use case using explainability techniques on state-of-the-\nart transformers to illustrate the usefulness of explainability techniques.\nThe goal is to compare a simple traditional interpretable machine learn-\ning algorithm like logistic regression to the state-of-the-art BERT and\nClinical BERT to see the performance gain and application of post-hoc\ntechniques to explain the black-box models."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens.",
              "prompt": {
                "meta": {},
                "user": "What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?",
                "system": "Contexte :  This behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k. There are two key results:\n1. Layerwise parameter sharing performs the best.\n2. Performance when using layerwise sharing is nearly identical to\nthe performance of using no parameter sharing. \n\n The Linformer speed remained largely constant and was\nsigniﬁcantly faster for long sequences. Wang et al. kept the sequence\nlength ﬁxed and varied the batch size to process the text sequences.\nThis behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k. \n\n Due to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B. When the sequence length is de-\ncreases but the depth is not increased, performance decreases on GLUE\ntext classiﬁcation datasets. \n\n The expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x.\nPerformance DeLighT performed as well as or better than the stan-\ndard transformer on machine translation, despite having considerably\nfewer parameters. But, when DeLighT is given more parameters, it out-\nperforms the standard transformer. It also obtains similar or better qual-\nity to SOTA models on machine translation. Also, performance increases\nwith the number of network parameters in the DeLighT block. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Regarding aspects of semantic knowledge, BERT has shown some\nknowledge for semantic roles. Also BERT encodes information about\nentity types, relations, semantic roles, and proto-roles. However, it was\nfound that BERT struggles with representations of numbers. Research\nprobing into BERT’s world knowledge capabilities showed that, for some\nrelation types, vanilla BERT is competitive with methods relying on\nknowledge bases. However, BERT cannot reason based on its world\nknowledge. Essentially BERT can “guess” the aﬀordances and properties\nof many objects, but does not have the information about their interac-\ntions (e.g., it “knows” that people can walk into houses, and that houses\nare big, but it cannot infer that houses are bigger than people.)\nAdditionally, BERTology is concerned with the localization of\nlinguistic information inside the BERT architecture, either at the \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n This acts as a\nresidual connection. They then add a few transformer layers on top of g\nfor it to learn how to best combine the features.\nScaling ThetimecomplexityofFunnel-Transformeris O(d·T2+T·d2).\nSinceTdecreases by half at successive encoder layers, the complexity\ndecreases by a factor of four for each layer. Since O(T·d2)has the large\nconstantd2, it tends to dominate, providing a linear speedup, instead of\na quadratic one.\nDue to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B. \n\n Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost. \n\n The dovector is what is then pro-\njected in the query, key, and value spaces. DeLighT blocks closer to the\noutput of the model are made wider and deeper than those closer to the\nmodel input. DeLightT used do=d/2and also shuﬄes the features be-\ntween the groups in the GLT, using a mixer connection [182] to combine\nthe shuﬄed features with the input, similar to a residual connection.\nThe expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n the edges as the attention weights; the maximum ﬂow algorithm can\ncompute the maximum attention ﬂow from any node of any layer to the\ninput nodes(tokens).\nFig. 7.6 shows attention ﬂow computation between an input token\n(source) and the output using the capacity (minimum weights).\nThe research shows that the two methods give complementary view-\npoints, and compared to draw attention, both yield higher correlations\nwith importance scores of input tokens using gradient-based and the\nablation method.\n7.6 CASE STUDY: TEXT CLASSIFICATION WITH EXPLAIN-\nABILITY\n7.6.1 Goal\nIn the healthcare domain, the transparency and interpretability of ma-\nchine learning models are imperative for their adoption. This section will\ngo through a use case using explainability techniques on state-of-the-\nart transformers to illustrate the usefulness of explainability techniques.\nThe goal is to compare a simple traditional interpretable machine learn-\ning algorithm like logistic regression to the state-of-the-art BERT and\nClinical BERT to see the performance gain and application of post-hoc\ntechniques to explain the black-box models. \n\n \n La question est What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:17.922442",
              "duration": 2.670098,
              "cost": 0.12882000000000002
            },
            "meta": {},
            "text": "The text does not provide information on the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens.\n\n1. 1. The text does not specify a maximum limit for the policyholder's responsibility for additional charges.\n2. 2. The policyholder may be held responsible for increases in fiscal and similar burdens, according to the text.\n3. 3. The text does not provide details on the policyholder's liability for charges beyond a certain limit.\n4. 4. The text mentions that the policyholder could be liable for fiscal increases and similar burdens.\n5. 5. The text lacks information on the upper limit of additional charges for which the policyholder can be held accountable.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:22.016425",
                "duration": 2.227835,
                "cost": 0.0084
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens, is typically defined in insurance policies as the \"policy limit\" or \"coverage limit.\" This limit represents the maximum amount that the insurance company will pay out for covered claims or expenses under the policy. Once the policy limit is reached, the policyholder is generally not responsible for any additional charges, except for specific circumstances such as increases in fiscal burdens or similar conditions as outlined in the policy terms and conditions.",
              "prompt": {
                "meta": {},
                "user": "What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?",
                "system": "Contexte :  This behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k. There are two key results:\n1. Layerwise parameter sharing performs the best.\n2. Performance when using layerwise sharing is nearly identical to\nthe performance of using no parameter sharing. \n\n The Linformer speed remained largely constant and was\nsigniﬁcantly faster for long sequences. Wang et al. kept the sequence\nlength ﬁxed and varied the batch size to process the text sequences.\nThis behavior is expected since the sequence length was held ﬁxed.\nTo test performance, Linformer was trained in the same way as\nRoBERTa [170] and compared using validation perplexities and perfor-\nmance on downstream tasks. Based on perplexities, Linformer perfor-\nmance increases as k increases and Linformer’s quality nearly matches\nthat of the standard transformer for L= 512,k= 128andL=\n1024,k = 256. Based on downstream task performance, Linformer per-\nformance is comparable to RoBERTa when L= 512,k = 128, and out-\nperforms RoBERTa at k= 256. Performance for L= 1024,k = 256and\nL= 512,k= 256are similar, suggesting that performance for Linformer\nis controlled more by kthan byL/k. \n\n Due to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B. When the sequence length is de-\ncreases but the depth is not increased, performance decreases on GLUE\ntext classiﬁcation datasets. \n\n The expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x.\nPerformance DeLighT performed as well as or better than the stan-\ndard transformer on machine translation, despite having considerably\nfewer parameters. But, when DeLighT is given more parameters, it out-\nperforms the standard transformer. It also obtains similar or better qual-\nity to SOTA models on machine translation. Also, performance increases\nwith the number of network parameters in the DeLighT block. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n Regarding aspects of semantic knowledge, BERT has shown some\nknowledge for semantic roles. Also BERT encodes information about\nentity types, relations, semantic roles, and proto-roles. However, it was\nfound that BERT struggles with representations of numbers. Research\nprobing into BERT’s world knowledge capabilities showed that, for some\nrelation types, vanilla BERT is competitive with methods relying on\nknowledge bases. However, BERT cannot reason based on its world\nknowledge. Essentially BERT can “guess” the aﬀordances and properties\nof many objects, but does not have the information about their interac-\ntions (e.g., it “knows” that people can walk into houses, and that houses\nare big, but it cannot infer that houses are bigger than people.)\nAdditionally, BERTology is concerned with the localization of\nlinguistic information inside the BERT architecture, either at the \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n This acts as a\nresidual connection. They then add a few transformer layers on top of g\nfor it to learn how to best combine the features.\nScaling ThetimecomplexityofFunnel-Transformeris O(d·T2+T·d2).\nSinceTdecreases by half at successive encoder layers, the complexity\ndecreases by a factor of four for each layer. Since O(T·d2)has the large\nconstantd2, it tends to dominate, providing a linear speedup, instead of\na quadratic one.\nDue to the complexity reduction aﬀorded by the pooling between\nlayers, it is possible to add additional encoder layers or make existing\nlayers wider without increasing the computational load in any signiﬁcant\nmanner.\nPerformance Three sizes of the standard transformer were compared\nto several conﬁgurations of Funnel-Transformer, each with fewer or sim-\nilar numbers of expected ﬂoating-point operations to the transformer to\nwhich it was being compared:\n•large: 24 layers, d= 1024\n•base: 12 layers, d= 768\n•small: 6 layers, d= 768\nQuality comparisons are made for GLUE and when Funnel-\nTransformer decreases the sequence length and adds more layers, it per-\nforms better than the standard transformer on text classiﬁcation and\nall GLUE datasets except for STS-B. \n\n Performance Talking-Heads Attention (THA) was evaluated by\ntraining a T5 model using THA and the same hyperparameters as the\nT5 paper (except for omitting dropout during pre-training) and simi-\nlarly training an ALBERT model. The authors found that THA consis-\ntently performed better than multi-head attention [227]. Just projecting\nthe attention logits or just the attention weights is only slightlybetter\nthan using pure multi-head attention. Signiﬁcant performance increases\ncame from using both projections. Using talking-heads attention on the\nencoder block’s self-attention layers has a larger eﬀect on model perfor-\nmance than using talking-heads attention on the decoder block’s atten-\ntion layers.\nMulti-headattentionalreadycomesatacostandtheprojectionsthat\nTHA adds, WαandWA, increase that cost. \n\n The dovector is what is then pro-\njected in the query, key, and value spaces. DeLighT blocks closer to the\noutput of the model are made wider and deeper than those closer to the\nmodel input. DeLightT used do=d/2and also shuﬄes the features be-\ntween the groups in the GLT, using a mixer connection [182] to combine\nthe shuﬄed features with the input, similar to a residual connection.\nThe expansion phase of the DeLighT block is shown in Fig. 5.3.\nMehta et al. proposed that by increasing the depth of the DeLighT\nblock and the width of its intermediate GLT layers, the transformer\nwill have an increased representational capacity and that one can then\nreplace multi-head attention with single-head attention. Similarly, they\npropose that the DeLighT block’s wide linear layers let one decrease the\nsize of the feedforward layer by up to 16x. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n i.e. the resulting rule or anchor, such that A(x) = 1 ,f\nrepresents the black-box model, Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A), \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Interpretability and Explainability Techniques for Transformers ■205\nJensen-Shannon Divergence is used to measure the diﬀerence between\nthe distributions. The optimization equation is given by:\nmaximizeα(1),···,α(k)f({α(i)}k\ni=1) (7.20)\nwheref({α(i)}k\ni=1)is:\nk\ni=1JSD[α(i),ˆα] +1\nk(k−1)k\ni<jJSD[α(i),α(j)](7.21)\nThe ﬁrst part of the equation∑k\ni=1JSD[α(i),ˆα]ﬁnds maximally\ndiﬀerent attention from the observed ˆαand the second part\n1\nk(k−1)∑k\ni<jJSD[α(i),α(j)is maximally diﬀerent from each other. \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n The word\ncloud (Fig. 7.7) and top frequency words plot (Fig. 7.9) clearly show\nbias towards terms such as diagnoses, injury,chronic, etc., which form\nthe base language for most medical transcriptions and records. The doc-\nument length distribution plot (Fig. 7.8) shows a long-tailed distribution\nwith greater than 25% documents falling beyond the maximum sequence\nlength for BERT. Finally, the conversion of 40classes to 4categories\n(Fig. 7.10) shows almost a balanced distribution and becomes helpful in\nclassiﬁer comparisons as the impact of imbalance is now minimized.\n7.6.3.2 Experiments\nWe use the 90–10% split of training and testing and further create a\nvalidation set of 10% for hyperparameter and learning curves. We will\nﬁrst compare the base BERT with BIO_ClinicalBERT both ﬁne-tuned\non the training data and evaluated on test data. \n\n Drepresent a known distribution\nof perturbations, zis a sample drawn from the conditional distri-\nbution when rule AappliesD(z|A), andτis the given precision\nthreshold, then the anchor deﬁnition is given by:\nED(z|A)\n⊮f(x)=f (z)\n≥τ, A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7. \n\n the edges as the attention weights; the maximum ﬂow algorithm can\ncompute the maximum attention ﬂow from any node of any layer to the\ninput nodes(tokens).\nFig. 7.6 shows attention ﬂow computation between an input token\n(source) and the output using the capacity (minimum weights).\nThe research shows that the two methods give complementary view-\npoints, and compared to draw attention, both yield higher correlations\nwith importance scores of input tokens using gradient-based and the\nablation method.\n7.6 CASE STUDY: TEXT CLASSIFICATION WITH EXPLAIN-\nABILITY\n7.6.1 Goal\nIn the healthcare domain, the transparency and interpretability of ma-\nchine learning models are imperative for their adoption. This section will\ngo through a use case using explainability techniques on state-of-the-\nart transformers to illustrate the usefulness of explainability techniques.\nThe goal is to compare a simple traditional interpretable machine learn-\ning algorithm like logistic regression to the state-of-the-art BERT and\nClinical BERT to see the performance gain and application of post-hoc\ntechniques to explain the black-box models. \n\n \n La question est What is the maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:21.227529",
              "duration": 3.301902,
              "cost": 0.0022724999999999998
            },
            "meta": {},
            "text": "The maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens, is typically defined in insurance policies as the \"policy limit\" or \"coverage limit.\" This limit represents the maximum amount that the insurance company will pay out for covered claims or expenses under the policy. Once the policy limit is reached, the policyholder is generally not responsible for any additional charges, except for specific circumstances such as increases in fiscal burdens or similar conditions as outlined in the policy terms and conditions.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The maximum limit beyond which the policyholder cannot be held responsible for additional charges, except for increases in fiscal and similar burdens, is typically defined in insurance policies as the \"policy limit\" or \"coverage limit.\" This limit represents the maximum amount that the insurance company will pay out for covered claims or expenses under the policy. Once the policy limit is reached, the policyholder is generally not responsible for any additional charges, except for specific circumstances such as increases in fiscal burdens or similar conditions as outlined in the policy terms and conditions.\n\n1. 1. The text does not specify a maximum limit for the policyholder's responsibility for additional charges.\n2. 2. The policyholder may be held responsible for increases in fiscal and similar burdens, according to the text.\n3. 3. The text does not provide details on the policyholder's liability for charges beyond a certain limit.\n4. 4. The text mentions that the policyholder could be liable for fiscal increases and similar burdens.\n5. 5. The text lacks information on the upper limit of additional charges for which the policyholder can be held accountable.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:23.239049",
                "duration": 1.219953,
                "cost": 0.010050000000000002
              },
              "meta": {},
              "text": "Validated: ['2', '4'] - Not validated: ['1', '3', '5']",
              "human": 1.0,
              "auto": 0.4
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the role of the board of directors in determining the remuneration of the director general?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not discuss the board of directors' role in setting the director general's salary.\n2. There is no information in the text about how the director general's remuneration is determined by the board of directors.\n3. The text lacks details on the involvement of the board of directors in deciding the director general's pay.\n4. The role of the board of directors in deciding the director general's remuneration is not mentioned in the text.\n5. The text does not provide insight into the board of directors' responsibility in determining the director general's compensation.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the role of the board of directors in determining the remuneration of the director general.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:28.298091",
          "duration": 9.020278000000001,
          "cost": 0.01122
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not discuss the board of directors' role in setting the director general's salary."
          },
          {
            "meta": {},
            "text": "2. There is no information in the text about how the director general's remuneration is determined by the board of directors."
          },
          {
            "meta": {},
            "text": "3. The text lacks details on the involvement of the board of directors in deciding the director general's pay."
          },
          {
            "meta": {},
            "text": "4. The role of the board of directors in deciding the director general's remuneration is not mentioned in the text."
          },
          {
            "meta": {},
            "text": "5. The text does not provide insight into the board of directors' responsibility in determining the director general's compensation."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 15.486336149216232,
              "Node id": "97a28007-9c62-4423-ab2b-32ac3abc1252"
            },
            "text": "154■Transformers for Machine Learning: A Deep Dive\nhas weights WO, giving the mechanism the following output:\nyi=\nL\nj=1Aijv(xj)(\n(WO (5.76)\n=L\nj=1Aijf(xj) (5.77)\nwhereAijare the attention weights from (5.27), v(xj)is a value\nvector from 5.25, and f(xj) =v(xj)WO.f(xj)is the output of the\nattention mechanism corresponding to the jthinput token xj.\nIn [143], Kobayashi et al. speciﬁcally suggest using ||Aijf(xj)||as\nit’s the degree to which token iattends to token j. They also show that\n||f(xj)||andAijare meaningful quantities and sometimes play opposing\nroles in determining model behavior."
          },
          {
            "meta": {
              "score": 14.19591294056454,
              "Node id": "b972029a-17e1-4be6-9286-e3b971eaa561"
            },
            "text": "Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi."
          },
          {
            "meta": {
              "score": 13.814073206556625,
              "Node id": "3b3dba8f-3cda-4fa1-8838-09bf9e9e4a3f"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3."
          },
          {
            "meta": {
              "score": 13.435694998452057,
              "Node id": "5560665a-f106-4ea9-a1ff-3d84207a0a44"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi."
          },
          {
            "meta": {
              "score": 11.842135971020177,
              "Node id": "c03d1928-c5c5-496e-ad66-e3d56b105517"
            },
            "text": "self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1."
          },
          {
            "meta": {
              "score": 10.809508210808692,
              "Node id": "3f83d614-c6b4-4b90-91f5-0847807c29c6"
            },
            "text": "Performance Clustered attention outperforms standard transformer\nand Reformer on automatic speech recognition with WSJ and Switch-\nboard audio datasets. It also approximates pre-trained RoBERTa on\nGLUE and SQuAD, with minute performance loss. It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer."
          },
          {
            "meta": {
              "score": 10.603185301645581,
              "Node id": "4d27ca73-4d1b-4cc2-bef2-0504089c7003"
            },
            "text": "In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration."
          },
          {
            "meta": {
              "score": 10.125490645335946,
              "Node id": "e3e8a5a1-ec07-468b-b421-10a340ee05b0"
            },
            "text": "BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it."
          },
          {
            "meta": {
              "score": 10.060739730920542,
              "Node id": "dd28f67d-5977-44c2-accc-a3b04b1a0c4c"
            },
            "text": "In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration. Addi-\ntionally, it was shown that BERT does not \"understand\" negation and\nis indiﬀerent to input that is faulty."
          },
          {
            "meta": {
              "score": 9.93369095952192,
              "Node id": "0ccf97f8-8c48-4899-8424-be8f48c76d7c"
            },
            "text": "3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree."
          },
          {
            "meta": {
              "score": 0.7235785830156869,
              "Node id": "55ff27c9-0d8d-492c-99f3-014cd0d0b3e1"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies."
          },
          {
            "meta": {
              "score": 0.7202690244359997,
              "Node id": "f4d2ab08-3665-4baf-9913-cdd36a2effd9"
            },
            "text": "provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]."
          },
          {
            "meta": {
              "score": 0.717011551401859,
              "Node id": "fc6213b4-f9ff-42ae-b0a5-550b62970007"
            },
            "text": "He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization."
          },
          {
            "meta": {
              "score": 0.7167857918168412,
              "Node id": "ac02af39-b35b-4b3b-9117-9296aca82f75"
            },
            "text": "Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance."
          },
          {
            "meta": {
              "score": 0.7156831545312851,
              "Node id": "6ae11509-009f-423f-a3a1-4d8e649b3d90"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?"
          },
          {
            "meta": {
              "score": 0.7156055739191356,
              "Node id": "3717f92d-e7f5-4240-a356-a4166eeaafca"
            },
            "text": "The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]."
          },
          {
            "meta": {
              "score": 0.7152154684430789,
              "Node id": "d3f527c5-3391-4393-a503-936de230a06a"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights."
          },
          {
            "meta": {
              "score": 0.7113622778790017,
              "Node id": "0e39081e-4d01-48bf-b303-80e22fbed6ea"
            },
            "text": "He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry."
          },
          {
            "meta": {
              "score": 0.7098641254987461,
              "Node id": "4252db37-df5d-472e-8796-fdc98f630413"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention."
          },
          {
            "meta": {
              "score": 0.7096744184522878,
              "Node id": "7591794c-b05c-4ad3-bbfa-83aa41803b93"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the role of the board of directors in determining the remuneration of the director general.",
              "prompt": {
                "meta": {},
                "user": "What is the role of the board of directors in determining the remuneration of the director general?",
                "system": "Contexte :  154■Transformers for Machine Learning: A Deep Dive\nhas weights WO, giving the mechanism the following output:\nyi=\nL\nj=1Aijv(xj)(\n(WO (5.76)\n=L\nj=1Aijf(xj) (5.77)\nwhereAijare the attention weights from (5.27), v(xj)is a value\nvector from 5.25, and f(xj) =v(xj)WO.f(xj)is the output of the\nattention mechanism corresponding to the jthinput token xj.\nIn [143], Kobayashi et al. speciﬁcally suggest using ||Aijf(xj)||as\nit’s the degree to which token iattends to token j. They also show that\n||f(xj)||andAijare meaningful quantities and sometimes play opposing\nroles in determining model behavior. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi. \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Performance Clustered attention outperforms standard transformer\nand Reformer on automatic speech recognition with WSJ and Switch-\nboard audio datasets. It also approximates pre-trained RoBERTa on\nGLUE and SQuAD, with minute performance loss. It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer. \n\n In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration. \n\n BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. \n\n In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration. Addi-\ntionally, it was shown that BERT does not \"understand\" negation and\nis indiﬀerent to input that is faulty. \n\n 3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n \n La question est What is the role of the board of directors in determining the remuneration of the director general?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:23.771514",
              "duration": 2.539862,
              "cost": 0.1308
            },
            "meta": {},
            "text": "The text does not provide information on the role of the board of directors in determining the remuneration of the director general.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the role of the board of directors in determining the remuneration of the director general.\n\n1. 1. The text does not discuss the board of directors' role in setting the director general's salary.\n2. 2. There is no information in the text about how the director general's remuneration is determined by the board of directors.\n3. 3. The text lacks details on the involvement of the board of directors in deciding the director general's pay.\n4. 4. The role of the board of directors in deciding the director general's remuneration is not mentioned in the text.\n5. 5. The text does not provide insight into the board of directors' responsibility in determining the director general's compensation.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:25.654196",
                "duration": 2.411565,
                "cost": 0.00855
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The board of directors plays a crucial role in determining the remuneration of the director general. Their responsibilities typically include:\n\n1. **Setting Compensation Policies:** The board establishes the overall compensation philosophy and policies for the organization, including guidelines for executive compensation.\n\n2. **Approving Compensation Packages:** The board reviews and approves the compensation package of the director general, which may include salary, bonuses, stock options, benefits, and other forms of remuneration.\n\n3. **Ensuring Fairness and Alignment:** The board ensures that the director general's compensation is fair, competitive, and aligned with the organization's goals, performance, and industry standards.\n\n4. **Performance Evaluation:** The board evaluates the performance of the director general against predetermined goals and benchmarks to determine if the compensation is justified.\n\n5. **Engaging Compensation Committees:** In some cases, boards may delegate compensation-related tasks to a specialized committee, such as the compensation committee, which makes recommendations to the full board.\n\n6. **Transparency and Disclosure:** Boards are responsible for ensuring transparency in executive compensation practices and disclosing relevant information to stakeholders, including shareholders.\n\nOverall, the board of directors plays a critical role in ensuring that the director general's remuneration is appropriate, competitive, and in line with the organization's objectives and values.",
              "prompt": {
                "meta": {},
                "user": "What is the role of the board of directors in determining the remuneration of the director general?",
                "system": "Contexte :  154■Transformers for Machine Learning: A Deep Dive\nhas weights WO, giving the mechanism the following output:\nyi=\nL\nj=1Aijv(xj)(\n(WO (5.76)\n=L\nj=1Aijf(xj) (5.77)\nwhereAijare the attention weights from (5.27), v(xj)is a value\nvector from 5.25, and f(xj) =v(xj)WO.f(xj)is the output of the\nattention mechanism corresponding to the jthinput token xj.\nIn [143], Kobayashi et al. speciﬁcally suggest using ||Aijf(xj)||as\nit’s the degree to which token iattends to token j. They also show that\n||f(xj)||andAijare meaningful quantities and sometimes play opposing\nroles in determining model behavior. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi. \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Performance Clustered attention outperforms standard transformer\nand Reformer on automatic speech recognition with WSJ and Switch-\nboard audio datasets. It also approximates pre-trained RoBERTa on\nGLUE and SQuAD, with minute performance loss. It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer. \n\n In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration. \n\n BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. \n\n In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. In terms of how syntactic information is represented, it\nseems that self-attention weights do not directly encode syntactic struc-\nture, but they may be changed to reﬂect it. When executing the cloze\ntask, BERT takes subject-predicate agreement into consideration. Addi-\ntionally, it was shown that BERT does not \"understand\" negation and\nis indiﬀerent to input that is faulty. \n\n 3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles. BERT’s\nunderstanding of syntax, on the other hand, is incomplete, since probing\nclassiﬁerswereunabletoretrievethelabelsofdistantparentnodesinthe\nsyntactic tree. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8].\n•Integrated Gradients (IG). Integrated gradients by Sundara-\njan et al. is an explanation technique by computing relevance for\na networkfbased on two axioms: sensitivity and implementation\nvariance [236]. The sensitivity axiom: for an input xwhich diﬀers\nfrom some baseline input x′along feature xiandf(x)̸=f(x′)\nthenxishould have a non-zero relevance. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n The relevance score R∆xi∆tassigned\nfor the input feature xiwithNneurons necessary to compute tis\ngiven by:\n∆t=N\ni=1R∆xi∆t (7.5)\nShrikumar et al. provide diﬀerent ways to calculate the weighting\nbetween the inﬂuence ∆xihad on ∆tusingLinearrule,Rescale\nrule and RevealCancel rule. Deﬁning a multiplier m∆x∆ twhich\nmeasures the relevance of ∆xwith respect to ∆t, averaged by ∆x\nas:\nm∆x∆t =R∆x∆t\n∆x(7.6)\nLayer by layer computation of relevance scores using chain rule is\nadopted in the DeepLIFT paper. Arkhangelskaia and Dutta apply\nDeepLIFT on the BERT model predictions and test the outcomes\nto monitor shifts in the attention values for input for the question-\nanswering application [8]. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n \n La question est What is the role of the board of directors in determining the remuneration of the director general?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:30.585734",
              "duration": 6.811062,
              "cost": 0.002542
            },
            "meta": {},
            "text": "The board of directors plays a crucial role in determining the remuneration of the director general. Their responsibilities typically include:\n\n1. **Setting Compensation Policies:** The board establishes the overall compensation philosophy and policies for the organization, including guidelines for executive compensation.\n\n2. **Approving Compensation Packages:** The board reviews and approves the compensation package of the director general, which may include salary, bonuses, stock options, benefits, and other forms of remuneration.\n\n3. **Ensuring Fairness and Alignment:** The board ensures that the director general's compensation is fair, competitive, and aligned with the organization's goals, performance, and industry standards.\n\n4. **Performance Evaluation:** The board evaluates the performance of the director general against predetermined goals and benchmarks to determine if the compensation is justified.\n\n5. **Engaging Compensation Committees:** In some cases, boards may delegate compensation-related tasks to a specialized committee, such as the compensation committee, which makes recommendations to the full board.\n\n6. **Transparency and Disclosure:** Boards are responsible for ensuring transparency in executive compensation practices and disclosing relevant information to stakeholders, including shareholders.\n\nOverall, the board of directors plays a critical role in ensuring that the director general's remuneration is appropriate, competitive, and in line with the organization's objectives and values.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The board of directors plays a crucial role in determining the remuneration of the director general. Their responsibilities typically include:\n\n1. **Setting Compensation Policies:** The board establishes the overall compensation philosophy and policies for the organization, including guidelines for executive compensation.\n\n2. **Approving Compensation Packages:** The board reviews and approves the compensation package of the director general, which may include salary, bonuses, stock options, benefits, and other forms of remuneration.\n\n3. **Ensuring Fairness and Alignment:** The board ensures that the director general's compensation is fair, competitive, and aligned with the organization's goals, performance, and industry standards.\n\n4. **Performance Evaluation:** The board evaluates the performance of the director general against predetermined goals and benchmarks to determine if the compensation is justified.\n\n5. **Engaging Compensation Committees:** In some cases, boards may delegate compensation-related tasks to a specialized committee, such as the compensation committee, which makes recommendations to the full board.\n\n6. **Transparency and Disclosure:** Boards are responsible for ensuring transparency in executive compensation practices and disclosing relevant information to stakeholders, including shareholders.\n\nOverall, the board of directors plays a critical role in ensuring that the director general's remuneration is appropriate, competitive, and in line with the organization's objectives and values.\n\n1. 1. The text does not discuss the board of directors' role in setting the director general's salary.\n2. 2. There is no information in the text about how the director general's remuneration is determined by the board of directors.\n3. 3. The text lacks details on the involvement of the board of directors in deciding the director general's pay.\n4. 4. The role of the board of directors in deciding the director general's remuneration is not mentioned in the text.\n5. 5. The text does not provide insight into the board of directors' responsibility in determining the director general's compensation.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:27.065743",
                "duration": 1.408093,
                "cost": 0.01554
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The expenses covered by an insurance policy can differ.\n2. Accommodation costs may be included in the insurance coverage.\n3. Meals might also be covered by the insurance policy.\n4. Some insurance policies may cover local transportation.\n5. The specific terms and conditions of the insurance policy should be checked for detailed information.",
          "prompt": {
            "meta": {},
            "user": "The specific expenses covered can vary depending on the insurance policy. However, typically, these may include accommodation costs, meals, and sometimes even local transportation. It's important to check the specific terms and conditions of the insurance policy for detailed information.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:33.726873",
          "duration": 5.425255,
          "cost": 0.008790000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The expenses covered by an insurance policy can differ."
          },
          {
            "meta": {},
            "text": "2. Accommodation costs may be included in the insurance coverage."
          },
          {
            "meta": {},
            "text": "3. Meals might also be covered by the insurance policy."
          },
          {
            "meta": {},
            "text": "4. Some insurance policies may cover local transportation."
          },
          {
            "meta": {},
            "text": "5. The specific terms and conditions of the insurance policy should be checked for detailed information."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.709445241397983,
              "Node id": "34ced21d-8ed4-4087-a78f-13469e3869e4"
            },
            "text": "6/60\n Intérieur du véhicule\nHabitacle, dessous du capot et coffre.\n Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement."
          },
          {
            "meta": {
              "score": 11.297082814042106,
              "Node id": "c575fe6d-296a-47a8-9e46-61221b0f270b"
            },
            "text": "Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé."
          },
          {
            "meta": {
              "score": 9.752171381434149,
              "Node id": "200998cd-a57e-4d9a-bb4e-d4dfdbe76a33"
            },
            "text": "II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96"
          },
          {
            "meta": {
              "score": 9.130782338341737,
              "Node id": "d85ab61f-2026-4419-90d4-b3321ef36a75"
            },
            "text": "These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43"
          },
          {
            "meta": {
              "score": 8.888548778666381,
              "Node id": "765fa205-1363-474e-95dd-804eea8cdf96"
            },
            "text": "Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé.\n Préposé\nToute personne travaillant pour le compte du souscripteur.\n Réclamation\nDéclaration actant, par téléphone, par courrier, par courriel ou en face à face, le mécontentement d’un client \nenvers l’assureur."
          },
          {
            "meta": {
              "score": 8.760694175276116,
              "Node id": "88913829-06fa-4457-9dfb-7e8a64fb109a"
            },
            "text": "121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ ."
          },
          {
            "meta": {
              "score": 8.729862644944342,
              "Node id": "0ef7e427-adba-4269-b11d-8e37a62e2ff0"
            },
            "text": "[34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1."
          },
          {
            "meta": {
              "score": 8.685673413525214,
              "Node id": "bc5c5b0a-035b-4d19-a9d0-4c51d5a48160"
            },
            "text": "Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output."
          },
          {
            "meta": {
              "score": 8.610629101968517,
              "Node id": "1cdbefe5-7c66-41f9-bcd7-b70ba5e5107e"
            },
            "text": "Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al."
          },
          {
            "meta": {
              "score": 8.543544344083507,
              "Node id": "a2cdd7de-ded3-43e2-a2a9-d3c37cc19354"
            },
            "text": "sommaire\nProtection Auto - Conditions générales EUR059H-0124\n31Véhicule\nLe véhicule de tourisme terrestre à moteur, d’un \npoids total autorisé en charge inférieur à 3,5 T (au -\ntomobile ou moto > 125 cm3), immatriculé en France \nmétropolitaine et désigné aux Conditions person -\nnelles du contrat d’assurance Automobile, ainsi \nque la caravane ou remorque inscrite sur le contrat \nd’assurance si elle y est attelée au moment de \nl’évènement.\nSont exclus\nles véhicules utilisés pour le transport  \nde passagers à titre onéreux tels que taxis, \nambulances, auto-écoles, corbillards, voitures \nde location."
          },
          {
            "meta": {
              "score": 0.7364724958031004,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7227335673678614,
              "Node id": "173dddf4-c179-4905-af71-765ba3abaaef"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...!"
          },
          {
            "meta": {
              "score": 0.720611830032027,
              "Node id": "195fae6f-18a4-40c0-9cde-68f16d41502b"
            },
            "text": "[96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383."
          },
          {
            "meta": {
              "score": 0.7150513361797922,
              "Node id": "a7be65c3-1b24-4bc4-ad36-e62ff8a1baa5"
            },
            "text": "To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara"
          },
          {
            "meta": {
              "score": 0.7110967955353025,
              "Node id": "eada2f44-1d6a-4882-a349-8816a3a160af"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model."
          },
          {
            "meta": {
              "score": 0.7082622887972815,
              "Node id": "44208246-3890-462a-8494-b45a317f440d"
            },
            "text": "Cross-lingual Word Recovery (CLWR) CLWR [125] uses par-\nallel corpora to learn word alignments between two languages. The\npre-training task starts with a trainable attention matrix [17] that learns\nto represent the source language token embeddings in terms of the tar-\nget language token embeddings. The transformation learned in the at-\ntention matrix is then provided to the cross-lingual model to learn to\nreconstruct the source token embedding. Formally, given a parallel sen-\ntence pair (x,y), wherex= (x1,x2,xm)is a sentence with mwords\nfrom a source language s, andy= (y 1,y2,yn)is a sentence with n\nwords from a target language t. CLWR starts by representing each xias\nxt\ni∈Rhby all word embeddings of y[125]:\nxt\ni=n\nj=1softmax (A ij)yt\nj (4."
          },
          {
            "meta": {
              "score": 0.7082217276297934,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          },
          {
            "meta": {
              "score": 0.706047124282251,
              "Node id": "7c59590c-0309-413b-a5b1-1fdbb8f9b70e"
            },
            "text": "4.1.2.8 ERNIE-M\nMany techniques have shown the value of using parallel and monolin-\ngual corpora to get better aligned cross-lingual representations. How-\never, parallel corpora are always limited in size relative to monolingual\ndata, limiting the possible progress unless we collect sizable parallel cor-\npora. ERNIE-M [194] uses large monolingual corpora and limited par-\nallel corpora to produce cross-lingual representations. To enhance pro-\nducedrepresentations,ERNIE-Mgeneratespseudo-parallelsentencesus-\ning the large available monolingual corpora and then uses them along\nwith the parallel data to improve the alignment of cross-lingual repre-\nsentations. ERNIE-M proposes two pre-training tasks, Cross-Attention\nmasked Language Modeling (CAMLM) and Back-Translation Masked\nLanguage Modeling (BTMLM)."
          },
          {
            "meta": {
              "score": 0.7052843244653489,
              "Node id": "845269f1-acb6-47f7-908c-905f8cee14f1"
            },
            "text": "A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way."
          },
          {
            "meta": {
              "score": 0.7052309252766569,
              "Node id": "a4d58dc2-44dd-46ea-9c3b-2f51d1c131cc"
            },
            "text": "They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The specific expenses covered can vary depending on the insurance policy. However, typically, these may include accommodation costs, meals, and sometimes even local transportation. It's important to check the specific terms and conditions of the insurance policy for detailed information.",
              "prompt": {
                "meta": {},
                "user": "What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?",
                "system": "Contexte :  6/60\n Intérieur du véhicule\nHabitacle, dessous du capot et coffre.\n Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement. \n\n Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé. \n\n II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96 \n\n These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé.\n Préposé\nToute personne travaillant pour le compte du souscripteur.\n Réclamation\nDéclaration actant, par téléphone, par courrier, par courriel ou en face à face, le mécontentement d’un client \nenvers l’assureur. \n\n 121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1. \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. \n\n sommaire\nProtection Auto - Conditions générales EUR059H-0124\n31Véhicule\nLe véhicule de tourisme terrestre à moteur, d’un \npoids total autorisé en charge inférieur à 3,5 T (au -\ntomobile ou moto > 125 cm3), immatriculé en France \nmétropolitaine et désigné aux Conditions person -\nnelles du contrat d’assurance Automobile, ainsi \nque la caravane ou remorque inscrite sur le contrat \nd’assurance si elle y est attelée au moment de \nl’évènement.\nSont exclus\nles véhicules utilisés pour le transport  \nde passagers à titre onéreux tels que taxis, \nambulances, auto-écoles, corbillards, voitures \nde location. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n Cross-lingual Word Recovery (CLWR) CLWR [125] uses par-\nallel corpora to learn word alignments between two languages. The\npre-training task starts with a trainable attention matrix [17] that learns\nto represent the source language token embeddings in terms of the tar-\nget language token embeddings. The transformation learned in the at-\ntention matrix is then provided to the cross-lingual model to learn to\nreconstruct the source token embedding. Formally, given a parallel sen-\ntence pair (x,y), wherex= (x1,x2,xm)is a sentence with mwords\nfrom a source language s, andy= (y 1,y2,yn)is a sentence with n\nwords from a target language t. CLWR starts by representing each xias\nxt\ni∈Rhby all word embeddings of y[125]:\nxt\ni=n\nj=1softmax (A ij)yt\nj (4. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n 4.1.2.8 ERNIE-M\nMany techniques have shown the value of using parallel and monolin-\ngual corpora to get better aligned cross-lingual representations. How-\never, parallel corpora are always limited in size relative to monolingual\ndata, limiting the possible progress unless we collect sizable parallel cor-\npora. ERNIE-M [194] uses large monolingual corpora and limited par-\nallel corpora to produce cross-lingual representations. To enhance pro-\nducedrepresentations,ERNIE-Mgeneratespseudo-parallelsentencesus-\ning the large available monolingual corpora and then uses them along\nwith the parallel data to improve the alignment of cross-lingual repre-\nsentations. ERNIE-M proposes two pre-training tasks, Cross-Attention\nmasked Language Modeling (CAMLM) and Back-Translation Masked\nLanguage Modeling (BTMLM). \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized. \n\n \n La question est What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:34.484104",
              "duration": 3.894349,
              "cost": 0.12618000000000001
            },
            "meta": {},
            "text": "The specific expenses covered can vary depending on the insurance policy. However, typically, these may include accommodation costs, meals, and sometimes even local transportation. It's important to check the specific terms and conditions of the insurance policy for detailed information.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The specific expenses covered can vary depending on the insurance policy. However, typically, these may include accommodation costs, meals, and sometimes even local transportation. It's important to check the specific terms and conditions of the insurance policy for detailed information.\n\n1. 1. The expenses covered by an insurance policy can differ.\n2. 2. Accommodation costs may be included in the insurance coverage.\n3. 3. Meals might also be covered by the insurance policy.\n4. 4. Some insurance policies may cover local transportation.\n5. 5. The specific terms and conditions of the insurance policy should be checked for detailed information.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:28.502348",
                "duration": 1.433539,
                "cost": 0.00768
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Expenses that may be covered for a person waiting for the repatriation of a non-transportable injured individual could include:\n\n1. Accommodation costs: Expenses related to staying in a hotel or temporary accommodation while waiting for repatriation.\n\n2. Meal expenses: Costs for meals and food during the waiting period.\n\n3. Transportation costs: Any necessary transportation expenses incurred during the waiting period.\n\n4. Medical expenses: Costs related to medical care or treatment needed while waiting for repatriation.\n\n5. Communication expenses: Costs for phone calls, internet access, or other communication needs during the waiting period.\n\nIt's important to check the specific terms and conditions of the insurance policy or assistance program to understand the exact coverage provided in such situations.",
              "prompt": {
                "meta": {},
                "user": "What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?",
                "system": "Contexte :  6/60\n Intérieur du véhicule\nHabitacle, dessous du capot et coffre.\n Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement. \n\n Marchandise\nTout objet destiné à être transformé, réparé ou vendu, ainsi que les approvisionnements et emballages, \nà l’exception des biens véhiculés dans le cadre du transport public de marchandises.\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé. \n\n II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96 \n\n These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n Matériel professionnel\nTout objet utilisé pour les besoins de l’activité professionnelle du souscripteur et non fixé au véhicule.\n Mécontentement\nIncompréhension définitive de l’assuré, ou désaccord, sur la réponse ou la solution apportée, avec \nmanifestation d’un sentiment d’injustice subie, exprimée dans le cadre d’une Réclamation. L’injure ou l’insulte \nne sont pas considérées contractuellement comme l’expression d’un mécontentement.\n Nullité\nAnnulation pure et simple du contrat qui est censé alors n’avoir jamais existé.\n Préposé\nToute personne travaillant pour le compte du souscripteur.\n Réclamation\nDéclaration actant, par téléphone, par courrier, par courriel ou en face à face, le mécontentement d’un client \nenvers l’assureur. \n\n 121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . \n\n [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. taxonomy, based on foundational explainability\nmethods, restricted only to generic or speciﬁc transformer-based models,\nis discussed in this section and as shown in Fig. 7.1. \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. The Xie et al. \n\n sommaire\nProtection Auto - Conditions générales EUR059H-0124\n31Véhicule\nLe véhicule de tourisme terrestre à moteur, d’un \npoids total autorisé en charge inférieur à 3,5 T (au -\ntomobile ou moto > 125 cm3), immatriculé en France \nmétropolitaine et désigné aux Conditions person -\nnelles du contrat d’assurance Automobile, ainsi \nque la caravane ou remorque inscrite sur le contrat \nd’assurance si elle y est attelée au moment de \nl’évènement.\nSont exclus\nles véhicules utilisés pour le transport  \nde passagers à titre onéreux tels que taxis, \nambulances, auto-écoles, corbillards, voitures \nde location. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !\nFestivals .Fun .Beer .Lots ofbeer .Charlotte Center City\nPartners (orFind Your Center )puts onalot ofthese types\noffestivals Uptown and inSouth End .When you check out\ntheir website ortheir weekly newsletter you ’llbeable to\nseelots ofevents coming uplike Beer ,Blues and BBQ ,\nTaste ofCharlotte ,Speedstreet and the like .\\n\\nMany of\nthese events and festivals usually have beer available ,\nhence why I’mafan .And ,yeah ,Ialso really like\nsupporting the local organization that ’sresponsible for\nUptown ’sdevelopment .Ifonly there was aPBR festival ...! \n\n [96]P. Gordaliza, E. Del Barrio, G. Fabrice, and J.-M.\nLoubes,Obtaining fairness using optimal transport theory , in\nInternational Conference on Machine Learning, PMLR, 2019,\npp. 2357–2365.\n[97]A.Graves ,Generating sequences with recurrent neural networks. ,\nCoRR, abs/1308.0850 (2013).\n[98]A. Graves, G. Wayne, and I. Danihelka ,Neural turing ma-\nchines, CoRR, abs/1410.5401 (2014).\n[99]R. M. Gray and D. L. Neuhoff ,Quantization , IEEE Trans.\nInf. Theory, 44 (1998), pp. 2325–2383. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n Cross-lingual Word Recovery (CLWR) CLWR [125] uses par-\nallel corpora to learn word alignments between two languages. The\npre-training task starts with a trainable attention matrix [17] that learns\nto represent the source language token embeddings in terms of the tar-\nget language token embeddings. The transformation learned in the at-\ntention matrix is then provided to the cross-lingual model to learn to\nreconstruct the source token embedding. Formally, given a parallel sen-\ntence pair (x,y), wherex= (x1,x2,xm)is a sentence with mwords\nfrom a source language s, andy= (y 1,y2,yn)is a sentence with n\nwords from a target language t. CLWR starts by representing each xias\nxt\ni∈Rhby all word embeddings of y[125]:\nxt\ni=n\nj=1softmax (A ij)yt\nj (4. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n 4.1.2.8 ERNIE-M\nMany techniques have shown the value of using parallel and monolin-\ngual corpora to get better aligned cross-lingual representations. How-\never, parallel corpora are always limited in size relative to monolingual\ndata, limiting the possible progress unless we collect sizable parallel cor-\npora. ERNIE-M [194] uses large monolingual corpora and limited par-\nallel corpora to produce cross-lingual representations. To enhance pro-\nducedrepresentations,ERNIE-Mgeneratespseudo-parallelsentencesus-\ning the large available monolingual corpora and then uses them along\nwith the parallel data to improve the alignment of cross-lingual repre-\nsentations. ERNIE-M proposes two pre-training tasks, Cross-Attention\nmasked Language Modeling (CAMLM) and Back-Translation Masked\nLanguage Modeling (BTMLM). \n\n A(x) = 1 (7.13)\nIn general, ﬁnding anchors is infeasible in a continuous space, and\nby introducing a parameter δ, the deﬁnition can be changed in\nterms of probability to meet this precision threshold for some ar-\nbitrarily small δas given by:\nPr [ρ(A)≥τ]≥1−δ (7.14)\nSince there may be several anchors that meet this criterion, pref-\nerence is given to the anchors with the highest coverage cov(A),\ndeﬁned as\ncov(A) =ED(z)[A(z)] (7.15)\nIfA′isthesetofanchorsthatsatisfy(7.14),thenanchorgeneration\nthen becomes a combinatorial optimization problem given by:\nmax\nA∈A′cov(A) (7.16)\nIn practice, various heuristics-based approaches are used. For ex-\nample, a greedy search technique such as a bottom-up approach\nwhere you start with an empty rule set and generate a set of can-\ndidate rules by iteratively extending an anchor by one additional\nfeature predicate creates this rule set in a greedy way. \n\n They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized. \n\n \n La question est What expenses are covered for a person waiting for the repatriation of a non-transportable injured individual?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:38.995969",
              "duration": 4.508769,
              "cost": 0.002277
            },
            "meta": {},
            "text": "Expenses that may be covered for a person waiting for the repatriation of a non-transportable injured individual could include:\n\n1. Accommodation costs: Expenses related to staying in a hotel or temporary accommodation while waiting for repatriation.\n\n2. Meal expenses: Costs for meals and food during the waiting period.\n\n3. Transportation costs: Any necessary transportation expenses incurred during the waiting period.\n\n4. Medical expenses: Costs related to medical care or treatment needed while waiting for repatriation.\n\n5. Communication expenses: Costs for phone calls, internet access, or other communication needs during the waiting period.\n\nIt's important to check the specific terms and conditions of the insurance policy or assistance program to understand the exact coverage provided in such situations.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Expenses that may be covered for a person waiting for the repatriation of a non-transportable injured individual could include:\n\n1. Accommodation costs: Expenses related to staying in a hotel or temporary accommodation while waiting for repatriation.\n\n2. Meal expenses: Costs for meals and food during the waiting period.\n\n3. Transportation costs: Any necessary transportation expenses incurred during the waiting period.\n\n4. Medical expenses: Costs related to medical care or treatment needed while waiting for repatriation.\n\n5. Communication expenses: Costs for phone calls, internet access, or other communication needs during the waiting period.\n\nIt's important to check the specific terms and conditions of the insurance policy or assistance program to understand the exact coverage provided in such situations.\n\n1. 1. The expenses covered by an insurance policy can differ.\n2. 2. Accommodation costs may be included in the insurance coverage.\n3. 3. Meals might also be covered by the insurance policy.\n4. 4. Some insurance policies may cover local transportation.\n5. 5. The specific terms and conditions of the insurance policy should be checked for detailed information.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:30.109613",
                "duration": 1.604438,
                "cost": 0.01065
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The insurer's compliance with data protection regulations is not detailed in the text.\n2. The handling of beneficiaries' personal information for the assistance contract is not explained.\n3. The text does not specify how the insurer safeguards the personal data of beneficiaries.\n4. There is no information on the insurer's data protection measures for the assistance contract.\n5. The text lacks details on how personal information is protected in the assistance contract.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on how the insurer ensures compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:40.408530",
          "duration": 6.678349,
          "cost": 0.00948
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The insurer's compliance with data protection regulations is not detailed in the text."
          },
          {
            "meta": {},
            "text": "2. The handling of beneficiaries' personal information for the assistance contract is not explained."
          },
          {
            "meta": {},
            "text": "3. The text does not specify how the insurer safeguards the personal data of beneficiaries."
          },
          {
            "meta": {},
            "text": "4. There is no information on the insurer's data protection measures for the assistance contract."
          },
          {
            "meta": {},
            "text": "5. The text lacks details on how personal information is protected in the assistance contract."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.90938905521328,
              "Node id": "d7e33d1d-6c53-4aad-a5d4-52bdc065e3c9"
            },
            "text": "TaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows."
          },
          {
            "meta": {
              "score": 11.906958168505405,
              "Node id": "200998cd-a57e-4d9a-bb4e-d4dfdbe76a33"
            },
            "text": "II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96"
          },
          {
            "meta": {
              "score": 11.765361055780266,
              "Node id": "02582669-ae1c-48e1-9b87-937dd30bbb58"
            },
            "text": "This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. In this case study, we see how to use pre-trained (or ﬁne-\ntuned) Transformer models to do topic modeling. If one is exploring a\nnewdataset,thismethodcouldbeusedduringexploratorydataanalysis.\n3.5.2 Data, Tools, and Libraries\npip install -U datasets bertopic\nListing 3.1 Python environment setup"
          },
          {
            "meta": {
              "score": 11.472487092087738,
              "Node id": "571cefe1-0343-4761-8963-099ac42f61e6"
            },
            "text": "Authors\nUday Kamath has spent more than two decades developing analyt-\nics products and combines this experience with learning in statistics,\noptimization, machine learning, bioinformatics, and evolutionary com-\nputing. He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh."
          },
          {
            "meta": {
              "score": 11.183679295917447,
              "Node id": "6331716b-cd66-4e71-8bf4-651d486faa69"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model."
          },
          {
            "meta": {
              "score": 11.124992311081895,
              "Node id": "2a38cbb6-66a0-44e7-bb9d-02f26a308cf3"
            },
            "text": "Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences."
          },
          {
            "meta": {
              "score": 10.90790449301797,
              "Node id": "b190fea6-79d6-4db6-b21c-3ba166c35bfb"
            },
            "text": "All these functional units\nget detailed treatment from a theoretical and practical perspec-\ntives for the readers to get a complete handle on the topic. Finally,\na real-world case study using transformers for machine translation\ntasks showing the operative aspects concludes the chapter.\n3. The advent of BERT has revolutionized the ﬁeld of natural lan-\nguage processing (NLP) and helped to get close to human-level\nperformance in many conventionally challenging tasks. Chapter\n3 introduces the details of the BERT architecture and how it is\npre-trained and ﬁne-tuned for classical NLP tasks such as sin-\ngle/pair text classiﬁcation, token tagging, and question answer-\ning. The chapter also discusses the ﬁeld of BERTology, which is\nresearch related to the inner workings of BERT and how it pro-\ncesses and analyzes text and information. Finally, the chapter in-\ntroduces some deep learning architectures that modify BERT for\nmore eﬃciency (e.g., RoBERTa) and other types of NLP appli-\ncations (e.g., NLP for tabular data—TaBERT)."
          },
          {
            "meta": {
              "score": 10.800795154018427,
              "Node id": "91281b84-105b-4d8f-8f6e-122eec7dabc1"
            },
            "text": "3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset."
          },
          {
            "meta": {
              "score": 10.73178805520903,
              "Node id": "5bb2599c-9b92-4d57-be89-3b2225c96acf"
            },
            "text": "Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr."
          },
          {
            "meta": {
              "score": 10.696409547382402,
              "Node id": "88913829-06fa-4457-9dfb-7e8a64fb109a"
            },
            "text": "121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ ."
          },
          {
            "meta": {
              "score": 0.7357409553934383,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7348453512047937,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7337369701576576,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.733148816767294,
              "Node id": "552b81e8-555c-4749-a44e-e0e3fdd9f077"
            },
            "text": "[20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019."
          },
          {
            "meta": {
              "score": 0.7331188038250595,
              "Node id": "eada2f44-1d6a-4882-a349-8816a3a160af"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model."
          },
          {
            "meta": {
              "score": 0.7305887958505806,
              "Node id": "0b3c0029-7d62-4b77-b8f2-148e243a7cd2"
            },
            "text": "[26]E. M. Bender, D. Hovy, and A. Schofield ,Integrating\nethics into the nlp curriculum , in Proceedings of the 58th Annual\nMeetingoftheAssociationforComputationalLinguistics:Tutorial\nAbstracts, 2020, pp. 6–9.\n[27]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle ,\nGreedy layer-wise training of deep networks , in Proceedings of the\n19th International Conference on Neural Information Processing\nSystems, NIPS’06, MIT Press, 2006, pp. 153–160.\n[28]Y. Bengio and Y. LeCun ,Scaling learning algorithms towards\nAI, MIT Press, 2007.\n[29]G. Bertasius, H. Wang, and L. Torresani ,Is space-time\nattention all you need for video understanding? , in ICML, 2021."
          },
          {
            "meta": {
              "score": 0.7304104786341812,
              "Node id": "78b973d1-bc09-46b8-b151-630ed535c4a7"
            },
            "text": "One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases."
          },
          {
            "meta": {
              "score": 0.7302592855052059,
              "Node id": "c194b57c-f2ab-4d62-adce-5d3482fec02d"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          },
          {
            "meta": {
              "score": 0.7298587575236176,
              "Node id": "55ff27c9-0d8d-492c-99f3-014cd0d0b3e1"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies."
          },
          {
            "meta": {
              "score": 0.7297425467917654,
              "Node id": "d380ad8c-1a90-4606-a113-6ff7104777c0"
            },
            "text": "As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on how the insurer ensures compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract.",
              "prompt": {
                "meta": {},
                "user": "How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?",
                "system": "Contexte :  TaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows. \n\n II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96 \n\n This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. In this case study, we see how to use pre-trained (or ﬁne-\ntuned) Transformer models to do topic modeling. If one is exploring a\nnewdataset,thismethodcouldbeusedduringexploratorydataanalysis.\n3.5.2 Data, Tools, and Libraries\npip install -U datasets bertopic\nListing 3.1 Python environment setup \n\n Authors\nUday Kamath has spent more than two decades developing analyt-\nics products and combines this experience with learning in statistics,\noptimization, machine learning, bioinformatics, and evolutionary com-\nputing. He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n All these functional units\nget detailed treatment from a theoretical and practical perspec-\ntives for the readers to get a complete handle on the topic. Finally,\na real-world case study using transformers for machine translation\ntasks showing the operative aspects concludes the chapter.\n3. The advent of BERT has revolutionized the ﬁeld of natural lan-\nguage processing (NLP) and helped to get close to human-level\nperformance in many conventionally challenging tasks. Chapter\n3 introduces the details of the BERT architecture and how it is\npre-trained and ﬁne-tuned for classical NLP tasks such as sin-\ngle/pair text classiﬁcation, token tagging, and question answer-\ning. The chapter also discusses the ﬁeld of BERTology, which is\nresearch related to the inner workings of BERT and how it pro-\ncesses and analyzes text and information. Finally, the chapter in-\ntroduces some deep learning architectures that modify BERT for\nmore eﬃciency (e.g., RoBERTa) and other types of NLP appli-\ncations (e.g., NLP for tabular data—TaBERT). \n\n 3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset. \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n 121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n [20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n [26]E. M. Bender, D. Hovy, and A. Schofield ,Integrating\nethics into the nlp curriculum , in Proceedings of the 58th Annual\nMeetingoftheAssociationforComputationalLinguistics:Tutorial\nAbstracts, 2020, pp. 6–9.\n[27]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle ,\nGreedy layer-wise training of deep networks , in Proceedings of the\n19th International Conference on Neural Information Processing\nSystems, NIPS’06, MIT Press, 2006, pp. 153–160.\n[28]Y. Bengio and Y. LeCun ,Scaling learning algorithms towards\nAI, MIT Press, 2007.\n[29]G. Bertasius, H. Wang, and L. Torresani ,Is space-time\nattention all you need for video understanding? , in ICML, 2021. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n \n La question est How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:41.999998",
              "duration": 2.999681,
              "cost": 0.12786
            },
            "meta": {},
            "text": "The text does not provide information on how the insurer ensures compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on how the insurer ensures compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract.\n\n1. 1. The insurer's compliance with data protection regulations is not detailed in the text.\n2. 2. The handling of beneficiaries' personal information for the assistance contract is not explained.\n3. 3. The text does not specify how the insurer safeguards the personal data of beneficiaries.\n4. 4. There is no information on the insurer's data protection measures for the assistance contract.\n5. 5. The text lacks details on how personal information is protected in the assistance contract.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:32.205149",
                "duration": 2.092499,
                "cost": 0.00774
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "To ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract, the insurer typically implements the following measures:\n\n1. **Data Encryption**: The insurer encrypts personal information to protect it from unauthorized access or breaches.\n\n2. **Access Control**: Access to personal information is restricted to authorized personnel only, and strict access control measures are in place.\n\n3. **Data Minimization**: The insurer only collects and processes personal information that is necessary for providing assistance services, minimizing the data collected.\n\n4. **Anonymization and Pseudonymization**: Personal data is anonymized or pseudonymized whenever possible to reduce the risk of identifying individuals.\n\n5. **Consent Management**: The insurer obtains explicit consent from beneficiaries before collecting and processing their personal information for the assistance contract.\n\n6. **Data Retention Policies**: The insurer establishes clear data retention policies to ensure that personal information is not kept longer than necessary for the purpose for which it was collected.\n\n7. **Regular Audits and Monitoring**: Regular audits are conducted to ensure compliance with data protection regulations, and monitoring systems are in place to detect any unauthorized access or breaches.\n\n8. **Employee Training**: Employees handling personal information receive training on data protection regulations and best practices to ensure proper handling of personal data.\n\n9. **Data Processing Agreements**: The insurer may enter into data processing agreements with third-party service providers to ensure that personal information is handled in compliance with data protection regulations.\n\nBy implementing these measures, the insurer can ensure compliance with data protection regulations and safeguard the personal information of beneficiaries for the assistance contract.",
              "prompt": {
                "meta": {},
                "user": "How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?",
                "system": "Contexte :  TaBERT is trained via content snapshots, in which the model only\nencodes the bits of a table that are most relevant to a query. Due to the\nenormous amount of rows in certain database tables, encoding them is a\ncomputationallyintensiveandineﬃcientprocedure.Byencodingjustthe\nportion of material that is most relevant to the utterance, content snap-\nshots enable TaBERT to handle with enormous tables. For instance, the\nphrase “In which city did Piotr ﬁnish ﬁrst last time?” (sample extracted\nfrom the WikiTableQuestions dataset) may be accompanied with a table\nincludinginformationontheyear,venue,position,andevent.Asnapshot\nof content will take a subset of three rows. This subset will not include\nall of the table’s data, but it is suﬃcient for the model to understand\nthat, for example, the venue column comprises cities. TaBERT describes\ntable structure using a mix of traditional horizontal self-attention, which\ncaptures the dependence between cells in speciﬁc rows, and vertical self-\nattention, which catches the information ﬂow between cells in diﬀerent\nrows. \n\n II - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . Page 96 \n\n This is followed by an enhanced understanding of the hierarchical\nstructure of sentences. Numerous studies have shown that the middle\nBERT layers contain the most syntactic information and that the last\nBERT levels include the most task-speciﬁc information. Additionally, it\nwas shown that, although the majority of syntactic information may be\nlocalized in a few levels, semantic information is distributed across the\nmodel, which explains why some non-trivial cases are initially handled\nwrong then successfully at higher layers.\n3.5 CASE STUDY: TOPIC MODELING WITH TRANSFORMERS\n3.5.1 Goal\nIn this chapter, we looked at several applications of the Transformer\narchitecture. In this case study, we see how to use pre-trained (or ﬁne-\ntuned) Transformer models to do topic modeling. If one is exploring a\nnewdataset,thismethodcouldbeusedduringexploratorydataanalysis.\n3.5.2 Data, Tools, and Libraries\npip install -U datasets bertopic\nListing 3.1 Python environment setup \n\n Authors\nUday Kamath has spent more than two decades developing analyt-\nics products and combines this experience with learning in statistics,\noptimization, machine learning, bioinformatics, and evolutionary com-\nputing. He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n All these functional units\nget detailed treatment from a theoretical and practical perspec-\ntives for the readers to get a complete handle on the topic. Finally,\na real-world case study using transformers for machine translation\ntasks showing the operative aspects concludes the chapter.\n3. The advent of BERT has revolutionized the ﬁeld of natural lan-\nguage processing (NLP) and helped to get close to human-level\nperformance in many conventionally challenging tasks. Chapter\n3 introduces the details of the BERT architecture and how it is\npre-trained and ﬁne-tuned for classical NLP tasks such as sin-\ngle/pair text classiﬁcation, token tagging, and question answer-\ning. The chapter also discusses the ﬁeld of BERTology, which is\nresearch related to the inner workings of BERT and how it pro-\ncesses and analyzes text and information. Finally, the chapter in-\ntroduces some deep learning architectures that modify BERT for\nmore eﬃciency (e.g., RoBERTa) and other types of NLP appli-\ncations (e.g., NLP for tabular data—TaBERT). \n\n 3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset. \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n 121 -1 du Code des assurances,  \nla clause de « réduction ou de majoration des primes ou cotisations » est applicable aux contrats  \ngarantissant les quadricycles à moteur.  \nII - CLAUSES TYPES APPLICABLES À L’ASSURANCE DES RISQUES DE CATASTROPHES NATURELLES  ...................  Page 84 \nIII – ASSISTANCE AU VÉHICULE ET AUX PERSON NES TRANSPORT ÉES ................................ ...............................  Page 85 \nIV - GAR ANTIES DE PROTECTION JURIDIQUE  : HONORAIRES ET FRAIS GARANTIS  ................................ ............  Page 93 \nV - TEXTE DE L’ARTICLE 29 DE LA LOI N° 85 -677 DU 5 JUILLET 1985  ................................ ................................ . Page 93 \nVI - SERVICES À LA PERSONNE ET PRESTATIONS D’ACCOMPAGNEMENT PERSONNALISÉ  ................................ . \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n [20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019. \n\n Interpretability and Explainability Techniques for Transformers ■211\n7.6.2 Data, Tools, and Libraries\nHealthcare data usage has lots of restrictions and constraints due to\nHIPAA privacy regulations. However, MTSamples.com collects tran-\nscribed medical reports with sample transcription reports for forty spe-\ncialties and work types to overcome these limitations and help transcrip-\ntions be more accessible. Kaggle’s medical transcriptions dataset and the\nclassiﬁcation task are based on this dataset. The medical-nlp project fur-\nther transforms the data into four labels or specialities (Surgery, Medical\nRecords, Internal Medicine, and Other) using the medical taxonomy. We\nwillusethetransformeddatasetandclassiﬁcationtaskforthecasestudy.\nWe use pandas for basic text processing and exploratory data anal-\nysis. Sklearn library is used for traditional NLP pipeline and logistic\nregression model. \n\n [26]E. M. Bender, D. Hovy, and A. Schofield ,Integrating\nethics into the nlp curriculum , in Proceedings of the 58th Annual\nMeetingoftheAssociationforComputationalLinguistics:Tutorial\nAbstracts, 2020, pp. 6–9.\n[27]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle ,\nGreedy layer-wise training of deep networks , in Proceedings of the\n19th International Conference on Neural Information Processing\nSystems, NIPS’06, MIT Press, 2006, pp. 153–160.\n[28]Y. Bengio and Y. LeCun ,Scaling learning algorithms towards\nAI, MIT Press, 2007.\n[29]G. Bertasius, H. Wang, and L. Torresani ,Is space-time\nattention all you need for video understanding? , in ICML, 2021. \n\n One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n \n La question est How does the insurer ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:49.680955",
              "duration": 7.677961,
              "cost": 0.0025789999999999997
            },
            "meta": {},
            "text": "To ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract, the insurer typically implements the following measures:\n\n1. **Data Encryption**: The insurer encrypts personal information to protect it from unauthorized access or breaches.\n\n2. **Access Control**: Access to personal information is restricted to authorized personnel only, and strict access control measures are in place.\n\n3. **Data Minimization**: The insurer only collects and processes personal information that is necessary for providing assistance services, minimizing the data collected.\n\n4. **Anonymization and Pseudonymization**: Personal data is anonymized or pseudonymized whenever possible to reduce the risk of identifying individuals.\n\n5. **Consent Management**: The insurer obtains explicit consent from beneficiaries before collecting and processing their personal information for the assistance contract.\n\n6. **Data Retention Policies**: The insurer establishes clear data retention policies to ensure that personal information is not kept longer than necessary for the purpose for which it was collected.\n\n7. **Regular Audits and Monitoring**: Regular audits are conducted to ensure compliance with data protection regulations, and monitoring systems are in place to detect any unauthorized access or breaches.\n\n8. **Employee Training**: Employees handling personal information receive training on data protection regulations and best practices to ensure proper handling of personal data.\n\n9. **Data Processing Agreements**: The insurer may enter into data processing agreements with third-party service providers to ensure that personal information is handled in compliance with data protection regulations.\n\nBy implementing these measures, the insurer can ensure compliance with data protection regulations and safeguard the personal information of beneficiaries for the assistance contract.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: To ensure compliance with data protection regulations in handling the personal information of the beneficiaries for the assistance contract, the insurer typically implements the following measures:\n\n1. **Data Encryption**: The insurer encrypts personal information to protect it from unauthorized access or breaches.\n\n2. **Access Control**: Access to personal information is restricted to authorized personnel only, and strict access control measures are in place.\n\n3. **Data Minimization**: The insurer only collects and processes personal information that is necessary for providing assistance services, minimizing the data collected.\n\n4. **Anonymization and Pseudonymization**: Personal data is anonymized or pseudonymized whenever possible to reduce the risk of identifying individuals.\n\n5. **Consent Management**: The insurer obtains explicit consent from beneficiaries before collecting and processing their personal information for the assistance contract.\n\n6. **Data Retention Policies**: The insurer establishes clear data retention policies to ensure that personal information is not kept longer than necessary for the purpose for which it was collected.\n\n7. **Regular Audits and Monitoring**: Regular audits are conducted to ensure compliance with data protection regulations, and monitoring systems are in place to detect any unauthorized access or breaches.\n\n8. **Employee Training**: Employees handling personal information receive training on data protection regulations and best practices to ensure proper handling of personal data.\n\n9. **Data Processing Agreements**: The insurer may enter into data processing agreements with third-party service providers to ensure that personal information is handled in compliance with data protection regulations.\n\nBy implementing these measures, the insurer can ensure compliance with data protection regulations and safeguard the personal information of beneficiaries for the assistance contract.\n\n1. 1. The insurer's compliance with data protection regulations is not detailed in the text.\n2. 2. The handling of beneficiaries' personal information for the assistance contract is not explained.\n3. 3. The text does not specify how the insurer safeguards the personal data of beneficiaries.\n4. 4. There is no information on the insurer's data protection measures for the assistance contract.\n5. 5. The text lacks details on how personal information is protected in the assistance contract.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:33.777071",
                "duration": 1.569185,
                "cost": 0.01641
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Are damages resulting from climatic events covered under the \"Garantie Dommages tous accidents\"?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not clarify if the \"Garantie Dommages tous accidents\" covers damages from climatic events.\n2. It is unclear from the text if climatic event damages fall under the \"Garantie Dommages tous accidents\".\n3. The text leaves it ambiguous whether the \"Garantie Dommages tous accidents\" includes coverage for climatic event damages.\n4. The text does not specify if the \"Garantie Dommages tous accidents\" encompasses damages caused by climatic events.\n5. It is not stated in the text whether damages due to climatic events are included in the \"Garantie Dommages tous accidents\".",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on whether damages resulting from climatic events are covered under the \"Garantie Dommages tous accidents\".",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:50.419334",
          "duration": 10.007281,
          "cost": 0.012450000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not clarify if the \"Garantie Dommages tous accidents\" covers damages from climatic events."
          },
          {
            "meta": {},
            "text": "2. It is unclear from the text if climatic event damages fall under the \"Garantie Dommages tous accidents\"."
          },
          {
            "meta": {},
            "text": "3. The text leaves it ambiguous whether the \"Garantie Dommages tous accidents\" includes coverage for climatic event damages."
          },
          {
            "meta": {},
            "text": "4. The text does not specify if the \"Garantie Dommages tous accidents\" encompasses damages caused by climatic events."
          },
          {
            "meta": {},
            "text": "5. It is not stated in the text whether damages due to climatic events are included in the \"Garantie Dommages tous accidents\"."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.318539085987368,
              "Node id": "a53b68ed-38fa-45f7-8fce-cbe4899e1ae0"
            },
            "text": "BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]."
          },
          {
            "meta": {
              "score": 10.011554883903088,
              "Node id": "b3929954-3f25-4fa1-be68-82f2184b7e47"
            },
            "text": "Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. The book rounds oﬀ its theory-\ndriven applied and practical coverage with hands-on case studies with\nxvii"
          },
          {
            "meta": {
              "score": 9.717747264752953,
              "Node id": "677dce21-635b-4ff4-a357-532465f3ff5e"
            },
            "text": "Since BERT, many ﬂavors of pre-trained models have been made\navailable across diﬀerent domains, providing models that can be\nﬁne-tuned to domain-speciﬁc data across science, medicine, and\nﬁnance. In addition, language-speciﬁc pre-trained models oﬀer in-\ncreasingly competitive results on downstream language speciﬁc\ntasks. In Chapter 6, we discuss the pre-trained models that are\navailable, showing their beneﬁts and applications to speciﬁc do-\nmains such as computer vision, speech, time series, and text. This\nchapter includes a case study that compares the performance of\nthree transformer-based automatic speech recognition models.\n7. There is a need to understand the models from an explainability\nstandpoint in many critical applications and given the black-box\nnature of transformers-based models. In Chapter 7, we will cover\nthe traits of the models that address explainability, related areas\nthat impact explainability, the taxonomy of explainable methods\napplied tothe transformer-based and attention-based systems, and\nﬁnally,adetailedcasestudyintheelectronichealthrecordsystems\nusing transformers with diﬀerent explainable techniques to become\nmore practical."
          },
          {
            "meta": {
              "score": 9.130782338341737,
              "Node id": "d85ab61f-2026-4419-90d4-b3321ef36a75"
            },
            "text": "These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43"
          },
          {
            "meta": {
              "score": 8.685673413525214,
              "Node id": "bc5c5b0a-035b-4d19-a9d0-4c51d5a48160"
            },
            "text": "Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output."
          },
          {
            "meta": {
              "score": 8.352533725704655,
              "Node id": "619c73c2-6012-46fb-bab0-b3e8b846eaa3"
            },
            "text": "C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]."
          },
          {
            "meta": {
              "score": 8.140935745743517,
              "Node id": "1253fe7b-8505-41d7-a243-252b5e2381c1"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals."
          },
          {
            "meta": {
              "score": 7.593082988116069,
              "Node id": "f3a1930d-74f5-4f54-988d-729bd16d0d23"
            },
            "text": "6008–6018.\n[161]J. Libovick `y, R. Rosa, and A. Fraser ,How language-neutral\nis multilingual bert? , arXiv preprint arXiv:1911.03310, (2019).\n[162]T. Limisiewicz, D. Marecek, and R. Rosa ,Universal de-\npendencies according to BERT: both more speciﬁc and more gen-\neral, in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, EMNLP 2020, Online\nEvent, 16–20 November 2020, T. Cohn, Y. He, and Y. Liu, eds.,\nvol. EMNLP 2020 of Findings of ACL, Association for Computa-\ntional Linguistics, 2020, pp. 2710–2722."
          },
          {
            "meta": {
              "score": 7.539751797277147,
              "Node id": "7e45352b-af1d-4ef1-b0c9-d5012932e7ae"
            },
            "text": "Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset."
          },
          {
            "meta": {
              "score": 7.347339386319438,
              "Node id": "f5e91c57-8f50-42bf-84c9-3a246d3790e5"
            },
            "text": "We show a partial\nlist of modiﬁed transformers in Table 1.1.\n1.2.1.2 Transformer sublayer changes\nIn Chapter 2, we’ll take a detailed look at the structure of a trans-\nformer block, covering its four components so we can later discuss ways\nin which researchers have modiﬁed them. In general, there are four parts\nto a transformer block [254]: positional encodings, multi-head attention,\nresidual connections with layer normalization [13], and a position-wise\nfeedforward network. Changes to transformer sublayers have focused on"
          },
          {
            "meta": {
              "score": 0.7289640252051922,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7258901001486763,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7250841614048197,
              "Node id": "d61f3c72-89ab-42c9-83a5-aa36452671d6"
            },
            "text": "Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and\nP. Frossard ,Universal adversarial perturbations , in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition, 2017, pp. 1765–1773.\n[190]N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and\nJ. F. Allen ,LSDSem 2017 shared task: The story cloze test , in\nLSDSem@EACL, 2017.\n[191]A. Nguyen, J. Yosinski, and J. Clune ,Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecognizable im-\nages, in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2015, pp. 427–436.\n[192]J. Nivre, M. Abrams, š. Agić, L. Ahrenberg, L. Anton-\nsen, K. Aplonova, M. J. Aranzabe, G. Arutie, M. Asa-\nhara, L. Ateyah, M. Attia, and et."
          },
          {
            "meta": {
              "score": 0.7248522347102101,
              "Node id": "9dc542c9-e156-42d0-88fd-d7d1f509b615"
            },
            "text": ", in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020.\n[201]O. Press, N. A. Smith, and O. Levy ,Improving transformer\nmodels by reordering their sublayers , in ACL, 2020."
          },
          {
            "meta": {
              "score": 0.724738377942055,
              "Node id": "f4a0ff82-c2d4-443d-80f6-9a4cc00902fd"
            },
            "text": "557–575.\n[199]T. Pires, E. Schlinger, and D. Garrette ,How multilingual\nis multilingual bert? , in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020."
          },
          {
            "meta": {
              "score": 0.7247096600466122,
              "Node id": "f4c27cbc-d4b9-4e41-a482-bc2741ccce81"
            },
            "text": "Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152."
          },
          {
            "meta": {
              "score": 0.7235862532737369,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          },
          {
            "meta": {
              "score": 0.7235683209670373,
              "Node id": "fe78f1b2-4301-4e47-85a2-e52abf0be2ef"
            },
            "text": "Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]."
          },
          {
            "meta": {
              "score": 0.7235668821665552,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7230990138785932,
              "Node id": "abbba5e2-b88d-477a-aa3c-f98ec10b3312"
            },
            "text": "Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152.\n[221]H. Schwenk and X. Li ,A corpus for multilingual document\nclassiﬁcation in eight languages , in Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation\n(LREC 2018), N. C. C. chair), K. Choukri, C. Cieri, T. De-\nclerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani,\nH. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga, eds.,\nParis, France, may 2018, European Language Resources Associa-\ntion (ELRA)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on whether damages resulting from climatic events are covered under the \"Garantie Dommages tous accidents\".",
              "prompt": {
                "meta": {},
                "user": "Are damages resulting from climatic events covered under the \"Garantie Dommages tous accidents\"?",
                "system": "Contexte :  BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. The book rounds oﬀ its theory-\ndriven applied and practical coverage with hands-on case studies with\nxvii \n\n Since BERT, many ﬂavors of pre-trained models have been made\navailable across diﬀerent domains, providing models that can be\nﬁne-tuned to domain-speciﬁc data across science, medicine, and\nﬁnance. In addition, language-speciﬁc pre-trained models oﬀer in-\ncreasingly competitive results on downstream language speciﬁc\ntasks. In Chapter 6, we discuss the pre-trained models that are\navailable, showing their beneﬁts and applications to speciﬁc do-\nmains such as computer vision, speech, time series, and text. This\nchapter includes a case study that compares the performance of\nthree transformer-based automatic speech recognition models.\n7. There is a need to understand the models from an explainability\nstandpoint in many critical applications and given the black-box\nnature of transformers-based models. In Chapter 7, we will cover\nthe traits of the models that address explainability, related areas\nthat impact explainability, the taxonomy of explainable methods\napplied tothe transformer-based and attention-based systems, and\nﬁnally,adetailedcasestudyintheelectronichealthrecordsystems\nusing transformers with diﬀerent explainable techniques to become\nmore practical. \n\n These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. \n\n 6008–6018.\n[161]J. Libovick `y, R. Rosa, and A. Fraser ,How language-neutral\nis multilingual bert? , arXiv preprint arXiv:1911.03310, (2019).\n[162]T. Limisiewicz, D. Marecek, and R. Rosa ,Universal de-\npendencies according to BERT: both more speciﬁc and more gen-\neral, in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, EMNLP 2020, Online\nEvent, 16–20 November 2020, T. Cohn, Y. He, and Y. Liu, eds.,\nvol. EMNLP 2020 of Findings of ACL, Association for Computa-\ntional Linguistics, 2020, pp. 2710–2722. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. \n\n We show a partial\nlist of modiﬁed transformers in Table 1.1.\n1.2.1.2 Transformer sublayer changes\nIn Chapter 2, we’ll take a detailed look at the structure of a trans-\nformer block, covering its four components so we can later discuss ways\nin which researchers have modiﬁed them. In general, there are four parts\nto a transformer block [254]: positional encodings, multi-head attention,\nresidual connections with layer normalization [13], and a position-wise\nfeedforward network. Changes to transformer sublayers have focused on \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and\nP. Frossard ,Universal adversarial perturbations , in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition, 2017, pp. 1765–1773.\n[190]N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and\nJ. F. Allen ,LSDSem 2017 shared task: The story cloze test , in\nLSDSem@EACL, 2017.\n[191]A. Nguyen, J. Yosinski, and J. Clune ,Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecognizable im-\nages, in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2015, pp. 427–436.\n[192]J. Nivre, M. Abrams, š. Agić, L. Ahrenberg, L. Anton-\nsen, K. Aplonova, M. J. Aranzabe, G. Arutie, M. Asa-\nhara, L. Ateyah, M. Attia, and et. \n\n , in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020.\n[201]O. Press, N. A. Smith, and O. Levy ,Improving transformer\nmodels by reordering their sublayers , in ACL, 2020. \n\n 557–575.\n[199]T. Pires, E. Schlinger, and D. Garrette ,How multilingual\nis multilingual bert? , in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020. \n\n Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152.\n[221]H. Schwenk and X. Li ,A corpus for multilingual document\nclassiﬁcation in eight languages , in Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation\n(LREC 2018), N. C. C. chair), K. Choukri, C. Cieri, T. De-\nclerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani,\nH. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga, eds.,\nParis, France, may 2018, European Language Resources Associa-\ntion (ELRA). \n\n \n La question est Are damages resulting from climatic events covered under the \"Garantie Dommages tous accidents\"?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:08:55.618095",
              "duration": 5.93248,
              "cost": 0.1344
            },
            "meta": {},
            "text": "The text does not provide information on whether damages resulting from climatic events are covered under the \"Garantie Dommages tous accidents\".",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on whether damages resulting from climatic events are covered under the \"Garantie Dommages tous accidents\".\n\n1. 1. The text does not clarify if the \"Garantie Dommages tous accidents\" covers damages from climatic events.\n2. 2. It is unclear from the text if climatic event damages fall under the \"Garantie Dommages tous accidents\".\n3. 3. The text leaves it ambiguous whether the \"Garantie Dommages tous accidents\" includes coverage for climatic event damages.\n4. 4. The text does not specify if the \"Garantie Dommages tous accidents\" encompasses damages caused by climatic events.\n5. 5. It is not stated in the text whether damages due to climatic events are included in the \"Garantie Dommages tous accidents\".",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:35.339766",
                "duration": 1.559919,
                "cost": 0.00921
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The \"Garantie Dommages tous accidents\" typically covers damages resulting from various accidents, such as fire, water damage, vandalism, and theft. However, coverage for damages resulting from climatic events may vary depending on the specific terms and conditions of the insurance policy. \n\nIt is essential to carefully review your insurance policy or consult with your insurance provider to determine if damages resulting from climatic events, such as storms, floods, or other natural disasters, are covered under the \"Garantie Dommages tous accidents\" policy. In some cases, additional coverage or specific insurance policies may be required to protect against damages caused by climatic events.",
              "prompt": {
                "meta": {},
                "user": "Are damages resulting from climatic events covered under the \"Garantie Dommages tous accidents\"?",
                "system": "Contexte :  BERT uses unlabeled text\nto pre-train deep bidirectional contextual representations. This resulted\nin rich pre-trained language models that can be ﬁne-tuned with a simple\nadditional output layer and a reasonably sized labeled dataset to pro-\nduce state-of-the-art performance in a broad range of NLP tasks. These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. \n\n Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. The book rounds oﬀ its theory-\ndriven applied and practical coverage with hands-on case studies with\nxvii \n\n Since BERT, many ﬂavors of pre-trained models have been made\navailable across diﬀerent domains, providing models that can be\nﬁne-tuned to domain-speciﬁc data across science, medicine, and\nﬁnance. In addition, language-speciﬁc pre-trained models oﬀer in-\ncreasingly competitive results on downstream language speciﬁc\ntasks. In Chapter 6, we discuss the pre-trained models that are\navailable, showing their beneﬁts and applications to speciﬁc do-\nmains such as computer vision, speech, time series, and text. This\nchapter includes a case study that compares the performance of\nthree transformer-based automatic speech recognition models.\n7. There is a need to understand the models from an explainability\nstandpoint in many critical applications and given the black-box\nnature of transformers-based models. In Chapter 7, we will cover\nthe traits of the models that address explainability, related areas\nthat impact explainability, the taxonomy of explainable methods\napplied tothe transformer-based and attention-based systems, and\nﬁnally,adetailedcasestudyintheelectronichealthrecordsystems\nusing transformers with diﬀerent explainable techniques to become\nmore practical. \n\n These\ndevelopments lowered the barrier of entry for the wide adoption of these\npowerful pre-trained models. It is now common practice in the artiﬁcial\nintelligence ﬁeld to share pre-trained models and ﬁne-tune them with\nminimal cost as opposed to the old paradigm of designing task-speciﬁc\narchitectures. In this chapter, we cover BERT’s basic design concepts,\ndevelopments, and applications.\n3.1 BERT\n3.1.1 Architecture\nCore layers One of the major contributions of BERT is the simplicity\nof design and the diversity of covered downstream tasks. BERT’s archi-\ntecture consists of multi-layers of the bidirectional transformer encoder\n[253]. The capacity of the BERT architecture is characterized by (i) the\nDOI: 10.1201/9781003170082-3 43 \n\n Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275].\n7.1 TRAITS OF EXPLAINABLE SYSTEMS\nOne of the decisive goals of an explainable system is that it allows the\nsystem’s end-users to understand the relationship between the input and\nthe output. \n\n C H A P T E R 7\nInterpretability and\nExplainability\nTechniques for\nTransformers\nIncritical applications in domains like healthcare, legislation, law en-\nforcement, or ﬁnancial, in addition to the predictions, there is a need\nto understand the models from an explainability standpoint. Unfortu-\nnately, one can categorize most state-of-the-art transformer models and\ntechniques covered in this book as a “black box” that can signiﬁcantly\nimpede adoption. Therefore, there is a paramount need for building ex-\nplainability around these complex state-of-the-art models from both an\nunderstanding and diagnosis perspective. As proposed by Xie et al., we\nwill cover the traits of the models that address explainability, related\nareas that impact explainability, the taxonomy of explainable meth-\nods applied to transformer-based and attention-based systems, and ﬁ-\nnally, a detailed case study in the electronic health record systems using\ntransformers with diﬀerent explainable techniques to get practical in-\nsights [275]. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. \n\n 6008–6018.\n[161]J. Libovick `y, R. Rosa, and A. Fraser ,How language-neutral\nis multilingual bert? , arXiv preprint arXiv:1911.03310, (2019).\n[162]T. Limisiewicz, D. Marecek, and R. Rosa ,Universal de-\npendencies according to BERT: both more speciﬁc and more gen-\neral, in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, EMNLP 2020, Online\nEvent, 16–20 November 2020, T. Cohn, Y. He, and Y. Liu, eds.,\nvol. EMNLP 2020 of Findings of ACL, Association for Computa-\ntional Linguistics, 2020, pp. 2710–2722. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■63\n(’liquor’, 0.005587858949299897),\n(’center city’, 0.005496678910160935),\n(’beers’, 0.005368697666709216),\n(’events’, 0.005089779403417317)]\nListing 3.12 Median topic: Nightlife in Charlotte, NC\n3.6 CASE STUDY: FINE-TUNING BERT\n3.6.1 Goal\nThe goals of this case study is to provide a step-by-step demo of ﬁne-\ntuning a standard BERT model for any sentence classiﬁcation, we chose\nsentiment classiﬁcation as a sample popular task.\n3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. \n\n We show a partial\nlist of modiﬁed transformers in Table 1.1.\n1.2.1.2 Transformer sublayer changes\nIn Chapter 2, we’ll take a detailed look at the structure of a trans-\nformer block, covering its four components so we can later discuss ways\nin which researchers have modiﬁed them. In general, there are four parts\nto a transformer block [254]: positional encodings, multi-head attention,\nresidual connections with layer normalization [13], and a position-wise\nfeedforward network. Changes to transformer sublayers have focused on \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and\nP. Frossard ,Universal adversarial perturbations , in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition, 2017, pp. 1765–1773.\n[190]N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and\nJ. F. Allen ,LSDSem 2017 shared task: The story cloze test , in\nLSDSem@EACL, 2017.\n[191]A. Nguyen, J. Yosinski, and J. Clune ,Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecognizable im-\nages, in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2015, pp. 427–436.\n[192]J. Nivre, M. Abrams, š. Agić, L. Ahrenberg, L. Anton-\nsen, K. Aplonova, M. J. Aranzabe, G. Arutie, M. Asa-\nhara, L. Ateyah, M. Attia, and et. \n\n , in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020.\n[201]O. Press, N. A. Smith, and O. Levy ,Improving transformer\nmodels by reordering their sublayers , in ACL, 2020. \n\n 557–575.\n[199]T. Pires, E. Schlinger, and D. Garrette ,How multilingual\nis multilingual bert? , in Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28–August 2, 2019, Volume 1: Long Papers, A. Korho-\nnen, D. R. Traum, and L. Màrquez, eds., Association for Compu-\ntational Linguistics, 2019, pp. 4996–5001.\n[200]E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vulić,\nand A. Korhonen ,XCOPA: A multilingual dataset for causal\ncommonsense reasoning , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2020. \n\n Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n Ablation Studies Several ablation studies have been proposed to ex-\namine several hypotheses about the validity of language-agnostic repre-\nsentations in multilingual transformers. First hypothesis tested in [199]\nis that the joint script between high resource language is a confounding\nfactor for good multilingual transformers performance. However this was\nshown to be true as multilingual transfer occur between languages that\ndonotsharescriptsuchasUrduwritteninArabicscriptandHindiinDe-\nvanagari script [199]. Other work [229] examined the input tokenization\nas a confounding factor and found that using subword tokenization bi-\nasesthemodelstowardslearninglanguage-agnosticrepresentationsmore\nthan word-level or character-level tokenization. Pre-training tasks has\nalso been examined as a possible confounding factor, it was shown that\nmodels trained with parallel corpora pre-training tasks such as XLM re-\ntain language-agnostic representations within the higher encoder layers\nin a stronger manner than models trained on monolingual corpora such\nas mBERT and XLMR [52]. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Learn-\ning Representations by Back-propagating Errors, pp. 696–699.\n[218]P. Samangouei, M. Kabkab, and R. Chellappa ,Defense-\nGAN: Protecting classiﬁers against adversarial attacks using gen-\nerative models , arXiv preprint arXiv:1805.06605, (2018).\n[219]S. Schneider, A. Baevski, R. Collobert, and M. Auli ,\nwav2vec: Unsupervised pre-training for speech recognition , in IN-\nTERSPEECH, 2019.\n[220]M. Schuster and K. Nakajima ,Japanese and Korean voice\nsearch, 2012 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), (2012), pp. 5149–5152.\n[221]H. Schwenk and X. Li ,A corpus for multilingual document\nclassiﬁcation in eight languages , in Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation\n(LREC 2018), N. C. C. chair), K. Choukri, C. Cieri, T. De-\nclerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani,\nH. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga, eds.,\nParis, France, may 2018, European Language Resources Associa-\ntion (ELRA). \n\n \n La question est Are damages resulting from climatic events covered under the \"Garantie Dommages tous accidents\"?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:08:59.520510",
              "duration": 3.898831,
              "cost": 0.0024085
            },
            "meta": {},
            "text": "The \"Garantie Dommages tous accidents\" typically covers damages resulting from various accidents, such as fire, water damage, vandalism, and theft. However, coverage for damages resulting from climatic events may vary depending on the specific terms and conditions of the insurance policy. \n\nIt is essential to carefully review your insurance policy or consult with your insurance provider to determine if damages resulting from climatic events, such as storms, floods, or other natural disasters, are covered under the \"Garantie Dommages tous accidents\" policy. In some cases, additional coverage or specific insurance policies may be required to protect against damages caused by climatic events.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The \"Garantie Dommages tous accidents\" typically covers damages resulting from various accidents, such as fire, water damage, vandalism, and theft. However, coverage for damages resulting from climatic events may vary depending on the specific terms and conditions of the insurance policy. \n\nIt is essential to carefully review your insurance policy or consult with your insurance provider to determine if damages resulting from climatic events, such as storms, floods, or other natural disasters, are covered under the \"Garantie Dommages tous accidents\" policy. In some cases, additional coverage or specific insurance policies may be required to protect against damages caused by climatic events.\n\n1. 1. The text does not clarify if the \"Garantie Dommages tous accidents\" covers damages from climatic events.\n2. 2. It is unclear from the text if climatic event damages fall under the \"Garantie Dommages tous accidents\".\n3. 3. The text leaves it ambiguous whether the \"Garantie Dommages tous accidents\" includes coverage for climatic event damages.\n4. 4. The text does not specify if the \"Garantie Dommages tous accidents\" encompasses damages caused by climatic events.\n5. 5. It is not stated in the text whether damages due to climatic events are included in the \"Garantie Dommages tous accidents\".",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:36.724571",
                "duration": 1.383082,
                "cost": 0.0123
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The warranty for the Crémaillère covers its internal parts.\n2. Internal components of the vérins de direction are included in the warranty.\n3. The pompe d’assistance's internal parts are covered under the warranty.\n4. The warranty does not cover conduits and durits.\n5. Hoses and pipes are excluded from the warranty. \n6. The warranty does not include external parts of the Crémaillère, vérins de direction, and pompe d’assistance.\n7. Only the internal parts of these components are covered by the warranty.",
          "prompt": {
            "meta": {},
            "user": "The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (assistance pump) are the internal parts of these components. However, conduits and durits (hoses and pipes) are excluded from the warranty.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:24:59.848535",
          "duration": 9.426431,
          "cost": 0.01272
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The warranty for the Crémaillère covers its internal parts."
          },
          {
            "meta": {},
            "text": "2. Internal components of the vérins de direction are included in the warranty."
          },
          {
            "meta": {},
            "text": "3. The pompe d’assistance's internal parts are covered under the warranty."
          },
          {
            "meta": {},
            "text": "4. The warranty does not cover conduits and durits."
          },
          {
            "meta": {},
            "text": "5. Hoses and pipes are excluded from the warranty."
          },
          {
            "meta": {},
            "text": "6. The warranty does not include external parts of the Crémaillère, vérins de direction, and pompe d’assistance."
          },
          {
            "meta": {},
            "text": "7. Only the internal parts of these components are covered by the warranty."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 31.136778742124875,
              "Node id": "035c7631-f7f0-45c4-90f2-68235d7a31f9"
            },
            "text": "électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits)."
          },
          {
            "meta": {
              "score": 30.5782763293523,
              "Node id": "1426677d-9a2a-4984-8901-f98d6e4ad728"
            },
            "text": "électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits)."
          },
          {
            "meta": {
              "score": 29.53602393019731,
              "Node id": "cf6d7667-ff6e-4ba4-9539-00df8d4bb72d"
            },
            "text": "potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,"
          },
          {
            "meta": {
              "score": 29.33872146183198,
              "Node id": "b46aa55f-6f11-473e-8f50-0e458c59aee4"
            },
            "text": "boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,"
          },
          {
            "meta": {
              "score": 26.192452828373483,
              "Node id": "16293c92-8fff-4bab-a245-5656e270d9b9"
            },
            "text": "régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur ."
          },
          {
            "meta": {
              "score": 25.732978318200114,
              "Node id": "68f03d4a-4e60-4524-8d6c-70801f88f8cb"
            },
            "text": "régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur ."
          },
          {
            "meta": {
              "score": 21.161361879662973,
              "Node id": "6609bffe-8a29-461c-831e-61402cbd03f5"
            },
            "text": "moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises,"
          },
          {
            "meta": {
              "score": 21.060359581927102,
              "Node id": "26c64386-4534-4e8c-868a-3b22fdd498bc"
            },
            "text": "moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises,"
          },
          {
            "meta": {
              "score": 20.8086132047276,
              "Node id": "cc1dc9a8-7692-4f69-bf33-e5aa15ca75e0"
            },
            "text": "La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs .\n  L’alimentation :  pompe à carburant, pompe d’injection, \nturbocompresseur, distributeur, débitmètre d’air, injecteurs."
          },
          {
            "meta": {
              "score": 19.907132431078054,
              "Node id": "bbe09f53-6354-463b-a238-9fcf7bb68f5d"
            },
            "text": "La transmission :  tous les organes et pièces internes, y \ncompris les arbres, boîte de transfert et du réducteur 4x4, les roulements de roue/moyeux et les joints spi.\n    La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs ."
          },
          {
            "meta": {
              "score": 0.7541556385161322,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7474524617286484,
              "Node id": "90d2a948-dcec-4a50-81ab-644ba9c9dd94"
            },
            "text": "Transformers: Basics and Introduction ■29\nFigure 2.14 Joint distribution for English and French sentences based on\nlength.\nThe top 20words for English and French are shown in Fig. 2.15. The\nfrequency and distribution shows some common words as expected such\nas “the-le”, “is-est”, etc. It also highlights certain contractions such as\n“don” for “don’t” and how the underlying tokenizer (spaCy) performs\npreprocessing. Iteratively improving and analyzing the data helps in the\noverall model results.\nBahdanau attention-based sequence-to-sequence and transformer-\nbased will be used to train/validate on the data and the best model\nfrom both will be evaluated on the test set.\n2.5.3.2 Attention\nThe encoder model class BahdanauEncoder as given in the Listing\n2.1 uses a bidirectional gated recurrent unit (GRU) to encode sentences\nin the source language."
          },
          {
            "meta": {
              "score": 0.7466141881894268,
              "Node id": "417fe1e0-ac85-4c9c-84be-650b6d0505ab"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 0.7432863765860315,
              "Node id": "ea60f0c6-b524-4065-949a-f71285cfb01c"
            },
            "text": "Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots."
          },
          {
            "meta": {
              "score": 0.7430628230351983,
              "Node id": "8528a867-3990-4efe-b3d4-8fee9179c3e6"
            },
            "text": "8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018."
          },
          {
            "meta": {
              "score": 0.7426951956138806,
              "Node id": "cc114184-b680-4833-978b-f380b8388f55"
            },
            "text": "228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067."
          },
          {
            "meta": {
              "score": 0.7421057834427139,
              "Node id": "b9c34789-6d83-4615-b032-030eb4446371"
            },
            "text": "Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp."
          },
          {
            "meta": {
              "score": 0.7415920429864272,
              "Node id": "c83627ab-7f32-40d7-a373-7f6c66f61112"
            },
            "text": "Transformers: Basics and Introduction ■41\nFigure 2.21 Explaining translations for—It is snowing at my house."
          },
          {
            "meta": {
              "score": 0.7401821511545327,
              "Node id": "ec83a3f1-b11c-49be-8b91-69da619d59ca"
            },
            "text": "[147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604."
          },
          {
            "meta": {
              "score": 0.7391710657886551,
              "Node id": "56a0beb5-78d0-4259-9ff6-501fa5d8c3cc"
            },
            "text": "40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here?"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (assistance pump) are the internal parts of these components. However, conduits and durits (hoses and pipes) are excluded from the warranty.",
              "prompt": {
                "meta": {},
                "user": "What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?",
                "system": "Contexte :  électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits). \n\n électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits). \n\n potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges, \n\n boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges, \n\n régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur . \n\n régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur . \n\n moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises, \n\n moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises, \n\n La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs .\n  L’alimentation :  pompe à carburant, pompe d’injection, \nturbocompresseur, distributeur, débitmètre d’air, injecteurs. \n\n La transmission :  tous les organes et pièces internes, y \ncompris les arbres, boîte de transfert et du réducteur 4x4, les roulements de roue/moyeux et les joints spi.\n    La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs . \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Transformers: Basics and Introduction ■29\nFigure 2.14 Joint distribution for English and French sentences based on\nlength.\nThe top 20words for English and French are shown in Fig. 2.15. The\nfrequency and distribution shows some common words as expected such\nas “the-le”, “is-est”, etc. It also highlights certain contractions such as\n“don” for “don’t” and how the underlying tokenizer (spaCy) performs\npreprocessing. Iteratively improving and analyzing the data helps in the\noverall model results.\nBahdanau attention-based sequence-to-sequence and transformer-\nbased will be used to train/validate on the data and the best model\nfrom both will be evaluated on the test set.\n2.5.3.2 Attention\nThe encoder model class BahdanauEncoder as given in the Listing\n2.1 uses a bidirectional gated recurrent unit (GRU) to encode sentences\nin the source language. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018. \n\n 228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067. \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n Transformers: Basics and Introduction ■41\nFigure 2.21 Explaining translations for—It is snowing at my house. \n\n [147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n \n La question est What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:09:05.658928",
              "duration": 6.132718,
              "cost": 0.13575
            },
            "meta": {},
            "text": "The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (assistance pump) are the internal parts of these components. However, conduits and durits (hoses and pipes) are excluded from the warranty.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 7",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (assistance pump) are the internal parts of these components. However, conduits and durits (hoses and pipes) are excluded from the warranty.\n\n1. 1. The warranty for the Crémaillère covers its internal parts.\n2. 2. Internal components of the vérins de direction are included in the warranty.\n3. 3. The pompe d’assistance's internal parts are covered under the warranty.\n4. 4. The warranty does not cover conduits and durits.\n5. 5. Hoses and pipes are excluded from the warranty.\n6. 6. The warranty does not include external parts of the Crémaillère, vérins de direction, and pompe d’assistance.\n7. 7. Only the internal parts of these components are covered by the warranty.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:38.561865",
                "duration": 1.833456,
                "cost": 0.010260000000000002
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '7'] - Not validated: ['6']",
              "human": 1.0,
              "auto": 0.8571428571428571
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (power steering pump) in the document include:\n\n- Crémaillère (rack and pinion)\n- Vérins de direction (steering cylinders)\n- Pompe d’assistance (power steering pump)\n\nThese components are part of the steering system and are covered under the warranty according to the document.",
              "prompt": {
                "meta": {},
                "user": "What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?",
                "system": "Contexte :  électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits). \n\n électr ovanne pomp e injection,  moteur\npas à pas, potentiomètre accélérateur, boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits). \n\n potentiomètre accélérateur, boîtier\npapillon,  régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges, \n\n boîtier\npapillon,  régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges, \n\n régulateur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur . \n\n régulat eur de débit\nAlternateur, régulateur de tension, démarreur, balais\nlanceurs, moteur essuie-glaces, moteur lève-vitres,\nmoteur toit ouvrant, moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur . \n\n moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduits et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises, \n\n moteur électromagnétique des\nportes, ventilateur de chauﬀage et résistance,\nbobine d’allumage, allumeur, calculateur injection\nArbre longitudinal (sauf ﬂectors), transmissions, souﬄets\nCylindres hydrauliques de suspension (sauf sphères\net amor tisseurs,  joints,  conduit s et durits,\nélectrovanne et régulateurs).\nCrémaillère, vérins de direction, pompe d’assistance\n(sauf conduits et durits).\nDéshydrateur et compresseur (sauf recharges,\nﬂexibles et tuyaux)\nExclu\nLa main d’œuvr e est prise en charge uniquement pour le remplac ement des pièces défectueuses\ngaranties selon le barème du constructeur .1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page49\n48A - Etendue des garanties (suite)\nG Garantie méc anique initiale\nLes organes concernés :\nGMoteur (pièc es lubriﬁées en mouv ement)\nGTurbo compr esseur \nGBoîte de vitesses (pièces lubriﬁées en mouvement)\nGBoîte automatique (pièces lubriﬁées en mouvement)\nGPont (pièc es lubriﬁées à l’intérieur du pont)\nGTransmission\nGAutr es organesListe exhaustiv e des pièc es garanties\ndans les organes :\nChemises, \n\n La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs .\n  L’alimentation :  pompe à carburant, pompe d’injection, \nturbocompresseur, distributeur, débitmètre d’air, injecteurs. \n\n La transmission :  tous les organes et pièces internes, y \ncompris les arbres, boîte de transfert et du réducteur 4x4, les roulements de roue/moyeux et les joints spi.\n    La direction : crémaillère, colonne de direction, système \nde commande électrique et hydraulique, à  l’exception du \ncontrôle et réglage du train avant.\n    Les freins : groupe hydraulique ABS, câble de frein à main, \ncylindres de roue, étriers de freins, maître cylindre, pompe d’assistance, boîtier d’aide au freinage d’urgence.\n    La suspension :  axes de pivots et bagues, axes, barres \nstabilisatrices, bras de suspension supérieurs et inférieurs, ressorts, rotules, électrovanne, suspension hydraulique et pneumatique, à l’exception des amortisseurs . \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n Transformers: Basics and Introduction ■29\nFigure 2.14 Joint distribution for English and French sentences based on\nlength.\nThe top 20words for English and French are shown in Fig. 2.15. The\nfrequency and distribution shows some common words as expected such\nas “the-le”, “is-est”, etc. It also highlights certain contractions such as\n“don” for “don’t” and how the underlying tokenizer (spaCy) performs\npreprocessing. Iteratively improving and analyzing the data helps in the\noverall model results.\nBahdanau attention-based sequence-to-sequence and transformer-\nbased will be used to train/validate on the data and the best model\nfrom both will be evaluated on the test set.\n2.5.3.2 Attention\nThe encoder model class BahdanauEncoder as given in the Listing\n2.1 uses a bidirectional gated recurrent unit (GRU) to encode sentences\nin the source language. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Transformers: Basics and Introduction ■37\n(a) Example showing the En-\nglish word “going” pays atten-\ntion to “je” and “vais”.\n(b) Example showing the En-\nglishword“be”paysattentionto\n“en” and “etre”.\n(c) English word “library” pays\nattention to multiple words.\n(d) The words “two” and “deux”\nare matched.\nFigure 2.17 Attention examples and plots. \n\n 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067.\n[64]A. Conneau, R. Rinott, G. Lample, A. Williams, S. R.\nBowman, H. Schwenk, and V. Stoyanov ,Xnli: Evaluating\ncross-lingual sentence representations , in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, 2018. \n\n 228■Bibliography\nTetreault, eds., Association for Computational Linguistics, 2020,\npp. 8440–8451.\n[62]A. Conneau, G. Kruszewski, G. Lample, L. Barrault,\nand M. Baroni ,What you can cram into a single vector: Prob-\ning sentence embeddings for linguistic properties , arXiv preprint\narXiv:1805.01070, (2018).\n[63]A. Conneau and G. Lample ,Cross-lingual language model\npretraining , in Advances in Neural Information Processing Sys-\ntems32:AnnualConferenceonNeuralInformationProcessingSys-\ntems 2019, NeurIPS 2019, 8–14 December 2019, Vancouver, BC,\nCanada,H.M.Wallach,H.Larochelle,A.Beygelzimer,F.d’Alché-\nBuc, E. B. Fox, and R. Garnett, eds., 2019, pp. 7057–7067. \n\n Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604.\n[150]Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel ,Backpropagation\napplied to handwritten zip code recognition , Neural Computation,\n1 (1989), pp. \n\n Transformers: Basics and Introduction ■41\nFigure 2.21 Explaining translations for—It is snowing at my house. \n\n [147]S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller,\nand W. Samek ,Analyzing classiﬁers: Fisher vectors and deep\nneural networks , in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016, pp. 2912–2920.\n[148]A. Lauscher, V. Ravishankar, I. Vulic, and G. Glavas ,\nFrom zero to hero: On the limitations of zero-shot cross-lingual\ntransfer with multilingual transformers , CoRR, abs/2005.00633\n(2020).\n[149]Y. LeCun ,Une procédure d’apprentissage pour réseau a seuil\nasymmetrique (a learning scheme for asymmetric threshold net-\nworks), in Proceedings of Cognitiva 85, 1985, pp. 599–604. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n \n La question est What components are covered under the warranty for the Crémaillère, vérins de direction, and pompe d’assistance in the document?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:09:08.349214",
              "duration": 2.686764,
              "cost": 0.0023425
            },
            "meta": {},
            "text": "The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (power steering pump) in the document include:\n\n- Crémaillère (rack and pinion)\n- Vérins de direction (steering cylinders)\n- Pompe d’assistance (power steering pump)\n\nThese components are part of the steering system and are covered under the warranty according to the document.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The components covered under the warranty for the Crémaillère (rack and pinion), vérins de direction (steering cylinders), and pompe d’assistance (power steering pump) in the document include:\n\n- Crémaillère (rack and pinion)\n- Vérins de direction (steering cylinders)\n- Pompe d’assistance (power steering pump)\n\nThese components are part of the steering system and are covered under the warranty according to the document.\n\n1. 1. The warranty for the Crémaillère covers its internal parts.\n2. 2. Internal components of the vérins de direction are included in the warranty.\n3. 3. The pompe d’assistance's internal parts are covered under the warranty.\n4. 4. The warranty does not cover conduits and durits.\n5. 5. Hoses and pipes are excluded from the warranty.\n6. 6. The warranty does not include external parts of the Crémaillère, vérins de direction, and pompe d’assistance.\n7. 7. Only the internal parts of these components are covered by the warranty.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:39.854287",
                "duration": 1.28971,
                "cost": 0.01065
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3'] - Not validated: ['4', '5', '6', '7']",
              "human": 1.0,
              "auto": 0.42857142857142855
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The insurer has the right to cancel the policy if the policyholder does not meet their obligations.\n2. The insurance company can deny a claim if the policyholder has not fulfilled their responsibilities.\n3. Legal action can be taken by the insurer against the policyholder in certain situations.\n4. The insurer can increase the policyholder's premiums if they fail to meet their obligations.\n5. The terms of the contract can be modified by the insurer due to the policyholder's failure to fulfill their duties.\n6. The actions taken by the insurer can vary based on the insurance contract and the jurisdiction's laws. \n7. The insurer usually cancels the policy after a certain period of non-compliance by the policyholder.\n8. Denial of claim often occurs if the policyholder has not paid their premiums or violated contract terms.\n9. Legal action is typically a last resort and is often used in cases of fraud or misrepresentation.\n10. The insurer may modify the contract to account for the policyholder's failure to fulfill their obligations.",
          "prompt": {
            "meta": {},
            "user": "The insurer can take several actions if the policyholder fails to fulfill their obligations under the contract. These can include:\n\n1. Cancellation of the policy: The insurer can choose to cancel the policy, effectively ending the coverage. This is usually done after a certain period of non-compliance by the policyholder.\n\n2. Denial of claim: If the policyholder fails to fulfill their obligations and then makes a claim, the insurer can deny this claim. This is often the case if the policyholder has not paid their premiums or has violated terms of the contract.\n\n3. Legal action: In some cases, the insurer may choose to take legal action against the policyholder. This is usually a last resort and is often used in cases of fraud or misrepresentation.\n\n4. Increase in premiums: The insurer may also choose to increase the policyholder's premiums as a result of their failure to fulfill their obligations.\n\n5. Modification of the contract: In some cases, the insurer may choose to modify the terms of the contract to account for the policyholder's failure to fulfill their obligations.\n\nIt's important to note that the specific actions taken can vary depending on the terms of the insurance contract and the laws of the jurisdiction in which the contract was issued.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:25:14.531692",
          "duration": 14.679508,
          "cost": 0.02355
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The insurer has the right to cancel the policy if the policyholder does not meet their obligations."
          },
          {
            "meta": {},
            "text": "2. The insurance company can deny a claim if the policyholder has not fulfilled their responsibilities."
          },
          {
            "meta": {},
            "text": "3. Legal action can be taken by the insurer against the policyholder in certain situations."
          },
          {
            "meta": {},
            "text": "4. The insurer can increase the policyholder's premiums if they fail to meet their obligations."
          },
          {
            "meta": {},
            "text": "5. The terms of the contract can be modified by the insurer due to the policyholder's failure to fulfill their duties."
          },
          {
            "meta": {},
            "text": "6. The actions taken by the insurer can vary based on the insurance contract and the jurisdiction's laws."
          },
          {
            "meta": {},
            "text": "7. The insurer usually cancels the policy after a certain period of non-compliance by the policyholder."
          },
          {
            "meta": {},
            "text": "8. Denial of claim often occurs if the policyholder has not paid their premiums or violated contract terms."
          },
          {
            "meta": {},
            "text": "9. Legal action is typically a last resort and is often used in cases of fraud or misrepresentation."
          },
          {
            "meta": {},
            "text": "10. The insurer may modify the contract to account for the policyholder's failure to fulfill their obligations."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.761765233170111,
              "Node id": "aef9a952-c24c-41ea-92e7-3213bb808491"
            },
            "text": "The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn."
          },
          {
            "meta": {
              "score": 9.69535198146259,
              "Node id": "d0208230-383e-4f7a-b272-0f018a8c4552"
            },
            "text": "CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation."
          },
          {
            "meta": {
              "score": 9.083179024768224,
              "Node id": "1d5744de-6230-4f87-ae59-8098322828c2"
            },
            "text": "Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé."
          },
          {
            "meta": {
              "score": 9.04159170014682,
              "Node id": "cb51a559-c37c-46be-a076-6f19a31ae981"
            },
            "text": "6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t."
          },
          {
            "meta": {
              "score": 8.646424799096476,
              "Node id": "fd9a6f3b-ff22-48be-9fd0-e307faeb123b"
            },
            "text": "Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé.  \n \nOutillage  \nTous out ils ou matériels nécessaires à l’exercice de l’activité professionnelle de l’assuré."
          },
          {
            "meta": {
              "score": 8.361544541387364,
              "Node id": "4493b24e-bfaf-4316-8595-62ccd811c50b"
            },
            "text": "178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions."
          },
          {
            "meta": {
              "score": 8.297328241999299,
              "Node id": "bc897d65-f05b-4c1f-b472-d4b3dcd73089"
            },
            "text": "CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes."
          },
          {
            "meta": {
              "score": 8.202704952217237,
              "Node id": "0a845b57-e86d-492c-a1eb-3239a72d8da2"
            },
            "text": "Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge une partie de l’indemnité due après sinistre. Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle."
          },
          {
            "meta": {
              "score": 8.019788535639314,
              "Node id": "89168de0-4ccd-4321-8733-7af512d5d457"
            },
            "text": "Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge \nune partie de l’indemnité due après sinistre. Il s’interdit de contracter une \nassurance pour la portion du risque constituée par la franchise.Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant \nde la franchise est de 380 € pour chaque véhicule endommagé. Toutefois, \npour les véhicules terrestres à moteur à usage professionnel, tel que prévu \nà l’article 9 sera appliquée la franchise prévue par le contrat, si celle-ci est \nsupérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre \nsusceptible de faire jouer la garantie dès qu’il en a connaissance et au \nplus tard dans les dix jours suivant la publication de l’arrêté interministériel \nconstatant l’état de catastrophe naturelle."
          },
          {
            "meta": {
              "score": 7.43917344028498,
              "Node id": "5c613af1-1076-4e29-9072-9bd0e064fc41"
            },
            "text": "Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle.\nQuand plusieurs assurances contractées par l’assuré peuvent permettre la réparation des dommages matériels directs \nnon assurables résultant de l’intensité anormale d’un agent naturel, l’assuré doit, en cas de sinistre et dans le délai \nmentionné au précédent alinéa, déclarer l’existence de ces assurances aux assureurs intéressés."
          },
          {
            "meta": {
              "score": 0.7198093058965329,
              "Node id": "631994bd-f09c-427f-bcb2-c78a1ebab13f"
            },
            "text": "The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities."
          },
          {
            "meta": {
              "score": 0.7145453728475099,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7144752194570116,
              "Node id": "6ad9e26c-9c9d-437c-a71a-b39a6976b462"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy."
          },
          {
            "meta": {
              "score": 0.71315068078876,
              "Node id": "bea710bb-c23d-41df-8ac0-1b6422a93a9c"
            },
            "text": "178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions."
          },
          {
            "meta": {
              "score": 0.7122871790778827,
              "Node id": "b57fdf0c-16e2-4621-a27e-450d0011114a"
            },
            "text": "The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn."
          },
          {
            "meta": {
              "score": 0.7073394531934175,
              "Node id": "7591794c-b05c-4ad3-bbfa-83aa41803b93"
            },
            "text": "This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied."
          },
          {
            "meta": {
              "score": 0.7071375263069938,
              "Node id": "d7d336e0-17b8-4036-93a3-f1217f9d34e0"
            },
            "text": "The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12)"
          },
          {
            "meta": {
              "score": 0.7064590899695987,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7062063160541201,
              "Node id": "73fee44f-f78a-490b-a010-506c0aa781cb"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400."
          },
          {
            "meta": {
              "score": 0.704562012951875,
              "Node id": "668017d0-663c-4a22-b2fd-3f5807d4b270"
            },
            "text": "\\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars ."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The insurer can take several actions if the policyholder fails to fulfill their obligations under the contract. These can include:\n\n1. Cancellation of the policy: The insurer can choose to cancel the policy, effectively ending the coverage. This is usually done after a certain period of non-compliance by the policyholder.\n\n2. Denial of claim: If the policyholder fails to fulfill their obligations and then makes a claim, the insurer can deny this claim. This is often the case if the policyholder has not paid their premiums or has violated terms of the contract.\n\n3. Legal action: In some cases, the insurer may choose to take legal action against the policyholder. This is usually a last resort and is often used in cases of fraud or misrepresentation.\n\n4. Increase in premiums: The insurer may also choose to increase the policyholder's premiums as a result of their failure to fulfill their obligations.\n\n5. Modification of the contract: In some cases, the insurer may choose to modify the terms of the contract to account for the policyholder's failure to fulfill their obligations.\n\nIt's important to note that the specific actions taken can vary depending on the terms of the insurance contract and the laws of the jurisdiction in which the contract was issued.",
              "prompt": {
                "meta": {},
                "user": "What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?",
                "system": "Contexte :  The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation. \n\n Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé. \n\n 6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t. \n\n Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé.  \n \nOutillage  \nTous out ils ou matériels nécessaires à l’exercice de l’activité professionnelle de l’assuré. \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes. \n\n Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge une partie de l’indemnité due après sinistre. Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle. \n\n Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge \nune partie de l’indemnité due après sinistre. Il s’interdit de contracter une \nassurance pour la portion du risque constituée par la franchise.Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant \nde la franchise est de 380 € pour chaque véhicule endommagé. Toutefois, \npour les véhicules terrestres à moteur à usage professionnel, tel que prévu \nà l’article 9 sera appliquée la franchise prévue par le contrat, si celle-ci est \nsupérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre \nsusceptible de faire jouer la garantie dès qu’il en a connaissance et au \nplus tard dans les dix jours suivant la publication de l’arrêté interministériel \nconstatant l’état de catastrophe naturelle. \n\n Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle.\nQuand plusieurs assurances contractées par l’assuré peuvent permettre la réparation des dommages matériels directs \nnon assurables résultant de l’intensité anormale d’un agent naturel, l’assuré doit, en cas de sinistre et dans le délai \nmentionné au précédent alinéa, déclarer l’existence de ces assurances aux assureurs intéressés. \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy. \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n \n La question est What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:09:27.770425",
              "duration": 19.41702,
              "cost": 0.15174
            },
            "meta": {},
            "text": "The insurer can take several actions if the policyholder fails to fulfill their obligations under the contract. These can include:\n\n1. Cancellation of the policy: The insurer can choose to cancel the policy, effectively ending the coverage. This is usually done after a certain period of non-compliance by the policyholder.\n\n2. Denial of claim: If the policyholder fails to fulfill their obligations and then makes a claim, the insurer can deny this claim. This is often the case if the policyholder has not paid their premiums or has violated terms of the contract.\n\n3. Legal action: In some cases, the insurer may choose to take legal action against the policyholder. This is usually a last resort and is often used in cases of fraud or misrepresentation.\n\n4. Increase in premiums: The insurer may also choose to increase the policyholder's premiums as a result of their failure to fulfill their obligations.\n\n5. Modification of the contract: In some cases, the insurer may choose to modify the terms of the contract to account for the policyholder's failure to fulfill their obligations.\n\nIt's important to note that the specific actions taken can vary depending on the terms of the insurance contract and the laws of the jurisdiction in which the contract was issued.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The insurer can take several actions if the policyholder fails to fulfill their obligations under the contract. These can include:\n\n1. Cancellation of the policy: The insurer can choose to cancel the policy, effectively ending the coverage. This is usually done after a certain period of non-compliance by the policyholder.\n\n2. Denial of claim: If the policyholder fails to fulfill their obligations and then makes a claim, the insurer can deny this claim. This is often the case if the policyholder has not paid their premiums or has violated terms of the contract.\n\n3. Legal action: In some cases, the insurer may choose to take legal action against the policyholder. This is usually a last resort and is often used in cases of fraud or misrepresentation.\n\n4. Increase in premiums: The insurer may also choose to increase the policyholder's premiums as a result of their failure to fulfill their obligations.\n\n5. Modification of the contract: In some cases, the insurer may choose to modify the terms of the contract to account for the policyholder's failure to fulfill their obligations.\n\nIt's important to note that the specific actions taken can vary depending on the terms of the insurance contract and the laws of the jurisdiction in which the contract was issued.\n\n1. 1. The insurer has the right to cancel the policy if the policyholder does not meet their obligations.\n2. 2. The insurance company can deny a claim if the policyholder has not fulfilled their responsibilities.\n3. 3. Legal action can be taken by the insurer against the policyholder in certain situations.\n4. 4. The insurer can increase the policyholder's premiums if they fail to meet their obligations.\n5. 5. The terms of the contract can be modified by the insurer due to the policyholder's failure to fulfill their duties.\n6. 6. The actions taken by the insurer can vary based on the insurance contract and the jurisdiction's laws.\n7. 7. The insurer usually cancels the policy after a certain period of non-compliance by the policyholder.\n8. 8. Denial of claim often occurs if the policyholder has not paid their premiums or violated contract terms.\n9. 9. Legal action is typically a last resort and is often used in cases of fraud or misrepresentation.\n10. 10. The insurer may modify the contract to account for the policyholder's failure to fulfill their obligations.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:42.937275",
                "duration": 3.079732,
                "cost": 0.01941
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "If the policyholder fails to fulfill their obligations under the contract, the insurer can take the following actions:\n\n1. **Nullity of the Contract**: The insurer may declare the contract null and void if the policyholder has made a false declaration of risk with the intention to deceive the insurer. This nullity can occur even if the false declaration did not impact any claims (Article L.113-8 of the Insurance Code).\n\n2. **Vices of Consent**: If there are vices of consent such as error, fraud, or violence that are significant enough to influence the decision to enter into the contract, the insurer may consider the contract as if it never existed (Articles 1130 to 1144 of the Civil Code).\n\n3. **Franchise**: The insurer may apply a deductible amount (franchise) that the policyholder is responsible for paying before receiving any indemnity after a claim. The policyholder is prohibited from insuring the portion of the risk covered by the deductible.\n\n4. **Obligation to Declare**: The policyholder is obligated to report any claim that may trigger coverage to the insurer or their local representative as soon as they become aware of it and within a specified timeframe.\n\n5. **Multiple Insurances**: If the policyholder has multiple insurance policies that could cover the same damages, they must declare the existence of these policies to the insurers involved in case of a claim.\n\nFailure to fulfill these obligations by the policyholder may lead to consequences such as denial of coverage, reduction of benefits, or legal actions by the insurer.",
              "prompt": {
                "meta": {},
                "user": "What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?",
                "system": "Contexte :  The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation. \n\n Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé. \n\n 6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn.\nFor oﬄine RL, the trajectories in the dataset are of the form\n(ˆR1,s1,a1,ˆR2,s2,a2,..., ˆRT,sT,aT) (6.38)\nwhere ˆRt=∑T\nt′=trt′isthe“return-to-go”,whichistheamountofreward\nthat needs to be generated to get to the end of the trajectory from t. \n\n Elle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes.  \nLe contrat est considéré comme n’ayant jamais existé.  \n \nOutillage  \nTous out ils ou matériels nécessaires à l’exercice de l’activité professionnelle de l’assuré. \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n CONDITIONS GÉNÉRALES  7 \nNullité du contrat  \nMesure visée par la loi pour rendre nul un contrat pour l’un des motifs suivants :  \n fausse  déclaration volontaire du risque par l’assuré, à la souscription ou en cours de contrat, dans l’intention de tromper l’assureur. \nElle constitue un manquement à l’obligation de contracter de bonne foi. La nullité est encourue même en l’absence  d’incidence de \nla fausse déclaration sur le sinistre (article L.113 -8 du Code des assurances) ;  \nExemples : fausse déclaration sur les antécédents d’assurance, déclaration erronée des conditions d’utilisation.  \n vices du consentement (erreur, dol ou violence - articles 1130  à 1144 du Code civil ) lorsqu’ils sont de telle nature que, sans eux, \nl’une des parties n’aurait pas contracté ou aurait contracté à des conditions substantiellement différentes. \n\n Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge une partie de l’indemnité due après sinistre. Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle. \n\n Franchise\nNonobstant toute disposition contraire, l’assuré conserve à sa charge \nune partie de l’indemnité due après sinistre. Il s’interdit de contracter une \nassurance pour la portion du risque constituée par la franchise.Pour les véhicules terrestres à moteur, quel que soit leur usage, le montant \nde la franchise est de 380 € pour chaque véhicule endommagé. Toutefois, \npour les véhicules terrestres à moteur à usage professionnel, tel que prévu \nà l’article 9 sera appliquée la franchise prévue par le contrat, si celle-ci est \nsupérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre \nsusceptible de faire jouer la garantie dès qu’il en a connaissance et au \nplus tard dans les dix jours suivant la publication de l’arrêté interministériel \nconstatant l’état de catastrophe naturelle. \n\n Il \ns’interdit de contracter une assurance pour la portion du risque constituée par la franchise.\nPour les véhicules terrestres à moteur, quel que soit leur usage, le montant de la franchise est de 380 € pour chaque \nvéhicule endommagé. Toutefois, pour les véhicules terrestres à moteur à usage professionnel, sera appliquée la franchise \nprévue par le contrat, si celle-ci est supérieure.\nObligation de l’assuré\nL’assuré doit déclarer à l’assureur ou à son représentant local tout sinistre susceptible de faire jouer la garantie dès qu’il \nen a connaissance et au plus tard dans les 10 jours suivant la publication de l’arrêté interministériel constatant l’état de \ncatastrophe naturelle.\nQuand plusieurs assurances contractées par l’assuré peuvent permettre la réparation des dommages matériels directs \nnon assurables résultant de l’intensité anormale d’un agent naturel, l’assuré doit, en cas de sinistre et dans le délai \nmentionné au précédent alinéa, déclarer l’existence de ces assurances aux assureurs intéressés. \n\n The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities. \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy. \n\n 178■Transformers for Machine Learning: A Deep Dive\nronment can take. The agent and the environment interact at a sequence\nof time steps t= 0,...,T . At eacht, the agent gets the environment’s\nstatest∈Sand then chooses an action at∈A(s). At time t+ 1, the\nagent receives a reward from the environment rt+1∈Rand the environ-\nment’s state is updated to st+1. This simple idea is illustrated in Fig. 6.5.\nNote that generally speaking, the reward will be a function of the state\nand action, rt=R(st,at). As time progresses, a sequence of states, ac-\ntions, and rewards accumulates: (s0,a0,r1,s1,a1,r2,s2,a2,r3,...). This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. \n\n The task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied.\nThere is one important diﬀerence between the ﬁnite Markov decision\nprocess outlined above and the process used for Decision Transformer.\nDecision Transformer is applied to a type of RL called oﬄine RL. In\noﬄine RL, instead of having the agent interact with an environment and\nbe given a state update after taking an action and getting the reward,\nthere is a ﬁxed dataset that contains trajectories drawn from arbitrary\npolicies. This is a harder way for the agent to learn. \n\n This\nsequence can be called a trajectory.\nIn practice, rtandstare random variables with probability distribu-\ntions. The probability of moving to state s′and earning reward r, given\nthat action awas taken in state sis given by\np(s′,r|s,a) =Prob (st=s′,rt=r|st−1=s,at−1=a)\ns′∈S\nr∈Rp(s′,r|s,a) = 1 ,∀s∈S, a∈A(s) (6.37)\nThetransitionprobabilities pfullycharacterizethedynamicsoftheenvi-\nronment and the agent does not know the set of rewards or the transition\nprobabilities.\nThe task of RL is to learn the probability that the agent chooses\nactionawhile in state s. This probability distribution is called a policy.\n6.7.1 Decision Transformer\nDecision Transformer [41] is the result of an attempt to replace rein-\nforcement learning (RL) methods with a transformer that models the\nsequence of states, actions, and rewards that is used to train an au-\ntonomousagent.Thisapplicationisfarfromthelanguagemodelingtasks\nto which most transformers are applied. \n\n The loss function Lprovides a metric of how close the explanation\nmodel approximates the real model fin the locality of instance x.\n•SHapley Additive exPlanations (SHAP): Shapley values\nhave foundations in game theory, particularly cooperative game\ntheory. One can view each feature as a player in a game, and the\ngoal is to distribute payouts fairly among players who form a coali-\ntion based on their contribution to the total payout. The marginal\ncontribution of each feature averaged across the set of all possible\ncoalitions of features gives the Shapley values.\nLet us represent the entire deep learning model through function\nf, input with dfeatures x={x1,x2,...,xd}, the Shapley value φj\nfor thej-th feature is given by\nφj(f) =|S|!(d−|S|−1)!\nd![f(S∪{xj})−f(S)](7.12) \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n \\n\\nThere ’splenty ofseating onboth the first and second\nfloors and they have arespectable ,ifpredictable ,Scotch\nselection that helps mekeep them inmind when I’m\ndowntown ,soI’msure the rest ofthe liquor options are\npretty good .Coming from Illinois where you can buy booze\natgrocery stores ,it’sdepressing towalk into abar in\nCharlotte and only have one ortwo choices .Cosmos expands\nthehorizons and Iappreciate that .\nNeed aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars . \n\n \n La question est What actions can the insurer take if the policyholder fails to fulfill their obligations under the contract?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:09:36.398649",
              "duration": 8.622481,
              "cost": 0.002747
            },
            "meta": {},
            "text": "If the policyholder fails to fulfill their obligations under the contract, the insurer can take the following actions:\n\n1. **Nullity of the Contract**: The insurer may declare the contract null and void if the policyholder has made a false declaration of risk with the intention to deceive the insurer. This nullity can occur even if the false declaration did not impact any claims (Article L.113-8 of the Insurance Code).\n\n2. **Vices of Consent**: If there are vices of consent such as error, fraud, or violence that are significant enough to influence the decision to enter into the contract, the insurer may consider the contract as if it never existed (Articles 1130 to 1144 of the Civil Code).\n\n3. **Franchise**: The insurer may apply a deductible amount (franchise) that the policyholder is responsible for paying before receiving any indemnity after a claim. The policyholder is prohibited from insuring the portion of the risk covered by the deductible.\n\n4. **Obligation to Declare**: The policyholder is obligated to report any claim that may trigger coverage to the insurer or their local representative as soon as they become aware of it and within a specified timeframe.\n\n5. **Multiple Insurances**: If the policyholder has multiple insurance policies that could cover the same damages, they must declare the existence of these policies to the insurers involved in case of a claim.\n\nFailure to fulfill these obligations by the policyholder may lead to consequences such as denial of coverage, reduction of benefits, or legal actions by the insurer.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 10",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: If the policyholder fails to fulfill their obligations under the contract, the insurer can take the following actions:\n\n1. **Nullity of the Contract**: The insurer may declare the contract null and void if the policyholder has made a false declaration of risk with the intention to deceive the insurer. This nullity can occur even if the false declaration did not impact any claims (Article L.113-8 of the Insurance Code).\n\n2. **Vices of Consent**: If there are vices of consent such as error, fraud, or violence that are significant enough to influence the decision to enter into the contract, the insurer may consider the contract as if it never existed (Articles 1130 to 1144 of the Civil Code).\n\n3. **Franchise**: The insurer may apply a deductible amount (franchise) that the policyholder is responsible for paying before receiving any indemnity after a claim. The policyholder is prohibited from insuring the portion of the risk covered by the deductible.\n\n4. **Obligation to Declare**: The policyholder is obligated to report any claim that may trigger coverage to the insurer or their local representative as soon as they become aware of it and within a specified timeframe.\n\n5. **Multiple Insurances**: If the policyholder has multiple insurance policies that could cover the same damages, they must declare the existence of these policies to the insurers involved in case of a claim.\n\nFailure to fulfill these obligations by the policyholder may lead to consequences such as denial of coverage, reduction of benefits, or legal actions by the insurer.\n\n1. 1. The insurer has the right to cancel the policy if the policyholder does not meet their obligations.\n2. 2. The insurance company can deny a claim if the policyholder has not fulfilled their responsibilities.\n3. 3. Legal action can be taken by the insurer against the policyholder in certain situations.\n4. 4. The insurer can increase the policyholder's premiums if they fail to meet their obligations.\n5. 5. The terms of the contract can be modified by the insurer due to the policyholder's failure to fulfill their duties.\n6. 6. The actions taken by the insurer can vary based on the insurance contract and the jurisdiction's laws.\n7. 7. The insurer usually cancels the policy after a certain period of non-compliance by the policyholder.\n8. 8. Denial of claim often occurs if the policyholder has not paid their premiums or violated contract terms.\n9. 9. Legal action is typically a last resort and is often used in cases of fraud or misrepresentation.\n10. 10. The insurer may modify the contract to account for the policyholder's failure to fulfill their obligations.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:44.435845",
                "duration": 1.495877,
                "cost": 0.02019
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '10'] - Not validated: ['4', '5', '6', '7', '8', '9']",
              "human": 1.0,
              "auto": 0.4
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What are the consequences of not respecting the deadlines for reporting a claim?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text does not discuss the repercussions of missing claim reporting deadlines.\n2. There is no information in the text about what happens if claim reporting deadlines are not met.\n3. The consequences of failing to report a claim on time are not addressed in the text.\n4. The text lacks details on the outcomes of not adhering to claim reporting deadlines.\n5. The text does not specify the effects of not complying with the deadlines for claim reporting.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on the consequences of not respecting the deadlines for reporting a claim.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:25:20.721527",
          "duration": 6.186513,
          "cost": 0.00948
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text does not discuss the repercussions of missing claim reporting deadlines."
          },
          {
            "meta": {},
            "text": "2. There is no information in the text about what happens if claim reporting deadlines are not met."
          },
          {
            "meta": {},
            "text": "3. The consequences of failing to report a claim on time are not addressed in the text."
          },
          {
            "meta": {},
            "text": "4. The text lacks details on the outcomes of not adhering to claim reporting deadlines."
          },
          {
            "meta": {},
            "text": "5. The text does not specify the effects of not complying with the deadlines for claim reporting."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.795321737560304,
              "Node id": "60059557-6ca2-4b37-bb77-4733ba68ca09"
            },
            "text": "They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized."
          },
          {
            "meta": {
              "score": 8.472932076052036,
              "Node id": "20c2ab66-3721-43ad-8b3a-19be694d4099"
            },
            "text": "Similartothepermutationattentionweightsexperiments,theresults\nfor the adversarial attention experiments also varied based on the data.\nThe attention distribution does not uniquely characterize why a par-\nticular prediction was made by the model as you can ﬁnd alternative\nattention heatmaps that give the same predictions .\nThe overall conclusion drawn from the research was—attention does\nnot provide a consistent interpretation of why a model made a particular\nprediction.\n7.4.2 Attention is Not Not an Explanation\nWiegreﬀe and Pinter contest the claims and assumptions made by Jain\nand Wallace in the research above and show through experiments the\nusefulness of attention mechanisms for explainability [269]. Wiegreﬀe\nand Pinter’s main contention from Jain and Wallace’s research is that\ntheir “explanation” study is ambiguous, correlation experiments are in-\nsuﬃcient, and the adversarial weight experiments were vague and im-\npractical."
          },
          {
            "meta": {
              "score": 8.244514255959597,
              "Node id": "d48d8f81-31b4-4755-a94a-17d1feb2ba07"
            },
            "text": "They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method."
          },
          {
            "meta": {
              "score": 8.12908550370476,
              "Node id": "0c1dbf3c-f46f-46a8-9774-def66cf09e36"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 7.675484776668124,
              "Node id": "1a73d673-33dd-4822-a68e-ba0c26e43a04"
            },
            "text": "They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training."
          },
          {
            "meta": {
              "score": 7.487562840091617,
              "Node id": "6181ddc7-3b1e-4409-ac6c-fa555c24ff95"
            },
            "text": "To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al."
          },
          {
            "meta": {
              "score": 7.352551269467137,
              "Node id": "89ad8aea-63bf-46d2-8e3f-389ac42059e6"
            },
            "text": "They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance."
          },
          {
            "meta": {
              "score": 7.226342508751951,
              "Node id": "1f27ffbe-1e3a-4b63-9dc3-32ed13ebc7d6"
            },
            "text": "2 DIRECT ASSURANCE AUTOGUIDE \nDE VOS \nGARANTIES\nSans valeur contractuelle\nPour le détail de vos garanties vous devez  \nvous reporter aux conditions générales page 13"
          },
          {
            "meta": {
              "score": 7.194590806201857,
              "Node id": "58c52e91-de4b-4fc1-8e03-aebf4a521732"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 6.716234572975095,
              "Node id": "fbbb9cf3-3ca1-4bd9-8c34-74d1585cc7cb"
            },
            "text": "Le contr at est susp endu\nautomatiquement à partir du lendemain à 0 heur e du\njour de la cession.\nque l’héritier nous avise dans les dix jours du transfer t de\npropriété du véhicule à son nom.  L’assur ance est alors\ntransférée automatiquement,  s’il peut devenir sociétair e*\nde la Macif et à charge pour lui de respecter les\nobligations du contr at.\nSi un sinistr e* surv enait après le transfer t de propriété et\navant que nous en ayions connaissanc e, l’indemnité due\npourr ait être réduite dans la proportion de la cotisation\nversée par rapport à celle due.M M\nVous pouvez vous reporter, pour les cas envisagés ci-dessus,  aux disp ositions relativ es aux\npossibilités de résiliation ﬁgur ant au chapitr e 3 « Vie du contr at », au paragraphe «Fin du\ncontr at».M1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page58"
          },
          {
            "meta": {
              "score": 0.734637148435988,
              "Node id": "5bc639ab-2a6b-4b8c-ac44-e39f867386bb"
            },
            "text": "Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE !"
          },
          {
            "meta": {
              "score": 0.7335600306237502,
              "Node id": "c519dce1-9d5f-44a7-b859-9657bbd78629"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600."
          },
          {
            "meta": {
              "score": 0.7332835230094318,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7317212015857189,
              "Node id": "20010ae9-c968-4c58-8312-204c37d847a5"
            },
            "text": "It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics."
          },
          {
            "meta": {
              "score": 0.7300795910684044,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 0.7294906760171604,
              "Node id": "3063710c-86bf-4f4e-9fd2-715230580212"
            },
            "text": "The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]."
          },
          {
            "meta": {
              "score": 0.7290520276875507,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7255635014890428,
              "Node id": "55ff27c9-0d8d-492c-99f3-014cd0d0b3e1"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies."
          },
          {
            "meta": {
              "score": 0.7240561736604496,
              "Node id": "72c0c068-6d28-4724-a9de-815e352cf5e6"
            },
            "text": "To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis."
          },
          {
            "meta": {
              "score": 0.7238282794355347,
              "Node id": "552b81e8-555c-4749-a44e-e0e3fdd9f077"
            },
            "text": "[20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on the consequences of not respecting the deadlines for reporting a claim.",
              "prompt": {
                "meta": {},
                "user": "What are the consequences of not respecting the deadlines for reporting a claim?",
                "system": "Contexte :  They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized. \n\n Similartothepermutationattentionweightsexperiments,theresults\nfor the adversarial attention experiments also varied based on the data.\nThe attention distribution does not uniquely characterize why a par-\nticular prediction was made by the model as you can ﬁnd alternative\nattention heatmaps that give the same predictions .\nThe overall conclusion drawn from the research was—attention does\nnot provide a consistent interpretation of why a model made a particular\nprediction.\n7.4.2 Attention is Not Not an Explanation\nWiegreﬀe and Pinter contest the claims and assumptions made by Jain\nand Wallace in the research above and show through experiments the\nusefulness of attention mechanisms for explainability [269]. Wiegreﬀe\nand Pinter’s main contention from Jain and Wallace’s research is that\ntheir “explanation” study is ambiguous, correlation experiments are in-\nsuﬃcient, and the adversarial weight experiments were vague and im-\npractical. \n\n They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. \n\n To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. \n\n They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. \n\n 2 DIRECT ASSURANCE AUTOGUIDE \nDE VOS \nGARANTIES\nSans valeur contractuelle\nPour le détail de vos garanties vous devez  \nvous reporter aux conditions générales page 13 \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Le contr at est susp endu\nautomatiquement à partir du lendemain à 0 heur e du\njour de la cession.\nque l’héritier nous avise dans les dix jours du transfer t de\npropriété du véhicule à son nom.  L’assur ance est alors\ntransférée automatiquement,  s’il peut devenir sociétair e*\nde la Macif et à charge pour lui de respecter les\nobligations du contr at.\nSi un sinistr e* surv enait après le transfer t de propriété et\navant que nous en ayions connaissanc e, l’indemnité due\npourr ait être réduite dans la proportion de la cotisation\nversée par rapport à celle due.M M\nVous pouvez vous reporter, pour les cas envisagés ci-dessus,  aux disp ositions relativ es aux\npossibilités de résiliation ﬁgur ant au chapitr e 3 « Vie du contr at », au paragraphe «Fin du\ncontr at».M1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page58 \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis. \n\n [20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019. \n\n \n La question est What are the consequences of not respecting the deadlines for reporting a claim?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:09:38.732552",
              "duration": 2.328387,
              "cost": 0.12474
            },
            "meta": {},
            "text": "The text does not provide information on the consequences of not respecting the deadlines for reporting a claim.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on the consequences of not respecting the deadlines for reporting a claim.\n\n1. 1. The text does not discuss the repercussions of missing claim reporting deadlines.\n2. 2. There is no information in the text about what happens if claim reporting deadlines are not met.\n3. 3. The consequences of failing to report a claim on time are not addressed in the text.\n4. 4. The text lacks details on the outcomes of not adhering to claim reporting deadlines.\n5. 5. The text does not specify the effects of not complying with the deadlines for claim reporting.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:46.595664",
                "duration": 2.1566440000000004,
                "cost": 0.00759
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Not respecting the deadlines for reporting a claim can have several consequences, which may vary depending on the specific insurance policy and the circumstances surrounding the claim. Here are some common consequences:\n\n1. **Loss of Coverage:** Failing to report a claim within the specified deadline may result in the insurance company denying coverage for that claim. This means you may have to bear the financial burden of the loss or damage on your own.\n\n2. **Delayed Processing:** Late reporting can lead to delays in the processing of your claim. This can prolong the time it takes to receive compensation or assistance from the insurance company.\n\n3. **Evidence Collection Challenges:** The longer you wait to report a claim, the more difficult it may be to gather evidence to support your claim. This can weaken your case and may result in a lower settlement amount.\n\n4. **Legal Issues:** In some cases, failing to report a claim on time could lead to legal complications. It may breach the terms of your insurance contract, and the insurance company may take legal action or refuse to renew your policy in the future.\n\n5. **Loss of Rights:** Insurance policies often have specific clauses regarding the reporting of claims. If you fail to adhere to these clauses, you may lose certain rights or benefits that you would have otherwise been entitled to.\n\n6. **Increased Premiums:** Late reporting of claims can also impact your future insurance premiums. Insurance companies may view delayed reporting as a sign of increased risk, leading to higher premiums when you renew your policy.\n\nIt is essential to familiarize yourself with the reporting requirements outlined in your insurance policy and ensure timely reporting of any claims to avoid these potential consequences.",
              "prompt": {
                "meta": {},
                "user": "What are the consequences of not respecting the deadlines for reporting a claim?",
                "system": "Contexte :  They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. [298] also report comparable results when the representations\nof word pairs are explicitly aligned and the vector spaces are further nor-\nmalized. \n\n Similartothepermutationattentionweightsexperiments,theresults\nfor the adversarial attention experiments also varied based on the data.\nThe attention distribution does not uniquely characterize why a par-\nticular prediction was made by the model as you can ﬁnd alternative\nattention heatmaps that give the same predictions .\nThe overall conclusion drawn from the research was—attention does\nnot provide a consistent interpretation of why a model made a particular\nprediction.\n7.4.2 Attention is Not Not an Explanation\nWiegreﬀe and Pinter contest the claims and assumptions made by Jain\nand Wallace in the research above and show through experiments the\nusefulness of attention mechanisms for explainability [269]. Wiegreﬀe\nand Pinter’s main contention from Jain and Wallace’s research is that\ntheir “explanation” study is ambiguous, correlation experiments are in-\nsuﬃcient, and the adversarial weight experiments were vague and im-\npractical. \n\n They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. \n\n To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. Taking this\ninto account, Wang et al. [263] provide an explicit strategy for align-\ning the representations of matched word pairs across languages during\nmBERT training. This is accomplished by including a loss function that\nminimizes the Euclidean distance between aligned words’ embeddings.\nZhao et al. \n\n They demonstrate how knowledge distillation from a monolingual\nmodel may enhance a mLM’s cross-lingual performance.\n4.3.1.3 Model tasks factors\nFine-tuning Strategies In [171] it is discussed that ﬁne-tuning a\nmLM changes its parameters, impairing its cross-lingual capacity by\nerasing part of the alignments learned during pre-training. They demon-\nstrate this by showing that when a mLM is ﬁne-tuned for POS tagging,\nits cross-lingual retrieval performance decreases signiﬁcantly. To over-\ncome this issue, they recommend utilizing a continual learning frame-\nwork for ﬁne-tuning the model so that it does not forget the original\ntask (MLM) on which it was trained. They claim improved results in\ncross-lingual POS tagging, NER, and sentence retrieval using this ﬁne-\ntuning method.\nRepresentations Alignment In [261, 168] the performance of zero-\nshot cross-lingual transfer is examined using implicitly aligned represen-\ntations learned by mLMs and representations from monolingual models\nthataresubsequentlyexplicitlyalignedusingparallelcorpora.Theynote\nthat the explicit alignment provide improved performance. \n\n 2 DIRECT ASSURANCE AUTOGUIDE \nDE VOS \nGARANTIES\nSans valeur contractuelle\nPour le détail de vos garanties vous devez  \nvous reporter aux conditions générales page 13 \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n Le contr at est susp endu\nautomatiquement à partir du lendemain à 0 heur e du\njour de la cession.\nque l’héritier nous avise dans les dix jours du transfer t de\npropriété du véhicule à son nom.  L’assur ance est alors\ntransférée automatiquement,  s’il peut devenir sociétair e*\nde la Macif et à charge pour lui de respecter les\nobligations du contr at.\nSi un sinistr e* surv enait après le transfer t de propriété et\navant que nous en ayions connaissanc e, l’indemnité due\npourr ait être réduite dans la proportion de la cotisation\nversée par rapport à celle due.M M\nVous pouvez vous reporter, pour les cas envisagés ci-dessus,  aux disp ositions relativ es aux\npossibilités de résiliation ﬁgur ant au chapitr e 3 « Vie du contr at », au paragraphe «Fin du\ncontr at».M1-int 12-13 actua 01-15 [VEH-AUTO-19 - 04-15 - N828]_CG Automobile  12/09/17  08:31  Page58 \n\n Need aplace inCharlotte toget black out drunk without fear\nofwaking upmissing vital organs ?Than the Westin\nCharlotte isthe place for you .Was witness toanamateur\ndrunk who passed out .Rather than doing the typical bounce\nandtoss the staff gave genuine care and concern tothe\nhelp him through the this moment ofself reflection .One\nemployee went sofar astoice his neck inhope tohelp him\nsober up,ifnothing else atleast remeber his name .Even\nwhen the EMTs arrived and drunk punches were thrown the\nstaff stood stoically ready toshow their support for their\ncustomer .So,itbecame adinner and ashow .Five stars .\nNERD RAGE ! \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n It has been shown that models with high conﬁdence in pre-\ndiction do not guarantee trustworthiness [95, 191,189,291]. Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies. \n\n To test the hypothesized ro-\nbustness, they authors add six kinds of noise to the data and see how it\naﬀects performance:\n1. Make deletions by giving each character has a 10%chance of being\ndeleted.\n2. Give each character a 10%chance of added, deleted, or mutated\n(with equal likelihood).\n3. Give each character a 20%chance of being duplicated 1-3 times.\n4. Capitalize each character and pad it with spaces.\n5. Make each character uppercase, when the language uses case.\n6. Set the case of each character randomly when the language uses\ncase.\nThe noise is injected in one of two ways: into the ﬁne-tuning and\nevaluation data or just into the evaluation data. For the question an-\nswering task, noise is added to the context but not to the question or\nanswer. For the sentence entailment training task, noise is added to the\npremise and hypothesis. \n\n [20]S. Barocas and D. Boyd ,Engaging the ethics of data science\nin practice , Communications of the ACM, 60 (2017), pp. 23–25.\n[21]O. Bastani, C. Kim, and H. Bastani ,Interpreting black-\nbox models via model extraction , arXiv preprint arXiv:1705.08504,\n(2017).\n[22]K. Baum, M. A. Köhl, and E. Schmidt ,Two challenges for ci\ntrustworthiness and how to address them , in Proceedings of the 1st\nWorkshop on Explainable Computational Intelligence (XCI 2017),\n2017.\n[23]Y. Belinkov ,Probing classiﬁers: Promises, shortcomings, and\nalternatives , arXiv preprint arXiv:2102.12452, (2021).\n[24]I. Beltagy, K. Lo, and A. Cohan ,Scibert: A pretrained lan-\nguage model for scientiﬁc text , in EMNLP, 2019. \n\n \n La question est What are the consequences of not respecting the deadlines for reporting a claim?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:09:47.564084",
              "duration": 8.827569,
              "cost": 0.0025534999999999998
            },
            "meta": {},
            "text": "Not respecting the deadlines for reporting a claim can have several consequences, which may vary depending on the specific insurance policy and the circumstances surrounding the claim. Here are some common consequences:\n\n1. **Loss of Coverage:** Failing to report a claim within the specified deadline may result in the insurance company denying coverage for that claim. This means you may have to bear the financial burden of the loss or damage on your own.\n\n2. **Delayed Processing:** Late reporting can lead to delays in the processing of your claim. This can prolong the time it takes to receive compensation or assistance from the insurance company.\n\n3. **Evidence Collection Challenges:** The longer you wait to report a claim, the more difficult it may be to gather evidence to support your claim. This can weaken your case and may result in a lower settlement amount.\n\n4. **Legal Issues:** In some cases, failing to report a claim on time could lead to legal complications. It may breach the terms of your insurance contract, and the insurance company may take legal action or refuse to renew your policy in the future.\n\n5. **Loss of Rights:** Insurance policies often have specific clauses regarding the reporting of claims. If you fail to adhere to these clauses, you may lose certain rights or benefits that you would have otherwise been entitled to.\n\n6. **Increased Premiums:** Late reporting of claims can also impact your future insurance premiums. Insurance companies may view delayed reporting as a sign of increased risk, leading to higher premiums when you renew your policy.\n\nIt is essential to familiarize yourself with the reporting requirements outlined in your insurance policy and ensure timely reporting of any claims to avoid these potential consequences.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Not respecting the deadlines for reporting a claim can have several consequences, which may vary depending on the specific insurance policy and the circumstances surrounding the claim. Here are some common consequences:\n\n1. **Loss of Coverage:** Failing to report a claim within the specified deadline may result in the insurance company denying coverage for that claim. This means you may have to bear the financial burden of the loss or damage on your own.\n\n2. **Delayed Processing:** Late reporting can lead to delays in the processing of your claim. This can prolong the time it takes to receive compensation or assistance from the insurance company.\n\n3. **Evidence Collection Challenges:** The longer you wait to report a claim, the more difficult it may be to gather evidence to support your claim. This can weaken your case and may result in a lower settlement amount.\n\n4. **Legal Issues:** In some cases, failing to report a claim on time could lead to legal complications. It may breach the terms of your insurance contract, and the insurance company may take legal action or refuse to renew your policy in the future.\n\n5. **Loss of Rights:** Insurance policies often have specific clauses regarding the reporting of claims. If you fail to adhere to these clauses, you may lose certain rights or benefits that you would have otherwise been entitled to.\n\n6. **Increased Premiums:** Late reporting of claims can also impact your future insurance premiums. Insurance companies may view delayed reporting as a sign of increased risk, leading to higher premiums when you renew your policy.\n\nIt is essential to familiarize yourself with the reporting requirements outlined in your insurance policy and ensure timely reporting of any claims to avoid these potential consequences.\n\n1. 1. The text does not discuss the repercussions of missing claim reporting deadlines.\n2. 2. There is no information in the text about what happens if claim reporting deadlines are not met.\n3. 3. The consequences of failing to report a claim on time are not addressed in the text.\n4. 4. The text lacks details on the outcomes of not adhering to claim reporting deadlines.\n5. 5. The text does not specify the effects of not complying with the deadlines for claim reporting.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:48.616517",
                "duration": 2.018499,
                "cost": 0.01689
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the prescription period interrupted by a conservatory measure or forced execution act?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The prescription period can be interrupted by a conservatory measure or forced execution act.\n2. The interruption of the prescription period is in line with article L114-2 of the Insurance Code.\n3. Any act of forced execution can cause an interruption in the prescription period.\n4. A conservatory measure taken in accordance with the code of civil execution procedures can also interrupt the prescription period.\n5. Legal action taken to enforce a judgement can interrupt the prescription period.\n6. A precautionary measure taken to preserve the rights of a party can also cause an interruption.\n7. The interruption effectively resets the prescription period.\n8. The reset of the prescription period allows for additional time to pursue a claim or action.",
          "prompt": {
            "meta": {},
            "user": "The prescription period is interrupted by a conservatory measure or forced execution act in accordance with the article L114-2 of the Insurance Code. This interruption can occur through any act of forced execution or any conservatory measure taken in accordance with the code of civil execution procedures. This means that any legal action taken to enforce a judgement or any precautionary measure taken to preserve the rights of a party can interrupt the prescription period. This interruption effectively resets the prescription period, allowing for additional time to pursue a claim or action.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:25:31.467185",
          "duration": 10.740515,
          "cost": 0.015120000000000001
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The prescription period can be interrupted by a conservatory measure or forced execution act."
          },
          {
            "meta": {},
            "text": "2. The interruption of the prescription period is in line with article L114-2 of the Insurance Code."
          },
          {
            "meta": {},
            "text": "3. Any act of forced execution can cause an interruption in the prescription period."
          },
          {
            "meta": {},
            "text": "4. A conservatory measure taken in accordance with the code of civil execution procedures can also interrupt the prescription period."
          },
          {
            "meta": {},
            "text": "5. Legal action taken to enforce a judgement can interrupt the prescription period."
          },
          {
            "meta": {},
            "text": "6. A precautionary measure taken to preserve the rights of a party can also cause an interruption."
          },
          {
            "meta": {},
            "text": "7. The interruption effectively resets the prescription period."
          },
          {
            "meta": {},
            "text": "8. The reset of the prescription period allows for additional time to pursue a claim or action."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.551450383281832,
              "Node id": "3554d232-0c37-416a-9cf1-37c7da59c252"
            },
            "text": "Quand l’action de l’assuré contre l’assureur a pour cause le recours d’un \ntiers, le délai de prescription ne court que du jour où ce tiers a exercé une \naction en justice contre l’assuré ou a été indemnisé par ce dernier.\nLa prescription est portée à dix ans dans les contrats d’assurance contre \nles accidents atteignant les personnes, lorsque les bénéficiaires sont les \nayants-droit de l’assuré décédé.\nConformément à l’article L114-2 du Code des assurances, la prescription est \ninterrompue par une des causes ordinaires d’interruption de la prescription \nvisée ci-après :\n• toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ;\n• tout acte d’exécution forcée, ou toute mesure conservatoire prise en \napplication du code des procédures civiles d’exécution ;\n• toute reconnaissance par l’assureur du droit à garantie de l’assuré, ou \ntoute reconnaissance de dette de l’assuré envers l’assureur ; \n• tout recours à la médiation ou à la conciliation lorsque la partie est dans \nl’impossibilité d’agir par suite d’un empêchement résultant de la loi, de la \nconvention d’assistance ou de la Force Majeure."
          },
          {
            "meta": {
              "score": 11.69615912320061,
              "Node id": "a8ebbe01-9246-4401-8a9c-1468bce0cdff"
            },
            "text": "65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance."
          },
          {
            "meta": {
              "score": 11.53955890462927,
              "Node id": "02e1d6ad-f30d-414f-a29c-f9bd01df0e95"
            },
            "text": "Article 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée."
          },
          {
            "meta": {
              "score": 11.165810491770188,
              "Node id": "fa0398d3-c7bf-4114-9018-8b78f1d6af54"
            },
            "text": "Passé ce délai, il y a prescription : toute action dérivant \ndu contrat d’assurance est éteinte.\nLe délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),"
          },
          {
            "meta": {
              "score": 11.165810491770188,
              "Node id": "639716b3-1743-490f-9baf-32f0b624ad97"
            },
            "text": "Le délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),"
          },
          {
            "meta": {
              "score": 11.165810491770188,
              "Node id": "1f3a6887-3a18-4f9d-bb9a-3745d2cd29b9"
            },
            "text": "ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,"
          },
          {
            "meta": {
              "score": 11.046326095695138,
              "Node id": "e4465ccc-dda3-47eb-95d9-2c568c2a4898"
            },
            "text": "Toutefois, ce délai ne court :  \n▪ en cas de réticence, omission, déclaration fausse ou inexacte sur le risque couru, que du jour où l’Assureur  en a eu \nconnaissance,  \n▪ en cas de Sinistre , que du jour où les intéressés en ont eu connaissance, s’ils prouvent qu’ils l’ont ignoré jusque -là. \n \nQuand l’action de l ’Assuré  contre l’Assureur  a pour cause le recours d’un Tiers , le délai de la prescript ion ne court que du jour \noù ce t iers a exercé une action en justice contre l’ Assuré  ou a été indemnisé par ce dernier.  \n \nConformément à l’article L.114 -2 du Code des assurances, la prescription est interrompue par une des causes ordinaires \nd’interruption de la prescription constituées par : toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ; tout acte d’exécution forcée, ou toute mesure conservatoire prise en application du code des \nprocédures civil es d’exécution ;  toute reconnaissance par l ’Assureur  du droit à garantie de l’ Assuré , ou toute reconnaissance \nde dette de l’ Assuré  envers l’ Assureur ."
          },
          {
            "meta": {
              "score": 11.023003300522765,
              "Node id": "686d5c73-459c-4bd4-aeb8-d5b08b9c2a25"
            },
            "text": "- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans,"
          },
          {
            "meta": {
              "score": 10.815512808820447,
              "Node id": "d947f396-3cb4-441a-9ddf-125532cd982c"
            },
            "text": "65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée."
          },
          {
            "meta": {
              "score": 10.815512808820447,
              "Node id": "6294071f-a379-4c92-82b1-4de556c5d7a2"
            },
            "text": "- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, \ndésistement ou rejet définitif de la demande de celui qui agissait en justice."
          },
          {
            "meta": {
              "score": 0.7287817779355413,
              "Node id": "55d9ed11-8f4a-4bbe-88d8-a374febff098"
            },
            "text": "To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above."
          },
          {
            "meta": {
              "score": 0.7199059856207919,
              "Node id": "80d975b0-c885-4650-b3b3-a66ab2baee77"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 0.7198513767206849,
              "Node id": "eb120220-ab1b-4060-81f8-e90714516c7e"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          },
          {
            "meta": {
              "score": 0.7191558711996532,
              "Node id": "24a4d5c0-f074-4452-826d-38ce0ee55a4f"
            },
            "text": "Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues."
          },
          {
            "meta": {
              "score": 0.7190111693130223,
              "Node id": "22d860c8-3a90-4d07-ab32-160e5f174c85"
            },
            "text": "To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT."
          },
          {
            "meta": {
              "score": 0.7185707689803094,
              "Node id": "417fe1e0-ac85-4c9c-84be-650b6d0505ab"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers."
          },
          {
            "meta": {
              "score": 0.7175671209589721,
              "Node id": "c0c4a6a9-c8e3-42d6-ab45-c07a0bac8553"
            },
            "text": "To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto."
          },
          {
            "meta": {
              "score": 0.717224353340358,
              "Node id": "3a2c5014-8f05-4801-b4be-8e46a890b7f4"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 0.7170370165269915,
              "Node id": "73fee44f-f78a-490b-a010-506c0aa781cb"
            },
            "text": "The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400."
          },
          {
            "meta": {
              "score": 0.7168897771433578,
              "Node id": "caf645f8-31dd-4c59-b934-6d7260464148"
            },
            "text": "Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The prescription period is interrupted by a conservatory measure or forced execution act in accordance with the article L114-2 of the Insurance Code. This interruption can occur through any act of forced execution or any conservatory measure taken in accordance with the code of civil execution procedures. This means that any legal action taken to enforce a judgement or any precautionary measure taken to preserve the rights of a party can interrupt the prescription period. This interruption effectively resets the prescription period, allowing for additional time to pursue a claim or action.",
              "prompt": {
                "meta": {},
                "user": "How is the prescription period interrupted by a conservatory measure or forced execution act?",
                "system": "Contexte :  Quand l’action de l’assuré contre l’assureur a pour cause le recours d’un \ntiers, le délai de prescription ne court que du jour où ce tiers a exercé une \naction en justice contre l’assuré ou a été indemnisé par ce dernier.\nLa prescription est portée à dix ans dans les contrats d’assurance contre \nles accidents atteignant les personnes, lorsque les bénéficiaires sont les \nayants-droit de l’assuré décédé.\nConformément à l’article L114-2 du Code des assurances, la prescription est \ninterrompue par une des causes ordinaires d’interruption de la prescription \nvisée ci-après :\n• toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ;\n• tout acte d’exécution forcée, ou toute mesure conservatoire prise en \napplication du code des procédures civiles d’exécution ;\n• toute reconnaissance par l’assureur du droit à garantie de l’assuré, ou \ntoute reconnaissance de dette de l’assuré envers l’assureur ; \n• tout recours à la médiation ou à la conciliation lorsque la partie est dans \nl’impossibilité d’agir par suite d’un empêchement résultant de la loi, de la \nconvention d’assistance ou de la Force Majeure. \n\n 65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance. \n\n Article 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée. \n\n Passé ce délai, il y a prescription : toute action dérivant \ndu contrat d’assurance est éteinte.\nLe délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie), \n\n Le délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire), \n\n ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure, \n\n Toutefois, ce délai ne court :  \n▪ en cas de réticence, omission, déclaration fausse ou inexacte sur le risque couru, que du jour où l’Assureur  en a eu \nconnaissance,  \n▪ en cas de Sinistre , que du jour où les intéressés en ont eu connaissance, s’ils prouvent qu’ils l’ont ignoré jusque -là. \n \nQuand l’action de l ’Assuré  contre l’Assureur  a pour cause le recours d’un Tiers , le délai de la prescript ion ne court que du jour \noù ce t iers a exercé une action en justice contre l’ Assuré  ou a été indemnisé par ce dernier.  \n \nConformément à l’article L.114 -2 du Code des assurances, la prescription est interrompue par une des causes ordinaires \nd’interruption de la prescription constituées par : toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ; tout acte d’exécution forcée, ou toute mesure conservatoire prise en application du code des \nprocédures civil es d’exécution ;  toute reconnaissance par l ’Assureur  du droit à garantie de l’ Assuré , ou toute reconnaissance \nde dette de l’ Assuré  envers l’ Assureur . \n\n - soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, \n\n 65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée. \n\n - soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, \ndésistement ou rejet définitif de la demande de celui qui agissait en justice. \n\n To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”. \n\n \n La question est How is the prescription period interrupted by a conservatory measure or forced execution act?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:09:54.955049",
              "duration": 7.387721,
              "cost": 0.1368
            },
            "meta": {},
            "text": "The prescription period is interrupted by a conservatory measure or forced execution act in accordance with the article L114-2 of the Insurance Code. This interruption can occur through any act of forced execution or any conservatory measure taken in accordance with the code of civil execution procedures. This means that any legal action taken to enforce a judgement or any precautionary measure taken to preserve the rights of a party can interrupt the prescription period. This interruption effectively resets the prescription period, allowing for additional time to pursue a claim or action.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 3, 4, 5, 6, 7, 8",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The prescription period is interrupted by a conservatory measure or forced execution act in accordance with the article L114-2 of the Insurance Code. This interruption can occur through any act of forced execution or any conservatory measure taken in accordance with the code of civil execution procedures. This means that any legal action taken to enforce a judgement or any precautionary measure taken to preserve the rights of a party can interrupt the prescription period. This interruption effectively resets the prescription period, allowing for additional time to pursue a claim or action.\n\n1. 1. The prescription period can be interrupted by a conservatory measure or forced execution act.\n2. 2. The interruption of the prescription period is in line with article L114-2 of the Insurance Code.\n3. 3. Any act of forced execution can cause an interruption in the prescription period.\n4. 4. A conservatory measure taken in accordance with the code of civil execution procedures can also interrupt the prescription period.\n5. 5. Legal action taken to enforce a judgement can interrupt the prescription period.\n6. 6. A precautionary measure taken to preserve the rights of a party can also cause an interruption.\n7. 7. The interruption effectively resets the prescription period.\n8. 8. The reset of the prescription period allows for additional time to pursue a claim or action.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:51.284682",
                "duration": 2.666049,
                "cost": 0.01248
              },
              "meta": {},
              "text": "Validated: ['1', '2', '3', '4', '5', '6', '7', '8'] - Not validated: []",
              "human": 1.0,
              "auto": 1.0
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The prescription period is interrupted by a conservatory measure or forced execution act in the following ways:\n\n1. **Recognition of Right to Guarantee**: If the insurer recognizes the insured's right to benefit from the contested guarantee, the prescription period is interrupted.\n\n2. **Forced Execution**: Any act of forced execution, such as a payment order or seizure, can interrupt the prescription period.\n\n3. **Conservatory Measure**: If a conservatory measure is taken in accordance with the civil enforcement procedures code, like a conservatory seizure, the prescription period is interrupted.\n\n4. **Legal Action**: Initiating legal action, including summary proceedings, even if brought before an incompetent court or if the filing is annulled due to a procedural defect, can interrupt the prescription period.\n\nIn all these cases, the interruption of the prescription period lasts until the conclusion of the legal proceedings, unless there is a lack of action by the parties for two years, withdrawal, or definitive rejection of the claim by the party taking legal action.",
              "prompt": {
                "meta": {},
                "user": "How is the prescription period interrupted by a conservatory measure or forced execution act?",
                "system": "Contexte :  Quand l’action de l’assuré contre l’assureur a pour cause le recours d’un \ntiers, le délai de prescription ne court que du jour où ce tiers a exercé une \naction en justice contre l’assuré ou a été indemnisé par ce dernier.\nLa prescription est portée à dix ans dans les contrats d’assurance contre \nles accidents atteignant les personnes, lorsque les bénéficiaires sont les \nayants-droit de l’assuré décédé.\nConformément à l’article L114-2 du Code des assurances, la prescription est \ninterrompue par une des causes ordinaires d’interruption de la prescription \nvisée ci-après :\n• toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ;\n• tout acte d’exécution forcée, ou toute mesure conservatoire prise en \napplication du code des procédures civiles d’exécution ;\n• toute reconnaissance par l’assureur du droit à garantie de l’assuré, ou \ntoute reconnaissance de dette de l’assuré envers l’assureur ; \n• tout recours à la médiation ou à la conciliation lorsque la partie est dans \nl’impossibilité d’agir par suite d’un empêchement résultant de la loi, de la \nconvention d’assistance ou de la Force Majeure. \n\n 65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance. \n\n Article 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée. \n\n Passé ce délai, il y a prescription : toute action dérivant \ndu contrat d’assurance est éteinte.\nLe délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie), \n\n Le délai de prescription est interrompu :\n-  soit par l’envoi d’une lettre recommandée avec accusé de réception adressée par MMA à votre dernier \ndomicile connu en ce qui concerne le paiement de la cotisation, ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire), \n\n ou par l’envoi d’un recommandé adressé \npar vos soins à MMA en ce qui concerne le règlement d’un sinistre,\n- soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure, \n\n Toutefois, ce délai ne court :  \n▪ en cas de réticence, omission, déclaration fausse ou inexacte sur le risque couru, que du jour où l’Assureur  en a eu \nconnaissance,  \n▪ en cas de Sinistre , que du jour où les intéressés en ont eu connaissance, s’ils prouvent qu’ils l’ont ignoré jusque -là. \n \nQuand l’action de l ’Assuré  contre l’Assureur  a pour cause le recours d’un Tiers , le délai de la prescript ion ne court que du jour \noù ce t iers a exercé une action en justice contre l’ Assuré  ou a été indemnisé par ce dernier.  \n \nConformément à l’article L.114 -2 du Code des assurances, la prescription est interrompue par une des causes ordinaires \nd’interruption de la prescription constituées par : toute demande en justice, même en référé, ou même portée devant une \njuridiction incompétente ; tout acte d’exécution forcée, ou toute mesure conservatoire prise en application du code des \nprocédures civil es d’exécution ;  toute reconnaissance par l ’Assureur  du droit à garantie de l’ Assuré , ou toute reconnaissance \nde dette de l’ Assuré  envers l’ Assureur . \n\n - soit par désignation de l’expert à la suite d’un sinistre,\n- soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, \n\n 65Causes ordinaires d'interruption de la prescription\nArticle 2240 du code civil\nLa reconnaissance par le débiteur du droit de celui contre lequel il prescrivait interrompt le délai de prescription.\nArticle 2241 du code civil\nLa demande en justice, même en référé, interrompt le délai de prescription ainsi que le délai de forclusion. Il en est de même lorsqu’elle est portée devant une juridiction incompétente ou lorsque l’acte de saisine de la juridiction est annulé par l’effet d’un vice de procédure.\nArticle 2242 du code civil\nL’interruption résultant de la demande en justice produit ses effets jusqu’à l’extinction de l’instance.\nArticle 2243 du code civil\nL’interruption est non avenue si le demandeur se désiste de sa demande ou laisse périmer l’instance, ou si sa demande est définitivement rejetée. \n\n - soit par une des causes ordinaires d’interruption de la prescription :\n* la reconnaissance par MMA de votre droit à bénéficier de la garantie contestée,\n* un acte d’exécution forcée (exemples : commandement de payer, saisie),\n*  une mesure conservatoire prise en application du code des procédures civiles d’exécution (exemple : \nsaisie conservatoire),\n*  l’exercice d’une action en justice y compris en référé, devant une juridiction incompétente ou en cas \nd’annulation de l’acte de saisine pour vice de procédure,\n*  l’interruption dure alors jusqu’au terme de cette procédure, sauf carence des parties pendant 2 ans, \ndésistement ou rejet définitif de la demande de celui qui agissait en justice. \n\n To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. (5.35) can be used in (5.31) to compute the attention under the LSH\nscheme described above. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT. \n\n First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers. \n\n To address this, and ensure that h(qi) =h(kj), [142] ﬁxes key\nkjso that kj=qj\n||qj||. To make the computation more eﬃcient, Reformer\ndoes two simple things\n1. Queries are sorted so those in the same hash bucket are adjacent.\nWithin the hash bucket, original sequence order is preserved\n2. Thesortedqueriesaregroupedintoblocksof mconsecutivequeries\nm=2L\nNumber of buckets(5.34)\nWithin each block, each position is allowed to attend to the others in\nthe block and to those in the preceding block. These two changes deﬁne\na new set of key positions that query ican attend to:\n˜Si={\nj:⌊si\nm⌋\n−1≤⌊sj\nm⌋\n≤⌊si\nm⌋}\n, (5.35)\nwheresiis the position in the sorted matrix that position iwas moved\nto. \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information stor -\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, access www.copyright.\ncom  or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA \n01923, 978-750-8400. \n\n Hewitt and Liang, in their work, devise a control task strategy as\na measure to address the probing confounder problem [110]. Probing\nconfounder problem can be deﬁned as—given the main neural architec-\nture that needs to be explained, the probing classiﬁer such as MLP or\nlogistic regression and the supervised auxiliary task, how do we assign\nthe credit for performance such as test accuracy to either of the three?\nThey devise control tasks that can be easily learned by a probing di-\nagnostic classiﬁer but not encoded in the representations (actual neural\nmodel or layers). Evaluating the performance (testing accuracy) diﬀer-\nence between the control tasks and auxiliary tasks with diﬀerent probing\nclassiﬁer choices as a selectivity measure can easily assign the credit of\nlearning to either the representation or the probing classiﬁer. The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”. \n\n \n La question est How is the prescription period interrupted by a conservatory measure or forced execution act?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:10:03.493211",
              "duration": 8.533724,
              "cost": 0.00248
            },
            "meta": {},
            "text": "The prescription period is interrupted by a conservatory measure or forced execution act in the following ways:\n\n1. **Recognition of Right to Guarantee**: If the insurer recognizes the insured's right to benefit from the contested guarantee, the prescription period is interrupted.\n\n2. **Forced Execution**: Any act of forced execution, such as a payment order or seizure, can interrupt the prescription period.\n\n3. **Conservatory Measure**: If a conservatory measure is taken in accordance with the civil enforcement procedures code, like a conservatory seizure, the prescription period is interrupted.\n\n4. **Legal Action**: Initiating legal action, including summary proceedings, even if brought before an incompetent court or if the filing is annulled due to a procedural defect, can interrupt the prescription period.\n\nIn all these cases, the interruption of the prescription period lasts until the conclusion of the legal proceedings, unless there is a lack of action by the parties for two years, withdrawal, or definitive rejection of the claim by the party taking legal action.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The prescription period is interrupted by a conservatory measure or forced execution act in the following ways:\n\n1. **Recognition of Right to Guarantee**: If the insurer recognizes the insured's right to benefit from the contested guarantee, the prescription period is interrupted.\n\n2. **Forced Execution**: Any act of forced execution, such as a payment order or seizure, can interrupt the prescription period.\n\n3. **Conservatory Measure**: If a conservatory measure is taken in accordance with the civil enforcement procedures code, like a conservatory seizure, the prescription period is interrupted.\n\n4. **Legal Action**: Initiating legal action, including summary proceedings, even if brought before an incompetent court or if the filing is annulled due to a procedural defect, can interrupt the prescription period.\n\nIn all these cases, the interruption of the prescription period lasts until the conclusion of the legal proceedings, unless there is a lack of action by the parties for two years, withdrawal, or definitive rejection of the claim by the party taking legal action.\n\n1. 1. The prescription period can be interrupted by a conservatory measure or forced execution act.\n2. 2. The interruption of the prescription period is in line with article L114-2 of the Insurance Code.\n3. 3. Any act of forced execution can cause an interruption in the prescription period.\n4. 4. A conservatory measure taken in accordance with the code of civil execution procedures can also interrupt the prescription period.\n5. 5. Legal action taken to enforce a judgement can interrupt the prescription period.\n6. 6. A precautionary measure taken to preserve the rights of a party can also cause an interruption.\n7. 7. The interruption effectively resets the prescription period.\n8. 8. The reset of the prescription period allows for additional time to pursue a claim or action.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:52.836913",
                "duration": 1.549679,
                "cost": 0.01473
              },
              "meta": {},
              "text": "Validated: ['1', '3', '4', '5'] - Not validated: ['2', '6', '7', '8']",
              "human": 1.0,
              "auto": 0.5
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does the insurer handle the analysis of a dispute and decision-making process?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The text lacks details on the insurer's dispute analysis procedure.\n2. There is no information on how an insurer makes decisions in the event of a dispute.\n3. The document does not explain the insurer's approach to dispute resolution.\n4. The decision-making process of an insurer in a dispute situation is not described in the text.\n5. The text does not shed light on the methods an insurer uses to analyze disputes.",
          "prompt": {
            "meta": {},
            "user": "The text does not provide information on how an insurer handles the analysis of a dispute and decision-making process.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:25:38.365617",
          "duration": 6.893665,
          "cost": 0.009240000000000002
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The text lacks details on the insurer's dispute analysis procedure."
          },
          {
            "meta": {},
            "text": "2. There is no information on how an insurer makes decisions in the event of a dispute."
          },
          {
            "meta": {},
            "text": "3. The document does not explain the insurer's approach to dispute resolution."
          },
          {
            "meta": {},
            "text": "4. The decision-making process of an insurer in a dispute situation is not described in the text."
          },
          {
            "meta": {},
            "text": "5. The text does not shed light on the methods an insurer uses to analyze disputes."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 15.367359982624713,
              "Node id": "84b06219-6eea-4794-a84e-6d28f47da95d"
            },
            "text": "188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe."
          },
          {
            "meta": {
              "score": 15.037760982702693,
              "Node id": "ab15a7f2-3d9a-4921-8b8e-d8358829c43a"
            },
            "text": "•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques."
          },
          {
            "meta": {
              "score": 14.420035178344099,
              "Node id": "a8b6c280-47a5-443d-9a02-26f3b97463dc"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 14.33191494230573,
              "Node id": "ef41ca8e-a2e2-48d9-9c7e-9987334cc6ee"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          },
          {
            "meta": {
              "score": 14.33191494230573,
              "Node id": "6d2ccc35-e866-4ba3-a97d-dfd40115d760"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]."
          },
          {
            "meta": {
              "score": 13.98965602110402,
              "Node id": "b12c2ec4-ffbe-4f28-9a2e-4b6dd7de7d82"
            },
            "text": "As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]."
          },
          {
            "meta": {
              "score": 13.3956114283429,
              "Node id": "016fb1ae-8d3e-4e17-8b97-3231ec3936e9"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 12.302426326318606,
              "Node id": "91281b84-105b-4d8f-8f6e-122eec7dabc1"
            },
            "text": "3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset."
          },
          {
            "meta": {
              "score": 12.069010799080912,
              "Node id": "422c6f71-020a-4287-968b-f7457fbe03c9"
            },
            "text": "5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3.1 Preprocessing speech data 180\n6.8.3.2 Evaluation 181"
          },
          {
            "meta": {
              "score": 11.981156963209393,
              "Node id": "d1815fbd-6df9-440c-8260-0031b19c3b22"
            },
            "text": "5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3."
          },
          {
            "meta": {
              "score": 0.7625198318884089,
              "Node id": "3063710c-86bf-4f4e-9fd2-715230580212"
            },
            "text": "The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]."
          },
          {
            "meta": {
              "score": 0.7584696574795624,
              "Node id": "dfd3e794-4e9b-4ccd-bf28-aa1792057a8a"
            },
            "text": "190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]."
          },
          {
            "meta": {
              "score": 0.7582919801820532,
              "Node id": "57665f2d-02ea-4e04-a3f4-a72e70e56957"
            },
            "text": "•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques."
          },
          {
            "meta": {
              "score": 0.7540736594936541,
              "Node id": "8d4a3bc8-2c14-4f07-8b8e-a630297aae89"
            },
            "text": "•Learning Mechanism : The ability to provide insights into the\nmodel training process helps to increase the conﬁdence and trust\nin the model. The process that leads to the learned state of the\nmodel can be further explored by: understanding the evolution of\ndiﬀerent layers from learning concepts or statistical patterns from\nthe perspective of ﬁlters, weights, and activation [301, 294]. Study\nof convergence of diﬀerent layers based on inputs and weights,\nand generalization properties of networks from memorization and\nstatistical mechanics viewpoint [206].\n•Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]."
          },
          {
            "meta": {
              "score": 0.7539398319242326,
              "Node id": "28bd6920-d38e-4e15-adb1-3b5a928f9d5d"
            },
            "text": "•Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems."
          },
          {
            "meta": {
              "score": 0.7522922490461865,
              "Node id": "ebc44546-5749-4af4-9b1c-03b987744c1e"
            },
            "text": "Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]."
          },
          {
            "meta": {
              "score": 0.7522232640431127,
              "Node id": "b4ee9144-aff2-4e44-886e-9ef157c2bb85"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]."
          },
          {
            "meta": {
              "score": 0.7519133815513829,
              "Node id": "0304f665-bf26-4987-9135-8d8937be64d2"
            },
            "text": "3–\n19.\n[110]J.HewittandP.Liang ,Designing and interpreting probes with\ncontrol tasks , arXiv preprint arXiv:1909.03368, (2019).\n[111]M. Hind, D. Wei, M. Campbell, N. C. Codella, A. Dhu-\nrandhar, A. Mojsilović, K. Natesan Ramamurthy, and\nK. R. Varshney ,Ted: Teaching ai to explain its decisions , in\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society, 2019, pp. 123–129."
          },
          {
            "meta": {
              "score": 0.751896824606102,
              "Node id": "9defdfad-b612-4bba-9955-dafab1a9ca1e"
            },
            "text": "19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015)."
          },
          {
            "meta": {
              "score": 0.7516940064337366,
              "Node id": "a66f4e00-c142-4205-9cb7-8e8888e87ce8"
            },
            "text": "Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The text does not provide information on how an insurer handles the analysis of a dispute and decision-making process.",
              "prompt": {
                "meta": {},
                "user": "How does the insurer handle the analysis of a dispute and decision-making process?",
                "system": "Contexte :  188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. \n\n •Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset. \n\n 5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3.1 Preprocessing speech data 180\n6.8.3.2 Evaluation 181 \n\n 5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n •Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. \n\n •Learning Mechanism : The ability to provide insights into the\nmodel training process helps to increase the conﬁdence and trust\nin the model. The process that leads to the learned state of the\nmodel can be further explored by: understanding the evolution of\ndiﬀerent layers from learning concepts or statistical patterns from\nthe perspective of ﬁlters, weights, and activation [301, 294]. Study\nof convergence of diﬀerent layers based on inputs and weights,\nand generalization properties of networks from memorization and\nstatistical mechanics viewpoint [206].\n•Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. \n\n •Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. \n\n Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3–\n19.\n[110]J.HewittandP.Liang ,Designing and interpreting probes with\ncontrol tasks , arXiv preprint arXiv:1909.03368, (2019).\n[111]M. Hind, D. Wei, M. Campbell, N. C. Codella, A. Dhu-\nrandhar, A. Mojsilović, K. Natesan Ramamurthy, and\nK. R. Varshney ,Ted: Teaching ai to explain its decisions , in\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society, 2019, pp. 123–129. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n \n La question est How does the insurer handle the analysis of a dispute and decision-making process?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:10:09.225249",
              "duration": 5.729726,
              "cost": 0.13491
            },
            "meta": {},
            "text": "The text does not provide information on how an insurer handles the analysis of a dispute and decision-making process.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The text does not provide information on how an insurer handles the analysis of a dispute and decision-making process.\n\n1. 1. The text lacks details on the insurer's dispute analysis procedure.\n2. 2. There is no information on how an insurer makes decisions in the event of a dispute.\n3. 3. The document does not explain the insurer's approach to dispute resolution.\n4. 4. The decision-making process of an insurer in a dispute situation is not described in the text.\n5. 5. The text does not shed light on the methods an insurer uses to analyze disputes.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:54.545428",
                "duration": 1.704611,
                "cost": 0.00732
              },
              "meta": {},
              "text": "Validated: ['1', '2', '4', '5'] - Not validated: ['3']",
              "human": 1.0,
              "auto": 0.8
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The insurer typically follows a structured process when analyzing a dispute and making decisions. Here is an overview of how insurers handle this:\n\n1. **Receipt of Dispute**: The insurer first receives the dispute or claim from the policyholder or a third party.\n\n2. **Documentation Review**: The insurer reviews all relevant documentation related to the dispute, including the insurance policy, claim details, evidence, and any communication between the parties.\n\n3. **Investigation**: An investigation is conducted to gather additional information, which may involve interviewing involved parties, witnesses, or experts, and conducting site visits if necessary.\n\n4. **Assessment of Liability**: Based on the information gathered, the insurer assesses the liability in the dispute. They determine if the claim is covered under the policy and if the policyholder is at fault.\n\n5. **Evaluation of Damages**: The insurer evaluates the extent of damages or losses claimed by the policyholder and determines the amount that may be payable under the policy.\n\n6. **Negotiation**: In some cases, the insurer may enter into negotiations with the policyholder or third parties to reach a settlement agreement.\n\n7. **Decision-Making**: Based on the investigation, assessment of liability, evaluation of damages, and any negotiations, the insurer makes a decision on how to resolve the dispute. This decision can include accepting the claim, offering a settlement, or denying the claim.\n\n8. **Communication**: The insurer communicates the decision to the policyholder or third party, providing reasons for the decision and any next steps.\n\n9. **Appeals Process**: If the policyholder disagrees with the decision, there may be an appeals process within the insurer's organization or through external dispute resolution mechanisms.\n\n10. **Resolution**: The insurer works towards resolving the dispute in a fair and timely manner, ensuring compliance with legal and regulatory requirements.\n\nOverall, the insurer aims to handle disputes efficiently, fairly, and in accordance with the terms of the insurance policy and relevant laws and regulations.",
              "prompt": {
                "meta": {},
                "user": "How does the insurer handle the analysis of a dispute and decision-making process?",
                "system": "Contexte :  188■Transformers for Machine Learning: A Deep Dive\nthat a user can evaluate and measure. As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. \n\n •Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. \n\n As proposed by Xie et al., the\nfour necessary traits are:\n1.Conﬁdence. When the end-user (decision-maker) can align the\ntransformer-based model’s processing with their thought process\nbased on the input and the output, the conﬁdence in the system\nincreases. Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3.6.2 Data, Tools, and Libraries\nWe chose the Google Play application review dataset. The dataset in-\ncludes 15,746 samples in three categories; namely negative, neutral, and\npositive.WeusetheHuggingfacetransformerlibrarytoperformtheﬁne-\ntuning task as well as standard Python data science stack for all other\ndata handling and visualization [249] (Figs. 3.9–3.11).\nWe use BERT BASEas the starting pre-trained BERT mode. Listing\n3.6.3 shows how to load the pre-trained BERT model from the Trans-\n(a)\n (b)\nFigure 3.9 Exploratory data analysis of the sentiment classiﬁcation\ndataset. \n\n 5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3.1 Preprocessing speech data 180\n6.8.3.2 Evaluation 181 \n\n 5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6.7 REINFORCEMENT LEARNING 177\n6.7.1 Decision Transformer 178\n6.8 CASE STUDY: AUTOMATIC SPEECH RECOGNITION 180\n6.8.1 Goal 180\n6.8.2 Data, Tools, and Libraries 180\n6.8.3 Experiments, Results, and Analysis 180\n6.8.3. \n\n The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. \n\n 190■Transformers for Machine Learning: A Deep Dive\nagainst adversarial examples. Two common methods of adversarial\ndefense are (i) adversarial training, in which the training dataset is\naugmented with adversarial examples to introduce robustness, and\n(ii) perturbation removal, in which the model identiﬁes adversarial\nexamples and rejects them [95, 218].\n•Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30]. \n\n •Fairness and Bias: One of the key goals for many models de-\nployed in critical areas impacting humans is to be “fair” and un-\nbiased in its decision-making process. Though an evolving ﬁeld,\nfairness implies (i) group fairness, also called demographic parity\nor statistical parity, focusing on race, gender, sexual orientation,\netc. [34], and (ii) individual fairness, focusing on individuals with\nsimilar features and characteristics generating similar model out-\nput [82]. Diﬀerent techniques to address fairness can be classiﬁed\nas (i) pre-processing methods, removing sensitive features from the\ndata, (ii) in-process methods, where fairness constraints are added,\nand (iii) post-processing methods to adjust the model predictions\nafter training [36, 139,96,30].\n7.3 EXPLAINABLE METHODS TAXONOMY\nThere are many surveys on explainable AI where diﬀerent strategies\nhave generated many taxonomies for categorizing explainability tech-\nniques. \n\n •Learning Mechanism : The ability to provide insights into the\nmodel training process helps to increase the conﬁdence and trust\nin the model. The process that leads to the learned state of the\nmodel can be further explored by: understanding the evolution of\ndiﬀerent layers from learning concepts or statistical patterns from\nthe perspective of ﬁlters, weights, and activation [301, 294]. Study\nof convergence of diﬀerent layers based on inputs and weights,\nand generalization properties of networks from memorization and\nstatistical mechanics viewpoint [206].\n•Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. \n\n •Model Debugging : Akin to software debugging, model debug-\nging corresponds to the inspection of the model architecture, data\nprocessing through the networks, and the errors introduced dur-\ning the training and runtime process [140]. A common research\napproach is to build auxiliary models that act as a diagnostic and\ninspection probe [2]. The proposal of ModelTracker by Amershi et\nal.allowsvisualinteractionsfrommislabeleddata,missingfeatures\nidentiﬁcation, insights into insuﬃciency of training data for label\nlearning, the impact of outliers, feature space visualization, model\nsummary from performance, etc., a model-agnostic approach in\ndebugging [4]. Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. \n\n Neural stethoscopes by Fuchs et al. are another\ngeneral-purpose framework that analyzes the learning process by\nquantifying the importance of inﬂuential factors by promoting and\nsuppressing information [90].\n•AdversarialAttacksandDefense :Adversarialexamplesarear-\ntiﬁcially engineered inputs to feed into the models and judge their\nabilitytodiscriminate.Constructingadversarialexamplesneedsan\nunderstanding of input spaces and the boundaries between classes\nin classiﬁcation problems. Adversarial attacks and defense are two\ndiﬀerent ways to explore the models from an explainability and di-\nagnostic perspective [291, 295]. Black-box and white-box attacks\naretwomaintypesofadversarialattacksthatgenerateexamplesto\ndeceive the model [233, 86, 76]. Recent research shows that adding\nimperceptible noise by perturbing inputs reveals model vulnerabil-\nities [178]. \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. Developing frameworks and guidelines for ethics-based AI\nis growing as a ﬁeld in and of itself [268, 59,26,20]. \n\n 3–\n19.\n[110]J.HewittandP.Liang ,Designing and interpreting probes with\ncontrol tasks , arXiv preprint arXiv:1909.03368, (2019).\n[111]M. Hind, D. Wei, M. Campbell, N. C. Codella, A. Dhu-\nrandhar, A. Mojsilović, K. Natesan Ramamurthy, and\nK. R. Varshney ,Ted: Teaching ai to explain its decisions , in\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society, 2019, pp. 123–129. \n\n 19–36.\n[208]M. T. Ribeiro, S. Singh, and C. Guestrin ,\" why should\ni trust you?\" explaining the predictions of any classiﬁer , in Pro-\nceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 2016, pp. 1135–1144.\n[209] ,Anchors: High-precision model-agnostic explanations , in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 32, 2018.\n[210]M. Robnik-Šikonja and I. Kononenko ,Explaining classiﬁ-\ncations for individual instances , IEEE Transactions on Knowledge\nand Data Engineering, 20 (2008), pp. 589–600.\n[211]T. Rockt äschel, E. Grefenstette, K. M. Hermann,\nT. Kočisk `y, and P. Blunsom ,Reasoning about entailment with\nneural attention , arXiv preprint arXiv:1509.06664, (2015). \n\n Xie\net al. consider two signiﬁcant trustworthiness criteria: (i) satisfac-\ntory testing and (ii) experience. Satisfactory testing is the ability\nof the model to approximate performance similar to its training.\nQuantitatively, when model performance metrics such as accuracy\nor precision are similar on training and unseen test data, the trust\nincreases. Experience is a more qualitative assessment where one\njudges the system to be suﬃcient to perform the task without in-\nspection. Data and concept drift in real-world deployments can\nfurther decrease the trust in the system. There have been many\nstudies in recent times that explore safety from a model prediction\nviewpoint [134, 22,252,5].\n4.Ethics. If the model does not violate any ethical principles set by\nthe end-user, it can be considered ethical in its decision-making\nprocess. \n\n \n La question est How does the insurer handle the analysis of a dispute and decision-making process?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:10:19.703889",
              "duration": 10.476726,
              "cost": 0.00282
            },
            "meta": {},
            "text": "The insurer typically follows a structured process when analyzing a dispute and making decisions. Here is an overview of how insurers handle this:\n\n1. **Receipt of Dispute**: The insurer first receives the dispute or claim from the policyholder or a third party.\n\n2. **Documentation Review**: The insurer reviews all relevant documentation related to the dispute, including the insurance policy, claim details, evidence, and any communication between the parties.\n\n3. **Investigation**: An investigation is conducted to gather additional information, which may involve interviewing involved parties, witnesses, or experts, and conducting site visits if necessary.\n\n4. **Assessment of Liability**: Based on the information gathered, the insurer assesses the liability in the dispute. They determine if the claim is covered under the policy and if the policyholder is at fault.\n\n5. **Evaluation of Damages**: The insurer evaluates the extent of damages or losses claimed by the policyholder and determines the amount that may be payable under the policy.\n\n6. **Negotiation**: In some cases, the insurer may enter into negotiations with the policyholder or third parties to reach a settlement agreement.\n\n7. **Decision-Making**: Based on the investigation, assessment of liability, evaluation of damages, and any negotiations, the insurer makes a decision on how to resolve the dispute. This decision can include accepting the claim, offering a settlement, or denying the claim.\n\n8. **Communication**: The insurer communicates the decision to the policyholder or third party, providing reasons for the decision and any next steps.\n\n9. **Appeals Process**: If the policyholder disagrees with the decision, there may be an appeals process within the insurer's organization or through external dispute resolution mechanisms.\n\n10. **Resolution**: The insurer works towards resolving the dispute in a fair and timely manner, ensuring compliance with legal and regulatory requirements.\n\nOverall, the insurer aims to handle disputes efficiently, fairly, and in accordance with the terms of the insurance policy and relevant laws and regulations.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "2, 3, 4, 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The insurer typically follows a structured process when analyzing a dispute and making decisions. Here is an overview of how insurers handle this:\n\n1. **Receipt of Dispute**: The insurer first receives the dispute or claim from the policyholder or a third party.\n\n2. **Documentation Review**: The insurer reviews all relevant documentation related to the dispute, including the insurance policy, claim details, evidence, and any communication between the parties.\n\n3. **Investigation**: An investigation is conducted to gather additional information, which may involve interviewing involved parties, witnesses, or experts, and conducting site visits if necessary.\n\n4. **Assessment of Liability**: Based on the information gathered, the insurer assesses the liability in the dispute. They determine if the claim is covered under the policy and if the policyholder is at fault.\n\n5. **Evaluation of Damages**: The insurer evaluates the extent of damages or losses claimed by the policyholder and determines the amount that may be payable under the policy.\n\n6. **Negotiation**: In some cases, the insurer may enter into negotiations with the policyholder or third parties to reach a settlement agreement.\n\n7. **Decision-Making**: Based on the investigation, assessment of liability, evaluation of damages, and any negotiations, the insurer makes a decision on how to resolve the dispute. This decision can include accepting the claim, offering a settlement, or denying the claim.\n\n8. **Communication**: The insurer communicates the decision to the policyholder or third party, providing reasons for the decision and any next steps.\n\n9. **Appeals Process**: If the policyholder disagrees with the decision, there may be an appeals process within the insurer's organization or through external dispute resolution mechanisms.\n\n10. **Resolution**: The insurer works towards resolving the dispute in a fair and timely manner, ensuring compliance with legal and regulatory requirements.\n\nOverall, the insurer aims to handle disputes efficiently, fairly, and in accordance with the terms of the insurance policy and relevant laws and regulations.\n\n1. 1. The text lacks details on the insurer's dispute analysis procedure.\n2. 2. There is no information on how an insurer makes decisions in the event of a dispute.\n3. 3. The document does not explain the insurer's approach to dispute resolution.\n4. 4. The decision-making process of an insurer in a dispute situation is not described in the text.\n5. 5. The text does not shed light on the methods an insurer uses to analyze disputes.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:56.341664",
                "duration": 1.793123,
                "cost": 0.01854
              },
              "meta": {},
              "text": "Validated: ['2', '3', '4', '5'] - Not validated: ['1']",
              "human": 1.0,
              "auto": 0.8
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the file size of the document?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. The AI model lacks the capability to access file information.\n2. It is not possible for the AI model to retrieve properties like file size.\n3. The AI model's abilities do not include accessing file details.\n4. File size is among the properties that the AI model cannot access.\n5. The AI model is not equipped to handle tasks like accessing file properties.",
          "prompt": {
            "meta": {},
            "user": "As an AI model, I don't have the ability to access file information or properties such as file size.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-4",
          "full_name": "gpt-4-0613",
          "timestamp": "2024-04-24T14:25:44.606291",
          "duration": 6.2371,
          "cost": 0.00861
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. The AI model lacks the capability to access file information."
          },
          {
            "meta": {},
            "text": "2. It is not possible for the AI model to retrieve properties like file size."
          },
          {
            "meta": {},
            "text": "3. The AI model's abilities do not include accessing file details."
          },
          {
            "meta": {},
            "text": "4. File size is among the properties that the AI model cannot access."
          },
          {
            "meta": {},
            "text": "5. The AI model is not equipped to handle tasks like accessing file properties."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.066275624979907,
              "Node id": "2ff697ec-75fe-42bf-a934-1dfe2e0fd871"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit."
          },
          {
            "meta": {
              "score": 9.778456141434546,
              "Node id": "d2236c55-6245-4614-8074-5c95bdd0e238"
            },
            "text": "Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]."
          },
          {
            "meta": {
              "score": 9.059644534577652,
              "Node id": "a869bb08-6da6-48f8-930e-b5d32f5274f2"
            },
            "text": "As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token."
          },
          {
            "meta": {
              "score": 8.958607661150419,
              "Node id": "bdd1c133-cd6f-4dcf-b368-ac150c1733a1"
            },
            "text": "Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]. Class-Based Term Frequency-Inverse Document Frequency (c-TF-\nIDF) is modiﬁcation of the classic Term Frequency-Inverse Document\nFrequency (TF-IDF) in which all documents in a certain category is\nconsidered as a single document then compute TF-IDF."
          },
          {
            "meta": {
              "score": 8.585274446498598,
              "Node id": "2cc68f28-135b-49b3-96b7-322bdeb35682"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit. map(process_data,\nremove_columns=[ ’file ’,’audio ’,\n’phonetic_detail ’,’word_detail ’,\n’dialect_region ’,’sentence_type ’,\n’speaker_id ’,’id’])\nListing 6.3 Preprocess speech\n6.8.3.2 Evaluation\nThere are three types of errors we can make during transcription:\n•substitution: swapping a correct word or character for an incorrect\none\n•deletion: removing a correct word or character\n•insertion: inserting a word or character\nWe can compute the word error rate and character error rate metrics\nby summing the three kinds of errors and dividing by the number of\nwords (or characters):\nWER =S+D+I\nN(6.39)\nCER =S+D+I\nN(6.40)\nwhereSis the number of substitutions, Dis the number of deletions, I\nis the number of insertions, and Nis the number of words (or characters,\nfor CER) in the transcript."
          },
          {
            "meta": {
              "score": 8.354414305905955,
              "Node id": "fffbbeb1-939b-4f77-bf74-795a20e7ff83"
            },
            "text": "As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance."
          },
          {
            "meta": {
              "score": 7.982822613094605,
              "Node id": "24a2f13e-c0be-4a6e-bef7-ede70228888d"
            },
            "text": "For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance. RoBERTa is also pre-trained on an order of magnitude\nmore data than BERT, the pre-training data consists of 160 GB includ-\ning Books Corpus, English Wikipedia, and CommonCrawl News dataset"
          },
          {
            "meta": {
              "score": 7.782085851961911,
              "Node id": "8667df3b-5fde-4d00-9341-63a4e70d8c10"
            },
            "text": "3.2 BERT VARIANTS\n3.2.1 Robustly Optimized BERT Pre-training Approach (RoBERTa)\nRoBERTa [170] is a Robustly Optimized BERT Pre-training Approach\nthatimprovesontheoriginalBERTmodelintermsofpre-trainingtasks,\ntraining hyperparameters, and pre-training data. As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents."
          },
          {
            "meta": {
              "score": 7.614719386902859,
              "Node id": "3b7d1ee5-3b80-4291-a5e6-f0a03f02ccb0"
            },
            "text": "3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6."
          },
          {
            "meta": {
              "score": 6.800110901952832,
              "Node id": "57039ec7-5c1a-4756-b25f-d7cb5d115eca"
            },
            "text": "3.5.3 Experiments, Results, and Analysis\n3.5.3.1 Building topics\nfrom bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 3))\ntopics, probs = topic_model.fit_transform(\ndataset[\"text\"],\nnp.array(dataset[\"embedding\"]))\nprint(f\"Number of topics: {len(topic_model.get_topics())}\")\nListing 3.4 Compute topics\nNow that we’ve computed the embeddings for the speciﬁed subset of\nYelp reviews, we can proceed with the topic modeling in Listing 3.4.\n3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics."
          },
          {
            "meta": {
              "score": 0.7784009369976886,
              "Node id": "e73df662-72ec-408e-b5d5-e2ff944c943d"
            },
            "text": "3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6."
          },
          {
            "meta": {
              "score": 0.7778818786660211,
              "Node id": "76ec8168-91fa-4222-9003-edd28b39d890"
            },
            "text": "230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437."
          },
          {
            "meta": {
              "score": 0.7716884821509,
              "Node id": "56a0beb5-78d0-4259-9ff6-501fa5d8c3cc"
            },
            "text": "40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here?"
          },
          {
            "meta": {
              "score": 0.7710357104778822,
              "Node id": "249e1c98-fab5-40da-9cf5-b147791e35dd"
            },
            "text": "230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437.\n[80]V. P. Dwivedi and X. Bresson ,A generalization of trans-\nformer networks to graphs , ArXiv, abs/2012.09699 (2020)."
          },
          {
            "meta": {
              "score": 0.7684534627734402,
              "Node id": "fbdb3ee2-fe4d-4153-8c95-a41680c6d57a"
            },
            "text": "Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences."
          },
          {
            "meta": {
              "score": 0.7675477830290709,
              "Node id": "468ca5fd-f5ea-473e-a9d6-09db9a9951e3"
            },
            "text": "As we can see, the content of the unassigned cluster contains words\nthat do not strongly belong to any topic. The largest, smallest, and\nmedian topics are shown in Listings 3.10, 3.11, and 3.12, respectively.\n>>> dump_topic_and_docs(\"Largest topic\", 0)\nLargest topic size: 349\n**** Representative reviews ****\nThis place makes me cringe! I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese."
          },
          {
            "meta": {
              "score": 0.7674137353379638,
              "Node id": "671f1195-9b53-4ee1-9cd3-9435ea9ab158"
            },
            "text": "212■Transformers for Machine Learning: A Deep Dive\nFigure 7.7 Word cloud.\nFigure 7.8 Document length distri-\nbution.\nFigure 7.9 Top words.\n Figure 7.10 Class distribution.\nTables 7.1 and 7.2 show that the BIO_ClinicalBERT with its pre-\ntraining on a large corpus of medical records has a slight advantage over\nthe base BERT for classiﬁcation.\n7.6.3.3 Error analysis and explainability\nNext, we will try to perform error analysis using explainable techniques\nto get more insights. The error analysis on the 500sized test data shows\n1. The BIO_ClinincalBERT performs better than the base model on\n51instances."
          },
          {
            "meta": {
              "score": 0.7670552652812896,
              "Node id": "16644bb4-e748-4267-8f70-9c9c34747642"
            },
            "text": ""
          },
          {
            "meta": {
              "score": 0.7666488957358997,
              "Node id": "9fdf668b-4f4d-454b-b019-1af23b5ff217"
            },
            "text": "242■Bibliography\nof the North American Chapter of the Association for Computa-\ntionalLinguistics:HumanLanguageTechnologies,Volume1(Long\nPapers), New Orleans, Louisiana, June 2018, Association for Com-\nputational Linguistics, pp. 2227–2237."
          },
          {
            "meta": {
              "score": 0.7664275901462452,
              "Node id": "bb101e07-64c5-4c01-b775-ddd0a0b8e2b0"
            },
            "text": "The massive decrease in\nvocabulary size that comes with using bytes means more parameters can\nbe used elsewhere in the model.\nSince byte sequences are longer than token sequences and trans-\nformer computational complexity is quadratic in sequence length (ex-\ncept for linearized attention), earlier work has tried to mitigate the in-\ncreased complexity that comes with using byte-level and character-level\nsequences. ByT5 starts with the mT5 architecture, but diﬀers from mT5\nin a few ways. Firstly, there is no SentencePiece tokenization. Raw UTF-\n8 bytes are used as input and embeddings are learned for each of the\n256 bytes. In addition, there are three special tokens added to the vo-\ncabulary: a padding token, an EOS token, and an unknown token. The\npre-training task is changed so that the length of the masked portion of\ninput spans is longer than in is in T5. in T5, the encoder and decoder\nmodules have the same number of layers."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "As an AI model, I don't have the ability to access file information or properties such as file size.",
              "prompt": {
                "meta": {},
                "user": "What is the file size of the document?",
                "system": "Contexte :  Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]. \n\n As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]. Class-Based Term Frequency-Inverse Document Frequency (c-TF-\nIDF) is modiﬁcation of the classic Term Frequency-Inverse Document\nFrequency (TF-IDF) in which all documents in a certain category is\nconsidered as a single document then compute TF-IDF. \n\n Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit. map(process_data,\nremove_columns=[ ’file ’,’audio ’,\n’phonetic_detail ’,’word_detail ’,\n’dialect_region ’,’sentence_type ’,\n’speaker_id ’,’id’])\nListing 6.3 Preprocess speech\n6.8.3.2 Evaluation\nThere are three types of errors we can make during transcription:\n•substitution: swapping a correct word or character for an incorrect\none\n•deletion: removing a correct word or character\n•insertion: inserting a word or character\nWe can compute the word error rate and character error rate metrics\nby summing the three kinds of errors and dividing by the number of\nwords (or characters):\nWER =S+D+I\nN(6.39)\nCER =S+D+I\nN(6.40)\nwhereSis the number of substitutions, Dis the number of deletions, I\nis the number of insertions, and Nis the number of words (or characters,\nfor CER) in the transcript. \n\n As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance. \n\n For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance. RoBERTa is also pre-trained on an order of magnitude\nmore data than BERT, the pre-training data consists of 160 GB includ-\ning Books Corpus, English Wikipedia, and CommonCrawl News dataset \n\n 3.2 BERT VARIANTS\n3.2.1 Robustly Optimized BERT Pre-training Approach (RoBERTa)\nRoBERTa [170] is a Robustly Optimized BERT Pre-training Approach\nthatimprovesontheoriginalBERTmodelintermsofpre-trainingtasks,\ntraining hyperparameters, and pre-training data. As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. \n\n 3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6. \n\n 3.5.3 Experiments, Results, and Analysis\n3.5.3.1 Building topics\nfrom bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 3))\ntopics, probs = topic_model.fit_transform(\ndataset[\"text\"],\nnp.array(dataset[\"embedding\"]))\nprint(f\"Number of topics: {len(topic_model.get_topics())}\")\nListing 3.4 Compute topics\nNow that we’ve computed the embeddings for the speciﬁed subset of\nYelp reviews, we can proceed with the topic modeling in Listing 3.4.\n3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. \n\n 3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6. \n\n 230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n 230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437.\n[80]V. P. Dwivedi and X. Bresson ,A generalization of trans-\nformer networks to graphs , ArXiv, abs/2012.09699 (2020). \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n As we can see, the content of the unassigned cluster contains words\nthat do not strongly belong to any topic. The largest, smallest, and\nmedian topics are shown in Listings 3.10, 3.11, and 3.12, respectively.\n>>> dump_topic_and_docs(\"Largest topic\", 0)\nLargest topic size: 349\n**** Representative reviews ****\nThis place makes me cringe! I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \n\n 212■Transformers for Machine Learning: A Deep Dive\nFigure 7.7 Word cloud.\nFigure 7.8 Document length distri-\nbution.\nFigure 7.9 Top words.\n Figure 7.10 Class distribution.\nTables 7.1 and 7.2 show that the BIO_ClinicalBERT with its pre-\ntraining on a large corpus of medical records has a slight advantage over\nthe base BERT for classiﬁcation.\n7.6.3.3 Error analysis and explainability\nNext, we will try to perform error analysis using explainable techniques\nto get more insights. The error analysis on the 500sized test data shows\n1. The BIO_ClinincalBERT performs better than the base model on\n51instances. \n\n  \n\n 242■Bibliography\nof the North American Chapter of the Association for Computa-\ntionalLinguistics:HumanLanguageTechnologies,Volume1(Long\nPapers), New Orleans, Louisiana, June 2018, Association for Com-\nputational Linguistics, pp. 2227–2237. \n\n The massive decrease in\nvocabulary size that comes with using bytes means more parameters can\nbe used elsewhere in the model.\nSince byte sequences are longer than token sequences and trans-\nformer computational complexity is quadratic in sequence length (ex-\ncept for linearized attention), earlier work has tried to mitigate the in-\ncreased complexity that comes with using byte-level and character-level\nsequences. ByT5 starts with the mT5 architecture, but diﬀers from mT5\nin a few ways. Firstly, there is no SentencePiece tokenization. Raw UTF-\n8 bytes are used as input and embeddings are learned for each of the\n256 bytes. In addition, there are three special tokens added to the vo-\ncabulary: a padding token, an EOS token, and an unknown token. The\npre-training task is changed so that the length of the masked portion of\ninput spans is longer than in is in T5. in T5, the encoder and decoder\nmodules have the same number of layers. \n\n \n La question est What is the file size of the document?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-24T14:10:22.929541",
              "duration": 3.222031,
              "cost": 0.10398
            },
            "meta": {},
            "text": "As an AI model, I don't have the ability to access file information or properties such as file size.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1, 2, 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: As an AI model, I don't have the ability to access file information or properties such as file size.\n\n1. 1. The AI model lacks the capability to access file information.\n2. 2. It is not possible for the AI model to retrieve properties like file size.\n3. 3. The AI model's abilities do not include accessing file details.\n4. 4. File size is among the properties that the AI model cannot access.\n5. 5. The AI model is not equipped to handle tasks like accessing file properties.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:57.880734",
                "duration": 1.535914,
                "cost": 0.006840000000000001
              },
              "meta": {},
              "text": "Validated: ['1', '2', '4'] - Not validated: ['3', '5']",
              "human": 1.0,
              "auto": 0.6
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The file size of the document is not explicitly mentioned in the provided text excerpts. The content mainly discusses various topics related to transformers, pre-training tasks, data preprocessing, evaluation metrics, and experiments. If you need information on a specific section or topic within the document, please let me know so I can provide more details.",
              "prompt": {
                "meta": {},
                "user": "What is the file size of the document?",
                "system": "Contexte :  Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]. \n\n As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token. \n\n Bidirectional Encoder Representations from Transformers (BERT) ■51\nthe token size, the documents are preprocessed before extracting the\nembeddings. Preprocessing divides the document to smaller paragraphs\nor sentences that are smaller than the token size for the transformer\nmodel. Then, clustering is performed on the document embeddings to\ncluster all the documents with similar topics together. It is worth not-\ning that dimensionality reduction for the embeddings is recommended\nbefore performing the clustering. Dimensionality reduction algorithms\nsuch as t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uni-\nform Manifold Approximation and Projection (UMAP) can be used to\nlowerthedimensions.Thetopicsarethenderivedfromtheclustereddoc-\numentsusingthenovelclass-basedmetricintroducedbyGrootendorstin\n[101]. Class-Based Term Frequency-Inverse Document Frequency (c-TF-\nIDF) is modiﬁcation of the classic Term Frequency-Inverse Document\nFrequency (TF-IDF) in which all documents in a certain category is\nconsidered as a single document then compute TF-IDF. \n\n Pre-trained and Application-Speciﬁc Transformers ■181\ndefprocess_data(batch):\nbatch[ \"speech \"], batch[ \"sampling_rate \"] =\nlibrosa.load(batch[ \"file \"], sr=16000)\nreturn batch\ntimit = timit. map(process_data,\nremove_columns=[ ’file ’,’audio ’,\n’phonetic_detail ’,’word_detail ’,\n’dialect_region ’,’sentence_type ’,\n’speaker_id ’,’id’])\nListing 6.3 Preprocess speech\n6.8.3.2 Evaluation\nThere are three types of errors we can make during transcription:\n•substitution: swapping a correct word or character for an incorrect\none\n•deletion: removing a correct word or character\n•insertion: inserting a word or character\nWe can compute the word error rate and character error rate metrics\nby summing the three kinds of errors and dividing by the number of\nwords (or characters):\nWER =S+D+I\nN(6.39)\nCER =S+D+I\nN(6.40)\nwhereSis the number of substitutions, Dis the number of deletions, I\nis the number of insertions, and Nis the number of words (or characters,\nfor CER) in the transcript. \n\n As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance. \n\n For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. If the end of a document is reached before getting\nthe required input sequence size, sentence sampling continues with the\nnext document after adding an extra separator token.\nOn the hyperparameters front, RoBERTa showed that using large\nmini-batches with increased learning rates during pre-training improves\nthe perplexity of the dynamic MLM task as well as the downstream task\nperformance. RoBERTa is also pre-trained on an order of magnitude\nmore data than BERT, the pre-training data consists of 160 GB includ-\ning Books Corpus, English Wikipedia, and CommonCrawl News dataset \n\n 3.2 BERT VARIANTS\n3.2.1 Robustly Optimized BERT Pre-training Approach (RoBERTa)\nRoBERTa [170] is a Robustly Optimized BERT Pre-training Approach\nthatimprovesontheoriginalBERTmodelintermsofpre-trainingtasks,\ntraining hyperparameters, and pre-training data. As discussed earlier,\nBERT uses two pre-training tasks, Masked Language Model (MLM) and\nNext Sentence Prediction (NSP). For the MLM task, BERT randomly\nmasks token during the data pre-processing stage. Therefore, the masks\nstay static throughout the entire model training process. RoBERTa on\nthe other hand follows a dynamic masking strategy where masked to-\nkens are randomly chosen for each training epoch. RoBERTa also drops\nthe NSP pre-training task and only uses the dynamic MLM task. An-\nother contribution for RoBERTa is the size of input sequences, where\nfull sentences of at most 512 tokens, are sampled contiguously from one\nor more documents. \n\n 3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6. \n\n 3.5.3 Experiments, Results, and Analysis\n3.5.3.1 Building topics\nfrom bertopic import BERTopic\ntopic_model = BERTopic(n_gram_range=(1, 3))\ntopics, probs = topic_model.fit_transform(\ndataset[\"text\"],\nnp.array(dataset[\"embedding\"]))\nprint(f\"Number of topics: {len(topic_model.get_topics())}\")\nListing 3.4 Compute topics\nNow that we’ve computed the embeddings for the speciﬁed subset of\nYelp reviews, we can proceed with the topic modeling in Listing 3.4.\n3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. \n\n 3.5.3.2 Topic size distribution\ntopic_sizes = topic_model.get_topic_freq()\ntopic_sizes\nListing 3.5 Sampling the topics\nNow that we have computed a topic distribution, we need to see\nwhat kind of reviews are in each topic. Let’s start by looking at the\ndistribution of topic sizes.\nThe commands in Listing 3.5 will output a summary table of the\nsizes of the topics. Note that the size of a topic is the number of reviews\nthat contain that topic. The table is shown in Fig. 3.6. \n\n 230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437. \n\n 40■Transformers for Machine Learning: A Deep Dive\nFigure 2.20 Explaining translations for—How long you will be here? \n\n 230■Bibliography\n[78]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby ,An image is worth 16x16 words: Transformers for\nimage recognition at scale , ArXiv, abs/2010.11929 (2021).\n[79]P. Dufter and H. Schütze ,Identifying elements essential for\nBERT’s multilinguality , in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\nOnline, November 2020, Association for Computational Linguis-\ntics, pp. 4423–4437.\n[80]V. P. Dwivedi and X. Bresson ,A generalization of trans-\nformer networks to graphs , ArXiv, abs/2012.09699 (2020). \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n As we can see, the content of the unassigned cluster contains words\nthat do not strongly belong to any topic. The largest, smallest, and\nmedian topics are shown in Listings 3.10, 3.11, and 3.12, respectively.\n>>> dump_topic_and_docs(\"Largest topic\", 0)\nLargest topic size: 349\n**** Representative reviews ****\nThis place makes me cringe! I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \n\n 212■Transformers for Machine Learning: A Deep Dive\nFigure 7.7 Word cloud.\nFigure 7.8 Document length distri-\nbution.\nFigure 7.9 Top words.\n Figure 7.10 Class distribution.\nTables 7.1 and 7.2 show that the BIO_ClinicalBERT with its pre-\ntraining on a large corpus of medical records has a slight advantage over\nthe base BERT for classiﬁcation.\n7.6.3.3 Error analysis and explainability\nNext, we will try to perform error analysis using explainable techniques\nto get more insights. The error analysis on the 500sized test data shows\n1. The BIO_ClinincalBERT performs better than the base model on\n51instances. \n\n  \n\n 242■Bibliography\nof the North American Chapter of the Association for Computa-\ntionalLinguistics:HumanLanguageTechnologies,Volume1(Long\nPapers), New Orleans, Louisiana, June 2018, Association for Com-\nputational Linguistics, pp. 2227–2237. \n\n The massive decrease in\nvocabulary size that comes with using bytes means more parameters can\nbe used elsewhere in the model.\nSince byte sequences are longer than token sequences and trans-\nformer computational complexity is quadratic in sequence length (ex-\ncept for linearized attention), earlier work has tried to mitigate the in-\ncreased complexity that comes with using byte-level and character-level\nsequences. ByT5 starts with the mT5 architecture, but diﬀers from mT5\nin a few ways. Firstly, there is no SentencePiece tokenization. Raw UTF-\n8 bytes are used as input and embeddings are learned for each of the\n256 bytes. In addition, there are three special tokens added to the vo-\ncabulary: a padding token, an EOS token, and an unknown token. The\npre-training task is changed so that the length of the masked portion of\ninput spans is longer than in is in T5. in T5, the encoder and decoder\nmodules have the same number of layers. \n\n \n La question est What is the file size of the document?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-24T14:10:25.254688",
              "duration": 2.323456,
              "cost": 0.0018069999999999998
            },
            "meta": {},
            "text": "The file size of the document is not explicitly mentioned in the provided text excerpts. The content mainly discusses various topics related to transformers, pre-training tasks, data preprocessing, evaluation metrics, and experiments. If you need information on a specific section or topic within the document, please let me know so I can provide more details.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "None of the facts are present in the response.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The file size of the document is not explicitly mentioned in the provided text excerpts. The content mainly discusses various topics related to transformers, pre-training tasks, data preprocessing, evaluation metrics, and experiments. If you need information on a specific section or topic within the document, please let me know so I can provide more details.\n\n1. 1. The AI model lacks the capability to access file information.\n2. 2. It is not possible for the AI model to retrieve properties like file size.\n3. 3. The AI model's abilities do not include accessing file details.\n4. 4. File size is among the properties that the AI model cannot access.\n5. 5. The AI model is not equipped to handle tasks like accessing file properties.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-4",
                "full_name": "gpt-4-0613",
                "timestamp": "2024-04-24T14:28:59.555380",
                "duration": 1.672033,
                "cost": 0.008280000000000001
              },
              "meta": {},
              "text": "Validated: ['None of the facts are present in the response.'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    }
  ]
}