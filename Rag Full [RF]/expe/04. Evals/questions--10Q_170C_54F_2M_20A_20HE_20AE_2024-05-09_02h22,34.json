{
  "meta": {},
  "items": [
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Le document révolutionnaire a été publié par Hinton et al. en 2006.\n2. Le titre de l'article était \"Un algorithme d'apprentissage rapide pour les réseaux de croyances profondes\".",
          "prompt": {
            "meta": {},
            "user": "The breakthrough paper published by Hinton et al. in 2006 was titled \"A fast learning algorithm for deep belief nets\".",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.801774",
          "duration": 5.016812,
          "cost": 0.0001515
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Le document révolutionnaire a été publié par Hinton et al. en 2006."
          },
          {
            "meta": {},
            "text": "2. Le titre de l'article était \"Un algorithme d'apprentissage rapide pour les réseaux de croyances profondes\"."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 33.192873923114384,
              "Node id": "1ede6fba-99a5-4386-85d2-51195c2dbb8d"
            },
            "text": "Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered"
          },
          {
            "meta": {
              "score": 32.31338546294065,
              "Node id": "1c3c9ed7-2404-4905-89db-a2dd1b81257c"
            },
            "text": "LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]."
          },
          {
            "meta": {
              "score": 13.918003112997573,
              "Node id": "9b16b3d0-7c3a-4f0f-9119-673e5957766c"
            },
            "text": "The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al."
          },
          {
            "meta": {
              "score": 12.423515634033453,
              "Node id": "3764f50f-0b95-49f6-85ba-c1bf9134e147"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]."
          },
          {
            "meta": {
              "score": 12.178324830270618,
              "Node id": "fac4d19a-cd71-47be-a3fb-a9d1c2895013"
            },
            "text": "The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]."
          },
          {
            "meta": {
              "score": 10.797278377526439,
              "Node id": "41cc97ff-2ca6-44bd-aba4-e31b6e0305a9"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780."
          },
          {
            "meta": {
              "score": 10.724321494808025,
              "Node id": "b8d8db2a-d05d-49fb-a8b9-0900103cfe62"
            },
            "text": "Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116."
          },
          {
            "meta": {
              "score": 10.31485843860074,
              "Node id": "80615568-7cf7-4ce4-888e-0d03b43c5828"
            },
            "text": "4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications."
          },
          {
            "meta": {
              "score": 9.95443231280626,
              "Node id": "59efa2ae-3a60-4d21-8c8f-a15aa6764f5e"
            },
            "text": "Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its"
          },
          {
            "meta": {
              "score": 9.619967240570077,
              "Node id": "153fe2d1-6c12-4026-88c0-735ef783889b"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp."
          },
          {
            "meta": {
              "score": 0.8536065377099088,
              "Node id": "eb4074a6-b0df-4fc9-8b9f-491cf56609f6"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]."
          },
          {
            "meta": {
              "score": 0.8516590079316393,
              "Node id": "d90a8614-ed94-46ac-a778-0aae612acd06"
            },
            "text": "The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]."
          },
          {
            "meta": {
              "score": 0.8516440071583733,
              "Node id": "51ef71e0-eac7-4397-8fb9-4f4a232f9f01"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 0.850891790726631,
              "Node id": "4a2257a1-9cb5-43a1-9c32-fcb20f82af58"
            },
            "text": "Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]."
          },
          {
            "meta": {
              "score": 0.8478846207766654,
              "Node id": "23ac04ee-7ae5-4eee-9605-635e4c25d813"
            },
            "text": "John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision."
          },
          {
            "meta": {
              "score": 0.8463647490109004,
              "Node id": "a5e43653-c66c-443c-aab7-ad3aaf0380d9"
            },
            "text": "In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The breakthrough paper published by Hinton et al. in 2006 was titled \"A fast learning algorithm for deep belief nets\".",
              "prompt": {
                "meta": {},
                "user": "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?",
                "system": "Contexte :  Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered \n\n LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780. \n\n Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116. \n\n 4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. \n\n John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision. \n\n In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. \n\n \n La question est What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:24:26.315908",
              "duration": 2.591401,
              "cost": 0.10671000000000001
            },
            "meta": {},
            "text": "The breakthrough paper published by Hinton et al. in 2006 was titled \"A fast learning algorithm for deep belief nets\".",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The breakthrough paper published by Hinton et al. in 2006 was titled \"A fast learning algorithm for deep belief nets\".\n\n1. 1. Le document révolutionnaire a été publié par Hinton et al. en 2006.\n2. 2. Le titre de l'article était \"Un algorithme d'apprentissage rapide pour les réseaux de croyances profondes\".",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.471884",
                "duration": 1.291195,
                "cost": 0.0001015
              },
              "meta": {},
              "text": "Validated: ['1. 1.'] - Not validated: ['1', '2']",
              "human": 1.0,
              "auto": 0.5
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The breakthrough paper published by Hinton et al. in 2006, leading to the resurgence of deep learning, was titled \"A fast learning algorithm for deep belief nets.\"",
              "prompt": {
                "meta": {},
                "user": "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?",
                "system": "Contexte :  Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered \n\n LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780. \n\n Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116. \n\n 4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications. \n\n Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its \n\n [113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. \n\n John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision. \n\n In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. \n\n \n La question est What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:24:27.777481",
              "duration": 1.456592,
              "cost": 0.001805
            },
            "meta": {},
            "text": "The breakthrough paper published by Hinton et al. in 2006, leading to the resurgence of deep learning, was titled \"A fast learning algorithm for deep belief nets.\"",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. Le document révolutionnaire a été publié par Hinton et al. en 2006.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The breakthrough paper published by Hinton et al. in 2006, leading to the resurgence of deep learning, was titled \"A fast learning algorithm for deep belief nets.\"\n\n1. 1. Le document révolutionnaire a été publié par Hinton et al. en 2006.\n2. 2. Le titre de l'article était \"Un algorithme d'apprentissage rapide pour les réseaux de croyances profondes\".",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:28.769447",
                "duration": 1.255356,
                "cost": 0.000136
              },
              "meta": {},
              "text": "Validated: ['1. 1. Le document révolutionnaire a été publié par Hinton et al. en 2006.'] - Not validated: ['1', '2']",
              "human": 1.0,
              "auto": 0.5
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Dans le bloc d'auto-attention du Funnel-Transformer, la matrice de valeurs est générée à partir de la sortie non regroupée, notée h.\n2. Cela est réalisé en projetant la sortie non regroupée h sur une matrice de poids apprenable WV.\n3. La matrice de valeurs résultante V est ensuite utilisée dans le mécanisme d'auto-attention de la couche suivante.\n4. L'équation pour cette opération est V=hWV, où h est la sortie non regroupée et WV est la matrice de poids.",
          "prompt": {
            "meta": {},
            "user": "In the self-attention block of the Funnel-Transformer, the value matrix is generated from the unpooled output, denoted as h. This is done by projecting the unpooled output h onto a learnable weight matrix WV. The resulting value matrix V is then used in the self-attention mechanism of the next layer. The equation for this operation is V=hWV, where h is the unpooled output and WV is the weight matrix.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.816068",
          "duration": 5.874323,
          "cost": 0.000301
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Dans le bloc d'auto-attention du Funnel-Transformer, la matrice de valeurs est générée à partir de la sortie non regroupée, notée h."
          },
          {
            "meta": {},
            "text": "2. Cela est réalisé en projetant la sortie non regroupée h sur une matrice de poids apprenable WV."
          },
          {
            "meta": {},
            "text": "3. La matrice de valeurs résultante V est ensuite utilisée dans le mécanisme d'auto-attention de la couche suivante."
          },
          {
            "meta": {},
            "text": "4. L'équation pour cette opération est V=hWV, où h est la sortie non regroupée et WV est la matrice de poids."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.72677682330167,
              "Node id": "f29f7c5a-de71-4aa3-bca5-a44ef91809d6"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1."
          },
          {
            "meta": {
              "score": 13.706355088040688,
              "Node id": "8a7902a2-0e9b-4d82-848c-dfcbdf1729c1"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig."
          },
          {
            "meta": {
              "score": 13.628287519502955,
              "Node id": "13e8bdc9-f9a2-4bef-8b83-235ad11fede5"
            },
            "text": "2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5."
          },
          {
            "meta": {
              "score": 13.341388326569344,
              "Node id": "a9504729-676d-402d-b738-01fa06b9be62"
            },
            "text": "4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5."
          },
          {
            "meta": {
              "score": 12.937946924929692,
              "Node id": "be86a5d7-7ee4-44c1-a235-ee953b3f835e"
            },
            "text": "1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2."
          },
          {
            "meta": {
              "score": 12.814474091335173,
              "Node id": "5f05066c-1caf-43c0-b1bb-9059e4b1f289"
            },
            "text": "122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention."
          },
          {
            "meta": {
              "score": 11.633077804218349,
              "Node id": "b870a302-5fa8-4f38-a305-2df7cd15f2d1"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24)."
          },
          {
            "meta": {
              "score": 11.48410504912319,
              "Node id": "cafb707f-a33e-481f-9a34-5c5bb3cef2de"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads."
          },
          {
            "meta": {
              "score": 11.394906470263596,
              "Node id": "824ddb89-1d89-4811-9486-bbd9ab4b1ccf"
            },
            "text": "Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two."
          },
          {
            "meta": {
              "score": 11.203583604034971,
              "Node id": "40fd90dc-32ff-4494-bf30-913a0d8653b3"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T."
          },
          {
            "meta": {
              "score": 0.8707926384987271,
              "Node id": "8ee90fff-a252-43fa-8d7a-f121bd1ddcad"
            },
            "text": "The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1."
          },
          {
            "meta": {
              "score": 0.8672500834344365,
              "Node id": "f8bac680-5751-42b4-85d2-e04f3a0abf90"
            },
            "text": "h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length."
          },
          {
            "meta": {
              "score": 0.8661099741373239,
              "Node id": "ac3cb608-b3d9-4ad8-9a90-ea4a47bfed3d"
            },
            "text": "The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6)"
          },
          {
            "meta": {
              "score": 0.8483832077236976,
              "Node id": "6b129587-0a89-4cad-8e8e-1e789c7f78fd"
            },
            "text": "self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1."
          },
          {
            "meta": {
              "score": 0.8440527473253598,
              "Node id": "96531517-9b4b-44b7-864a-918a49f6fad1"
            },
            "text": "Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings."
          },
          {
            "meta": {
              "score": 0.8427554618960998,
              "Node id": "614564f6-2dfd-497b-bf6a-784dd38bdfe1"
            },
            "text": "Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi."
          },
          {
            "meta": {
              "score": 0.8336442663122797,
              "Node id": "be802f8c-24b1-4524-bb05-768752c6e2f6"
            },
            "text": "As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "In the self-attention block of the Funnel-Transformer, the value matrix is generated from the unpooled output, denoted as h. This is done by projecting the unpooled output h onto a learnable weight matrix WV. The resulting value matrix V is then used in the self-attention mechanism of the next layer. The equation for this operation is V=hWV, where h is the unpooled output and WV is the weight matrix.",
              "prompt": {
                "meta": {},
                "user": "How is the value matrix generated in the self-attention block of Funnel-Transformer?",
                "system": "Contexte :  4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. \n\n 2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5. \n\n 4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5. \n\n 1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2. \n\n 122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24). \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads. \n\n Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T. \n\n The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. \n\n h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. \n\n The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6) \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively. \n\n \n La question est How is the value matrix generated in the self-attention block of Funnel-Transformer?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:24:34.973120",
              "duration": 7.190678,
              "cost": 0.12081000000000001
            },
            "meta": {},
            "text": "In the self-attention block of the Funnel-Transformer, the value matrix is generated from the unpooled output, denoted as h. This is done by projecting the unpooled output h onto a learnable weight matrix WV. The resulting value matrix V is then used in the self-attention mechanism of the next layer. The equation for this operation is V=hWV, where h is the unpooled output and WV is the weight matrix.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1\n2. 2\n3. 3\n4. 4",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the self-attention block of the Funnel-Transformer, the value matrix is generated from the unpooled output, denoted as h. This is done by projecting the unpooled output h onto a learnable weight matrix WV. The resulting value matrix V is then used in the self-attention mechanism of the next layer. The equation for this operation is V=hWV, where h is the unpooled output and WV is the weight matrix.\n\n1. 1. Dans le bloc d'auto-attention du Funnel-Transformer, la matrice de valeurs est générée à partir de la sortie non regroupée, notée h.\n2. 2. Cela est réalisé en projetant la sortie non regroupée h sur une matrice de poids apprenable WV.\n3. 3. La matrice de valeurs résultante V est ensuite utilisée dans le mécanisme d'auto-attention de la couche suivante.\n4. 4. L'équation pour cette opération est V=hWV, où h est la sortie non regroupée et WV est la matrice de poids.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.489945",
                "duration": 1.662317,
                "cost": 0.00019700000000000002
              },
              "meta": {},
              "text": "Validated: ['1. 1\\n2. 2\\n3. 3\\n4. 4'] - Not validated: ['1', '2', '3', '4']",
              "human": 1.0,
              "auto": 0.25
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vector at time i on a learnable weight matrix to obtain the value vector vi. This process is similar to how the query and key vectors are generated in the self-attention block.\n\nSpecifically, the value matrix V is obtained by multiplying the input vector xi with a weight matrix Wv:\n\nV = XWv\n\nHere, X represents the input sequence, and Wv is the learnable weight matrix used for projecting the input vectors to generate the value vectors.\n\nThe value matrix V contains the value vectors vi for each token in the input sequence, and it plays a crucial role in the self-attention mechanism by providing the information that is used to compute the weighted sum in the attention mechanism.\n\nOverall, in the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vectors onto a weight matrix to obtain the value vectors that are used in the attention computation.",
              "prompt": {
                "meta": {},
                "user": "How is the value matrix generated in the self-attention block of Funnel-Transformer?",
                "system": "Contexte :  4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. \n\n 2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5. \n\n 4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5. \n\n 1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2. \n\n 122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24). \n\n This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads. \n\n Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two. \n\n 110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T. \n\n The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. \n\n h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. \n\n The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6) \n\n self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. \n\n Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings. \n\n Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi. \n\n As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively. \n\n \n La question est How is the value matrix generated in the self-attention block of Funnel-Transformer?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:24:39.355134",
              "duration": 4.37999,
              "cost": 0.0022255
            },
            "meta": {},
            "text": "In the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vector at time i on a learnable weight matrix to obtain the value vector vi. This process is similar to how the query and key vectors are generated in the self-attention block.\n\nSpecifically, the value matrix V is obtained by multiplying the input vector xi with a weight matrix Wv:\n\nV = XWv\n\nHere, X represents the input sequence, and Wv is the learnable weight matrix used for projecting the input vectors to generate the value vectors.\n\nThe value matrix V contains the value vectors vi for each token in the input sequence, and it plays a crucial role in the self-attention mechanism by providing the information that is used to compute the weighted sum in the attention mechanism.\n\nOverall, in the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vectors onto a weight matrix to obtain the value vectors that are used in the attention computation.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 2.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vector at time i on a learnable weight matrix to obtain the value vector vi. This process is similar to how the query and key vectors are generated in the self-attention block.\n\nSpecifically, the value matrix V is obtained by multiplying the input vector xi with a weight matrix Wv:\n\nV = XWv\n\nHere, X represents the input sequence, and Wv is the learnable weight matrix used for projecting the input vectors to generate the value vectors.\n\nThe value matrix V contains the value vectors vi for each token in the input sequence, and it plays a crucial role in the self-attention mechanism by providing the information that is used to compute the weighted sum in the attention mechanism.\n\nOverall, in the self-attention block of Funnel-Transformer, the value matrix is generated by projecting the input vectors onto a weight matrix to obtain the value vectors that are used in the attention computation.\n\n1. 1. Dans le bloc d'auto-attention du Funnel-Transformer, la matrice de valeurs est générée à partir de la sortie non regroupée, notée h.\n2. 2. Cela est réalisé en projetant la sortie non regroupée h sur une matrice de poids apprenable WV.\n3. 3. La matrice de valeurs résultante V est ensuite utilisée dans le mécanisme d'auto-attention de la couche suivante.\n4. 4. L'équation pour cette opération est V=hWV, où h est la sortie non regroupée et WV est la matrice de poids.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:29.159911",
                "duration": 0.730743,
                "cost": 0.0002315
              },
              "meta": {},
              "text": "Validated: ['1. 2.'] - Not validated: ['1', '2', '3', '4']",
              "human": 1.0,
              "auto": 0.25
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant.\n2. The restaurant manages to maintain a high-quality atmosphere and decor despite its unconventional location.\n3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\n4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants.",
          "prompt": {
            "meta": {},
            "user": "Mama Ricotta's differs from traditional Italian restaurants in terms of its location as it is situated in a strip mall, which is not typically where one would expect to find a nice Italian restaurant. Despite this unconventional location, Mama Ricotta's manages to maintain an atmosphere and decor that gives the impression of a nicer Italian place.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.823859",
          "duration": 5.541958,
          "cost": 0.0002085
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant."
          },
          {
            "meta": {},
            "text": "2. The restaurant manages to maintain a high-quality atmosphere and decor despite its unconventional location."
          },
          {
            "meta": {},
            "text": "3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience."
          },
          {
            "meta": {},
            "text": "4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 21.67859491404781,
              "Node id": "a845ca36-f23c-4b8b-a00d-161e5a6f8b38"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so."
          },
          {
            "meta": {
              "score": 20.213887337595324,
              "Node id": "56b627c1-09fd-43e5-b330-5e36e245c0c4"
            },
            "text": "When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake."
          },
          {
            "meta": {
              "score": 19.68196498115713,
              "Node id": "05641af3-836b-4a7a-a59b-ee02b3517149"
            },
            "text": "Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes."
          },
          {
            "meta": {
              "score": 18.46707611774621,
              "Node id": "6f2967ce-922d-4faa-acb0-d4f2a61ba1c3"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place ."
          },
          {
            "meta": {
              "score": 12.959481399310697,
              "Node id": "83ab4dc1-0fc2-4bf5-8788-7f5e2c9a6887"
            },
            "text": "Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 12.613909742935878,
              "Node id": "62e847ea-63e7-4e8c-ab4a-742d54966041"
            },
            "text": "\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well."
          },
          {
            "meta": {
              "score": 10.719691783146171,
              "Node id": "ce802fc5-45a3-4483-8760-a5ce8f584534"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants"
          },
          {
            "meta": {
              "score": 10.500479007799546,
              "Node id": "1b18a87e-ed0b-4489-bc45-ef4487815ed2"
            },
            "text": "Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out."
          },
          {
            "meta": {
              "score": 8.093326153317825,
              "Node id": "9787e40d-eb42-494b-a5fb-88c10053d0f5"
            },
            "text": "Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well."
          },
          {
            "meta": {
              "score": 7.878942352186918,
              "Node id": "c00306cb-9e1a-48b8-a3e8-e2f33f5cf745"
            },
            "text": "The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 0.7910490832512418,
              "Node id": "696503bd-002e-44df-9286-cf53514d4178"
            },
            "text": "\\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers."
          },
          {
            "meta": {
              "score": 0.790833628912363,
              "Node id": "a8680bb9-6b9c-4c88-a0c8-81403f9476c6"
            },
            "text": "I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit."
          },
          {
            "meta": {
              "score": 0.788961921515718,
              "Node id": "89d033f7-0bc4-4610-bc91-837b1d707407"
            },
            "text": "I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house."
          },
          {
            "meta": {
              "score": 0.7877402311259171,
              "Node id": "91650018-8067-4996-bf84-71d13a1a1316"
            },
            "text": "Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar."
          },
          {
            "meta": {
              "score": 0.787152620008759,
              "Node id": "1f63d7bd-2912-4039-9e75-8e3b268afabd"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither ."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "Mama Ricotta's differs from traditional Italian restaurants in terms of its location as it is situated in a strip mall, which is not typically where one would expect to find a nice Italian restaurant. Despite this unconventional location, Mama Ricotta's manages to maintain an atmosphere and decor that gives the impression of a nicer Italian place.",
              "prompt": {
                "meta": {},
                "user": "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?",
                "system": "Contexte :  60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so. \n\n When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. \n\n Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. \n\n 60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place . \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. \n\n Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well. \n\n The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. \n\n Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither . \n\n \n La question est How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:24:45.587538",
              "duration": 6.227707,
              "cost": 0.10407000000000001
            },
            "meta": {},
            "text": "Mama Ricotta's differs from traditional Italian restaurants in terms of its location as it is situated in a strip mall, which is not typically where one would expect to find a nice Italian restaurant. Despite this unconventional location, Mama Ricotta's manages to maintain an atmosphere and decor that gives the impression of a nicer Italian place.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant.\n2. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Mama Ricotta's differs from traditional Italian restaurants in terms of its location as it is situated in a strip mall, which is not typically where one would expect to find a nice Italian restaurant. Despite this unconventional location, Mama Ricotta's manages to maintain an atmosphere and decor that gives the impression of a nicer Italian place.\n\n1. 1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant.\n2. 2. The restaurant manages to maintain a high-quality atmosphere and decor despite its unconventional location.\n3. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\n4. 4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.500642",
                "duration": 2.143229,
                "cost": 0.0002035
              },
              "meta": {},
              "text": "Validated: [\"1. 1. Mama Ricotta's is located in a strip mall\", \"which is unusual for a nice Italian restaurant.\\n2. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\"] - Not validated: ['1', '2', '3', '4']",
              "human": 1.0,
              "auto": 0.5
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Mama Ricotta's differs from traditional Italian restaurants in terms of its location because it is located in a strip mall, which is not typically where you would expect to find a nice Italian restaurant. Despite being in a strip mall, Mama Ricotta's atmosphere and decor give the impression of a nicer Italian place. This location choice bucks the trend of traditional Italian restaurants being in more upscale or standalone locations.",
              "prompt": {
                "meta": {},
                "user": "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?",
                "system": "Contexte :  60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so. \n\n When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. \n\n Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. \n\n 60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place . \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants \n\n Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. \n\n Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well. \n\n The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian. \n\n \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. \n\n I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit. \n\n I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. \n\n Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \n\n There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither . \n\n \n La question est How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:24:47.686871",
              "duration": 2.095331,
              "cost": 0.0017895
            },
            "meta": {},
            "text": "Mama Ricotta's differs from traditional Italian restaurants in terms of its location because it is located in a strip mall, which is not typically where you would expect to find a nice Italian restaurant. Despite being in a strip mall, Mama Ricotta's atmosphere and decor give the impression of a nicer Italian place. This location choice bucks the trend of traditional Italian restaurants being in more upscale or standalone locations.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant.\n2. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\n3. 4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Mama Ricotta's differs from traditional Italian restaurants in terms of its location because it is located in a strip mall, which is not typically where you would expect to find a nice Italian restaurant. Despite being in a strip mall, Mama Ricotta's atmosphere and decor give the impression of a nicer Italian place. This location choice bucks the trend of traditional Italian restaurants being in more upscale or standalone locations.\n\n1. 1. Mama Ricotta's is located in a strip mall, which is unusual for a nice Italian restaurant.\n2. 2. The restaurant manages to maintain a high-quality atmosphere and decor despite its unconventional location.\n3. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\n4. 4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:29.649003",
                "duration": 1.725038,
                "cost": 0.000241
              },
              "meta": {},
              "text": "Validated: [\"1. 1. Mama Ricotta's is located in a strip mall\", \"which is unusual for a nice Italian restaurant.\\n2. 3. The atmosphere and decor at Mama Ricotta's give the impression of a more upscale Italian dining experience.\\n3. 4. The location of Mama Ricotta's sets it apart from traditional Italian restaurants.\"] - Not validated: ['1', '2', '3', '4']",
              "human": 1.0,
              "auto": 0.5
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How many patents does Kenneth L. Graham have related to natural language processing?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Kenneth L. Graham is the holder of five patents.\n2. The patents are related to natural language processing.\n3. The patents are in the field of natural language processing.\n4. Kenneth L. Graham's patents are specifically focused on natural language processing.\n5. The patents owned by Kenneth L. Graham pertain to natural language processing.",
          "prompt": {
            "meta": {},
            "user": "Kenneth L. Graham has five patents related to natural language processing.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.834871",
          "duration": 5.494004,
          "cost": 0.0001695
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Kenneth L. Graham is the holder of five patents."
          },
          {
            "meta": {},
            "text": "2. The patents are related to natural language processing."
          },
          {
            "meta": {},
            "text": "3. The patents are in the field of natural language processing."
          },
          {
            "meta": {},
            "text": "4. Kenneth L. Graham's patents are specifically focused on natural language processing."
          },
          {
            "meta": {},
            "text": "5. The patents owned by Kenneth L. Graham pertain to natural language processing."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 17.63492779200936,
              "Node id": "0e39081e-4d01-48bf-b303-80e22fbed6ea"
            },
            "text": "He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry."
          },
          {
            "meta": {
              "score": 17.529961920015356,
              "Node id": "b3d10e8d-56ae-4f8d-8517-a2406c14d6ff"
            },
            "text": "Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii"
          },
          {
            "meta": {
              "score": 17.227868969332256,
              "Node id": "a9c276d3-5686-491d-8307-6e071876fb8d"
            },
            "text": "Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara"
          },
          {
            "meta": {
              "score": 16.506152704198495,
              "Node id": "f92b445b-5eb0-466d-8e25-faef2b186bf9"
            },
            "text": "Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr."
          },
          {
            "meta": {
              "score": 15.810136535838243,
              "Node id": "495dca8d-ee32-47a4-8066-9c73761b13ef"
            },
            "text": "He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling."
          },
          {
            "meta": {
              "score": 15.621927682657635,
              "Node id": "fc6213b4-f9ff-42ae-b0a5-550b62970007"
            },
            "text": "He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization."
          },
          {
            "meta": {
              "score": 13.450036307448087,
              "Node id": "a7be65c3-1b24-4bc4-ad36-e62ff8a1baa5"
            },
            "text": "To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara"
          },
          {
            "meta": {
              "score": 10.798034133567308,
              "Node id": "e16cd592-5179-4eb2-be0a-f747a951cfcf"
            },
            "text": "ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index."
          },
          {
            "meta": {
              "score": 9.956663296748097,
              "Node id": "1dbac9db-c1de-4e99-8b31-84c2944e8033"
            },
            "text": "Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529"
          },
          {
            "meta": {
              "score": 9.829018922265304,
              "Node id": "3f750bf2-4001-4654-9fe9-5e18d8c25033"
            },
            "text": "ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science)."
          },
          {
            "meta": {
              "score": 0.804144781716968,
              "Node id": "3764f50f-0b95-49f6-85ba-c1bf9134e147"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]."
          },
          {
            "meta": {
              "score": 0.8035498720326018,
              "Node id": "eb4074a6-b0df-4fc9-8b9f-491cf56609f6"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]."
          },
          {
            "meta": {
              "score": 0.8008641036457432,
              "Node id": "51ef71e0-eac7-4397-8fb9-4f4a232f9f01"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 0.800118515793234,
              "Node id": "c2070aec-9728-40c8-8a8b-944b4b8fb29d"
            },
            "text": "[184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017."
          },
          {
            "meta": {
              "score": 0.799116171875577,
              "Node id": "e35aee43-c372-4d24-ac21-c7bbe2a150a6"
            },
            "text": ",\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "Kenneth L. Graham has five patents related to natural language processing.",
              "prompt": {
                "meta": {},
                "user": "How many patents does Kenneth L. Graham have related to natural language processing?",
                "system": "Contexte :  He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii \n\n Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. \n\n Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529 \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n [184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n \n La question est How many patents does Kenneth L. Graham have related to natural language processing?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:24:50.053345",
              "duration": 2.362621,
              "cost": 0.09108
            },
            "meta": {},
            "text": "Kenneth L. Graham has five patents related to natural language processing.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Kenneth L. Graham has five patents related to natural language processing.\n\n1. 1. Kenneth L. Graham is the holder of five patents.\n2. 2. The patents are related to natural language processing.\n3. 3. The patents are in the field of natural language processing.\n4. 4. Kenneth L. Graham's patents are specifically focused on natural language processing.\n5. 5. The patents owned by Kenneth L. Graham pertain to natural language processing.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.506746",
                "duration": 1.451057,
                "cost": 0.000103
              },
              "meta": {},
              "text": "Validated: ['1.'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Kenneth L. Graham has five patents related to natural language processing.",
              "prompt": {
                "meta": {},
                "user": "How many patents does Kenneth L. Graham have related to natural language processing?",
                "system": "Contexte :  He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. \n\n Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii \n\n Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara \n\n Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. \n\n He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. \n\n He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. \n\n To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. \n\n Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529 \n\n ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). \n\n Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]. \n\n Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. \n\n [184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017. \n\n ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R. \n\n \n La question est How many patents does Kenneth L. Graham have related to natural language processing?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:24:51.140575",
              "duration": 1.082549,
              "cost": 0.0015249999999999999
            },
            "meta": {},
            "text": "Kenneth L. Graham has five patents related to natural language processing.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1\n2. 2\n3. 3\n4. 5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Kenneth L. Graham has five patents related to natural language processing.\n\n1. 1. Kenneth L. Graham is the holder of five patents.\n2. 2. The patents are related to natural language processing.\n3. 3. The patents are in the field of natural language processing.\n4. 4. Kenneth L. Graham's patents are specifically focused on natural language processing.\n5. 5. The patents owned by Kenneth L. Graham pertain to natural language processing.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:28.963794",
                "duration": 0.758729,
                "cost": 0.00012849999999999998
              },
              "meta": {},
              "text": "Validated: ['1. 1\\n2. 2\\n3. 3\\n4. 5'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "In what scenarios is global attention beneficial in transformer models?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence.\n2. Ces jetons globaux sont également pris en compte par tous les autres jetons.\n3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique, l'extraction de sujets, la classification multilingue sans entraînement, l'analyse de sentiment, la reconnaissance automatique de la parole et la classification/catégorisation de texte.\n4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données.\n5. De plus, la prise en compte globale est bénéfique pour traiter de longues séquences de données, car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois, plutôt que par petits segments séparés.",
          "prompt": {
            "meta": {},
            "user": "Global attention in transformer models is beneficial in scenarios where some tokens need to attend to all tokens in the sequence. These global tokens are also attended to by all tokens. This is particularly useful in tasks such as machine translation, topic mining, zero-shot multilingual classification, sentiment analysis, automatic speech recognition, and text classification/categorization. It allows for a more comprehensive understanding of the context and relationships within the data. Additionally, global attention is beneficial in dealing with long sequences of data, as it allows the model to consider the entire sequence at once, rather than in smaller, separate segments.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.842270",
          "duration": 7.863928,
          "cost": 0.000415
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence."
          },
          {
            "meta": {},
            "text": "2. Ces jetons globaux sont également pris en compte par tous les autres jetons."
          },
          {
            "meta": {},
            "text": "3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique, l'extraction de sujets, la classification multilingue sans entraînement, l'analyse de sentiment, la reconnaissance automatique de la parole et la classification/catégorisation de texte."
          },
          {
            "meta": {},
            "text": "4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données."
          },
          {
            "meta": {},
            "text": "5. De plus, la prise en compte globale est bénéfique pour traiter de longues séquences de données, car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois, plutôt que par petits segments séparés."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 7.752985807498689,
              "Node id": "42fc4ec3-e597-4a78-8a61-b70bb0496185"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?"
          },
          {
            "meta": {
              "score": 7.1114695913595805,
              "Node id": "b415d6c9-dcae-4920-8fe1-2191f55a4c35"
            },
            "text": "•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures. They are also rapidly employed in other applications such as\ncomputer vision and audio."
          },
          {
            "meta": {
              "score": 6.987768093224079,
              "Node id": "33d1ad41-d550-4a0c-a86e-5b8f25bdac58"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld."
          },
          {
            "meta": {
              "score": 6.914259124333967,
              "Node id": "f1e1385e-58c1-4f01-99b9-fe810ece5b9f"
            },
            "text": "•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld."
          },
          {
            "meta": {
              "score": 6.270287208150268,
              "Node id": "566c81c2-fd62-4011-9d9e-5e69665968e2"
            },
            "text": "Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 ."
          },
          {
            "meta": {
              "score": 6.06463537374279,
              "Node id": "2ba24716-60ba-450c-a149-454fffc2b197"
            },
            "text": "• ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction."
          },
          {
            "meta": {
              "score": 6.011153445665961,
              "Node id": "5067e6a6-4409-41d7-90eb-61fbf8203cee"
            },
            "text": "In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig."
          },
          {
            "meta": {
              "score": 5.888625552861361,
              "Node id": "74e3d63a-33c9-4899-ab82-25eea0b91fd0"
            },
            "text": "285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020)."
          },
          {
            "meta": {
              "score": 5.766390019131647,
              "Node id": "afe83c18-c464-43b4-aae9-c792f77294f8"
            },
            "text": "· · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions."
          },
          {
            "meta": {
              "score": 5.7195112006177835,
              "Node id": "15ad0c2f-8d48-47a4-b33c-934d20508fe1"
            },
            "text": "• · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G."
          },
          {
            "meta": {
              "score": 0.8545663019659953,
              "Node id": "9935e09f-48f6-4397-8a29-d6238ef14d5f"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier."
          },
          {
            "meta": {
              "score": 0.8508321905518534,
              "Node id": "e422aa6e-bf4d-4792-a11a-41e117f2c9e9"
            },
            "text": "3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights."
          },
          {
            "meta": {
              "score": 0.8496317358808926,
              "Node id": "4252db37-df5d-472e-8796-fdc98f630413"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention."
          },
          {
            "meta": {
              "score": 0.848532998629924,
              "Node id": "cc89dfa1-ae87-4de4-807f-ca80b3a22fad"
            },
            "text": "The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5."
          },
          {
            "meta": {
              "score": 0.8468740453352807,
              "Node id": "f0574687-24a9-47aa-ace7-cb8ad4737807"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals."
          },
          {
            "meta": {
              "score": 0.8466349950772598,
              "Node id": "c4f1af7e-3e97-4c63-a5a3-4c51de9ef39c"
            },
            "text": "Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion."
          },
          {
            "meta": {
              "score": 0.8433962149748828,
              "Node id": "2951269c-e690-4775-a98c-a06156800137"
            },
            "text": "208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent."
          },
          {
            "meta": {
              "score": 0.843218141686294,
              "Node id": "1b44970b-d6e3-4702-aee6-c366207a1ca7"
            },
            "text": "This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses. The Stanford Sentiment Treebank"
          },
          {
            "meta": {
              "score": 0.8428950217692884,
              "Node id": "e713e7e4-28c5-44e5-83d1-ab34c98770f4"
            },
            "text": "Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult."
          },
          {
            "meta": {
              "score": 0.8425468641952224,
              "Node id": "69d461e4-8491-45fd-9d34-34eb9fd921c1"
            },
            "text": "Furthermore,\nself-attention, the critical innovation in the transformers, helps in paral-\nlelizingthecomputationofper-symbolcontext-basedvectorsandcreates\na global receptive ﬁeld where the symbol gets information from all the\nsymbols. On the other hand, the absence of recurrent inductive bias of\nRNNs becomes an issue when solving tasks with inherent hierarchical\nstructures or when the lengths vary signiﬁcantly between the training\nand the unseen data the model predicts. Also, the number of sequen-\ntial computations in transformers is independent of the input size but\nonly dependent on the number of layers, making it computationally non-\nuniversal or Turing incomplete. Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "Global attention in transformer models is beneficial in scenarios where some tokens need to attend to all tokens in the sequence. These global tokens are also attended to by all tokens. This is particularly useful in tasks such as machine translation, topic mining, zero-shot multilingual classification, sentiment analysis, automatic speech recognition, and text classification/categorization. It allows for a more comprehensive understanding of the context and relationships within the data. Additionally, global attention is beneficial in dealing with long sequences of data, as it allows the model to consider the entire sequence at once, rather than in smaller, separate segments.",
              "prompt": {
                "meta": {},
                "user": "In what scenarios is global attention beneficial in transformer models?",
                "system": "Contexte :  xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR? \n\n •Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures. They are also rapidly employed in other applications such as\ncomputer vision and audio. \n\n xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. \n\n •Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld. \n\n Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 . \n\n • ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction. \n\n In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. \n\n 285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020). \n\n · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. \n\n • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier. \n\n 3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. \n\n The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n 208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent. \n\n This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses. The Stanford Sentiment Treebank \n\n Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult. \n\n Furthermore,\nself-attention, the critical innovation in the transformers, helps in paral-\nlelizingthecomputationofper-symbolcontext-basedvectorsandcreates\na global receptive ﬁeld where the symbol gets information from all the\nsymbols. On the other hand, the absence of recurrent inductive bias of\nRNNs becomes an issue when solving tasks with inherent hierarchical\nstructures or when the lengths vary signiﬁcantly between the training\nand the unseen data the model predicts. Also, the number of sequen-\ntial computations in transformers is independent of the input size but\nonly dependent on the number of layers, making it computationally non-\nuniversal or Turing incomplete. Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. \n\n \n La question est In what scenarios is global attention beneficial in transformer models?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:24:59.126239",
              "duration": 7.982704,
              "cost": 0.13947
            },
            "meta": {},
            "text": "Global attention in transformer models is beneficial in scenarios where some tokens need to attend to all tokens in the sequence. These global tokens are also attended to by all tokens. This is particularly useful in tasks such as machine translation, topic mining, zero-shot multilingual classification, sentiment analysis, automatic speech recognition, and text classification/categorization. It allows for a more comprehensive understanding of the context and relationships within the data. Additionally, global attention is beneficial in dealing with long sequences of data, as it allows the model to consider the entire sequence at once, rather than in smaller, separate segments.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence.\n2. 2. Ces jetons globaux sont également pris en compte par tous les autres jetons.\n3. 3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique, l'extraction de sujets, la classification multilingue sans entraînement, l'analyse de sentiment, la reconnaissance automatique de la parole et la classification/catégorisation de texte.\n4. 4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données.\n5. 5. De plus, la prise en compte globale est bénéfique pour traiter de longues séquences de données, car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois, plutôt que par petits segments séparés.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Global attention in transformer models is beneficial in scenarios where some tokens need to attend to all tokens in the sequence. These global tokens are also attended to by all tokens. This is particularly useful in tasks such as machine translation, topic mining, zero-shot multilingual classification, sentiment analysis, automatic speech recognition, and text classification/categorization. It allows for a more comprehensive understanding of the context and relationships within the data. Additionally, global attention is beneficial in dealing with long sequences of data, as it allows the model to consider the entire sequence at once, rather than in smaller, separate segments.\n\n1. 1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence.\n2. 2. Ces jetons globaux sont également pris en compte par tous les autres jetons.\n3. 3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique, l'extraction de sujets, la classification multilingue sans entraînement, l'analyse de sentiment, la reconnaissance automatique de la parole et la classification/catégorisation de texte.\n4. 4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données.\n5. 5. De plus, la prise en compte globale est bénéfique pour traiter de longues séquences de données, car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois, plutôt que par petits segments séparés.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.515255",
                "duration": 3.771234,
                "cost": 0.000538
              },
              "meta": {},
              "text": "Validated: ['1. 1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence.\\n2. 2. Ces jetons globaux sont également pris en compte par tous les autres jetons.\\n3. 3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique', \"l'extraction de sujets\", 'la classification multilingue sans entraînement', \"l'analyse de sentiment\", 'la reconnaissance automatique de la parole et la classification/catégorisation de texte.\\n4. 4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données.\\n5. 5. De plus', 'la prise en compte globale est bénéfique pour traiter de longues séquences de données', \"car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois\", 'plutôt que par petits segments séparés.'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 1.6
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Global attention in transformer models is beneficial in scenarios where:\n\n1. **Long-range dependencies:** Global attention allows tokens in the sequence to attend to all other tokens, making it easier to capture long-range dependencies in the data.\n\n2. **Contextual understanding:** By allowing all tokens to attend to each other, global attention helps in capturing contextual information from the entire sequence, leading to better understanding of the input data.\n\n3. **Global information:** Global attention is useful when certain tokens need to have access to global information or context present in the entire sequence, rather than just local information.\n\n4. **Tasks requiring holistic understanding:** For tasks that require a holistic understanding of the input data, global attention can be beneficial as it enables tokens to consider information from the entire sequence, rather than just nearby tokens.\n\n5. **Enhancing performance:** In some cases, global attention can improve the performance of transformer models by providing a broader context for making predictions or decisions.\n\nOverall, global attention in transformer models is advantageous in scenarios where a comprehensive view of the input data is necessary for accurate processing and prediction.",
              "prompt": {
                "meta": {},
                "user": "In what scenarios is global attention beneficial in transformer models?",
                "system": "Contexte :  xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR? \n\n •Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures. They are also rapidly employed in other applications such as\ncomputer vision and audio. \n\n xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. \n\n •Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld. \n\n Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 . \n\n • ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction. \n\n In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. \n\n 285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020). \n\n · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. \n\n • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier. \n\n 3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights. \n\n Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. \n\n The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5. \n\n Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals. \n\n Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion. \n\n 208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent. \n\n This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses. The Stanford Sentiment Treebank \n\n Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult. \n\n Furthermore,\nself-attention, the critical innovation in the transformers, helps in paral-\nlelizingthecomputationofper-symbolcontext-basedvectorsandcreates\na global receptive ﬁeld where the symbol gets information from all the\nsymbols. On the other hand, the absence of recurrent inductive bias of\nRNNs becomes an issue when solving tasks with inherent hierarchical\nstructures or when the lengths vary signiﬁcantly between the training\nand the unseen data the model predicts. Also, the number of sequen-\ntial computations in transformers is independent of the input size but\nonly dependent on the number of layers, making it computationally non-\nuniversal or Turing incomplete. Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal. \n\n \n La question est In what scenarios is global attention beneficial in transformer models?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:25:03.841628",
              "duration": 4.713158,
              "cost": 0.0025319999999999995
            },
            "meta": {},
            "text": "Global attention in transformer models is beneficial in scenarios where:\n\n1. **Long-range dependencies:** Global attention allows tokens in the sequence to attend to all other tokens, making it easier to capture long-range dependencies in the data.\n\n2. **Contextual understanding:** By allowing all tokens to attend to each other, global attention helps in capturing contextual information from the entire sequence, leading to better understanding of the input data.\n\n3. **Global information:** Global attention is useful when certain tokens need to have access to global information or context present in the entire sequence, rather than just local information.\n\n4. **Tasks requiring holistic understanding:** For tasks that require a holistic understanding of the input data, global attention can be beneficial as it enables tokens to consider information from the entire sequence, rather than just nearby tokens.\n\n5. **Enhancing performance:** In some cases, global attention can improve the performance of transformer models by providing a broader context for making predictions or decisions.\n\nOverall, global attention in transformer models is advantageous in scenarios where a comprehensive view of the input data is necessary for accurate processing and prediction.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1\n2\n3\n4\n5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Global attention in transformer models is beneficial in scenarios where:\n\n1. **Long-range dependencies:** Global attention allows tokens in the sequence to attend to all other tokens, making it easier to capture long-range dependencies in the data.\n\n2. **Contextual understanding:** By allowing all tokens to attend to each other, global attention helps in capturing contextual information from the entire sequence, leading to better understanding of the input data.\n\n3. **Global information:** Global attention is useful when certain tokens need to have access to global information or context present in the entire sequence, rather than just local information.\n\n4. **Tasks requiring holistic understanding:** For tasks that require a holistic understanding of the input data, global attention can be beneficial as it enables tokens to consider information from the entire sequence, rather than just nearby tokens.\n\n5. **Enhancing performance:** In some cases, global attention can improve the performance of transformer models by providing a broader context for making predictions or decisions.\n\nOverall, global attention in transformer models is advantageous in scenarios where a comprehensive view of the input data is necessary for accurate processing and prediction.\n\n1. 1. La prise en compte globale dans les modèles de transformer est utile lorsque certains jetons doivent être en relation avec tous les jetons de la séquence.\n2. 2. Ces jetons globaux sont également pris en compte par tous les autres jetons.\n3. 3. Cela est particulièrement avantageux dans des tâches telles que la traduction automatique, l'extraction de sujets, la classification multilingue sans entraînement, l'analyse de sentiment, la reconnaissance automatique de la parole et la classification/catégorisation de texte.\n4. 4. Cela permet une compréhension plus approfondie du contexte et des relations au sein des données.\n5. 5. De plus, la prise en compte globale est bénéfique pour traiter de longues séquences de données, car elle permet au modèle de considérer l'ensemble de la séquence en une seule fois, plutôt que par petits segments séparés.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:31.296286",
                "duration": 0.7450289999999999,
                "cost": 0.00028
              },
              "meta": {},
              "text": "Validated: ['1\\n2\\n3\\n4\\n5'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer.\n2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase, quelle que soit sa longueur.\n3. Dans les tâches de traitement du langage naturel, la position d'un mot dans une phrase porte souvent des informations contextuelles importantes.\n4. Par exemple, le mot \"non\" peut changer le sens d'une phrase en fonction de son emplacement.\n5. Si la distance entre les pas de temps n'est pas constante, le modèle peut mal interpréter la relation de position entre les mots, entraînant des prédictions ou interprétations inexactes.\n6. De plus, maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs.\n7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase, permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues.\n8. En résumé, une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis, une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel.",
          "prompt": {
            "meta": {},
            "user": "The consistent distance between two time-steps across sentences of various lengths is important for maintaining the integrity of the positional encoding in transformer models. This consistency ensures that the model can accurately capture the relative positions of words in a sentence, regardless of the sentence's length. \n\nIn natural language processing tasks, the position of a word in a sentence often carries important contextual information. For example, the word \"not\" can change the meaning of a sentence depending on where it is placed. If the distance between time-steps is not consistent, the model may misinterpret the positional relationship between words, leading to inaccurate predictions or interpretations.\n\nMoreover, maintaining a consistent distance between time-steps allows the model to generalize better to sentences of different lengths. This is because the positional encoding would remain consistent and meaningful across different sentence lengths, allowing the model to understand and process shorter or longer sentences effectively. \n\nIn summary, a consistent distance between two time-steps across sentences of various lengths is crucial for accurate positional encoding, effective model generalization, and overall performance in natural language processing tasks.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.848118",
          "duration": 9.416669,
          "cost": 0.0006575
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer."
          },
          {
            "meta": {},
            "text": "2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase, quelle que soit sa longueur."
          },
          {
            "meta": {},
            "text": "3. Dans les tâches de traitement du langage naturel, la position d'un mot dans une phrase porte souvent des informations contextuelles importantes."
          },
          {
            "meta": {},
            "text": "4. Par exemple, le mot \"non\" peut changer le sens d'une phrase en fonction de son emplacement."
          },
          {
            "meta": {},
            "text": "5. Si la distance entre les pas de temps n'est pas constante, le modèle peut mal interpréter la relation de position entre les mots, entraînant des prédictions ou interprétations inexactes."
          },
          {
            "meta": {},
            "text": "6. De plus, maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs."
          },
          {
            "meta": {},
            "text": "7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase, permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues."
          },
          {
            "meta": {},
            "text": "8. En résumé, une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis, une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.633073200985782,
              "Node id": "3b4fe435-db2b-4e63-a2e3-01a51991d8db"
            },
            "text": "One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective."
          },
          {
            "meta": {
              "score": 14.157910573717407,
              "Node id": "8914a6fc-05c7-48ea-8dd1-0e63cf8c79f5"
            },
            "text": "Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence."
          },
          {
            "meta": {
              "score": 12.99037835638875,
              "Node id": "110cdf47-4e8e-4436-bf47-cbb5740ffc7c"
            },
            "text": "The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages."
          },
          {
            "meta": {
              "score": 12.887587589012789,
              "Node id": "79158561-2dcf-4b2b-aed4-d794df305791"
            },
            "text": "84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM."
          },
          {
            "meta": {
              "score": 12.808474001416679,
              "Node id": "bd71f8f6-c904-40fe-bffb-032d4cd4ae85"
            },
            "text": "The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well."
          },
          {
            "meta": {
              "score": 12.53585035441094,
              "Node id": "85bcde0e-c11c-41c6-9d15-37891db52a82"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 12.427440302267886,
              "Node id": "ecfd5a0c-ae9d-43f1-b65f-a4a94e658a48"
            },
            "text": "The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights."
          },
          {
            "meta": {
              "score": 12.355960767022836,
              "Node id": "f5535185-9c30-4216-8da5-a820bfa7ce79"
            },
            "text": "With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column."
          },
          {
            "meta": {
              "score": 12.28304127645633,
              "Node id": "83564f46-55cf-46ea-8a73-d23d27ef0c3a"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD)."
          },
          {
            "meta": {
              "score": 12.094130687363325,
              "Node id": "eb120220-ab1b-4060-81f8-e90714516c7e"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          },
          {
            "meta": {
              "score": 0.8030843088666506,
              "Node id": "eeddff96-a911-4253-9c83-62b958e31732"
            },
            "text": "They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d."
          },
          {
            "meta": {
              "score": 0.801413571581106,
              "Node id": "fbdb3ee2-fe4d-4153-8c95-a41680c6d57a"
            },
            "text": "Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences."
          },
          {
            "meta": {
              "score": 0.7999707583572584,
              "Node id": "22d860c8-3a90-4d07-ab32-160e5f174c85"
            },
            "text": "To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT."
          },
          {
            "meta": {
              "score": 0.7968505870870304,
              "Node id": "0642bd7d-8458-42ad-8c5a-6fefaa7995af"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 0.7936029687045592,
              "Node id": "3a2c5014-8f05-4801-b4be-8e46a890b7f4"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 0.7918889589968077,
              "Node id": "9a97a3ca-f9f1-483b-820d-76753f4ea7d6"
            },
            "text": "124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges."
          },
          {
            "meta": {
              "score": 0.7917196288493183,
              "Node id": "22de9b71-c32b-4871-b03b-22143f840bec"
            },
            "text": "Next Sentence Prediction (NSP) Many downstream NLP tasks\nrequire understanding the relationship between two sentences, such as\nQuestion Answering (QA) and Natural Language Inference (NLI). Stan-\ndard language models do not pick up this type of knowledge. This moti-\nvatestheNSPtask,whereBERTisfedpairsofsentencesandpre-trained\nto predict if the second sentence should follow the ﬁrst one in a contin-\nuous context. As discussed earlier, the ﬁrst sentence is preﬁxed with the\n[CLS]token, then the two sentences are delimited by the special token\n[SEP]. During NSP task pre-training, the model is given sentence pairs\nwhere 50% of the time the second sentence comes after the ﬁrst sentence\nand the other 50% the second sentence is a random sentence from the\nfull training corpus. The self-attention of Transformer layers encourages"
          },
          {
            "meta": {
              "score": 0.7906167330118984,
              "Node id": "aaa3bfd9-a1e0-40d7-b46b-1394328f1147"
            },
            "text": "Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The consistent distance between two time-steps across sentences of various lengths is important for maintaining the integrity of the positional encoding in transformer models. This consistency ensures that the model can accurately capture the relative positions of words in a sentence, regardless of the sentence's length. \n\nIn natural language processing tasks, the position of a word in a sentence often carries important contextual information. For example, the word \"not\" can change the meaning of a sentence depending on where it is placed. If the distance between time-steps is not consistent, the model may misinterpret the positional relationship between words, leading to inaccurate predictions or interpretations.\n\nMoreover, maintaining a consistent distance between time-steps allows the model to generalize better to sentences of different lengths. This is because the positional encoding would remain consistent and meaningful across different sentence lengths, allowing the model to understand and process shorter or longer sentences effectively. \n\nIn summary, a consistent distance between two time-steps across sentences of various lengths is crucial for accurate positional encoding, effective model generalization, and overall performance in natural language processing tasks.",
              "prompt": {
                "meta": {},
                "user": "Explain the importance of consistent distance between two time-steps across sentences of various lengths.",
                "system": "Contexte :  One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective. \n\n Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. \n\n The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages. \n\n 84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM. \n\n The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights. \n\n With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column. \n\n Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD). \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d. \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT. \n\n As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n 124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges. \n\n Next Sentence Prediction (NSP) Many downstream NLP tasks\nrequire understanding the relationship between two sentences, such as\nQuestion Answering (QA) and Natural Language Inference (NLI). Stan-\ndard language models do not pick up this type of knowledge. This moti-\nvatestheNSPtask,whereBERTisfedpairsofsentencesandpre-trained\nto predict if the second sentence should follow the ﬁrst one in a contin-\nuous context. As discussed earlier, the ﬁrst sentence is preﬁxed with the\n[CLS]token, then the two sentences are delimited by the special token\n[SEP]. During NSP task pre-training, the model is given sentence pairs\nwhere 50% of the time the second sentence comes after the ﬁrst sentence\nand the other 50% the second sentence is a random sentence from the\nfull training corpus. The self-attention of Transformer layers encourages \n\n Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. \n\n \n La question est Explain the importance of consistent distance between two time-steps across sentences of various lengths."
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:25:16.734225",
              "duration": 12.886432,
              "cost": 0.12633
            },
            "meta": {},
            "text": "The consistent distance between two time-steps across sentences of various lengths is important for maintaining the integrity of the positional encoding in transformer models. This consistency ensures that the model can accurately capture the relative positions of words in a sentence, regardless of the sentence's length. \n\nIn natural language processing tasks, the position of a word in a sentence often carries important contextual information. For example, the word \"not\" can change the meaning of a sentence depending on where it is placed. If the distance between time-steps is not consistent, the model may misinterpret the positional relationship between words, leading to inaccurate predictions or interpretations.\n\nMoreover, maintaining a consistent distance between time-steps allows the model to generalize better to sentences of different lengths. This is because the positional encoding would remain consistent and meaningful across different sentence lengths, allowing the model to understand and process shorter or longer sentences effectively. \n\nIn summary, a consistent distance between two time-steps across sentences of various lengths is crucial for accurate positional encoding, effective model generalization, and overall performance in natural language processing tasks.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer.\n2. 2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase, quelle que soit sa longueur.\n3. 3. Dans les tâches de traitement du langage naturel, la position d'un mot dans une phrase porte souvent des informations contextuelles importantes.\n4. 4. Par exemple, le mot \"non\" peut changer le sens d'une phrase en fonction de son emplacement.\n5. 5. Si la distance entre les pas de temps n'est pas constante, le modèle peut mal interpréter la relation de position entre les mots, entraînant des prédictions ou interprétations inexactes.\n6. 6. De plus, maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs.\n7. 7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase, permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues.\n8. 8. En résumé, une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis, une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The consistent distance between two time-steps across sentences of various lengths is important for maintaining the integrity of the positional encoding in transformer models. This consistency ensures that the model can accurately capture the relative positions of words in a sentence, regardless of the sentence's length. \n\nIn natural language processing tasks, the position of a word in a sentence often carries important contextual information. For example, the word \"not\" can change the meaning of a sentence depending on where it is placed. If the distance between time-steps is not consistent, the model may misinterpret the positional relationship between words, leading to inaccurate predictions or interpretations.\n\nMoreover, maintaining a consistent distance between time-steps allows the model to generalize better to sentences of different lengths. This is because the positional encoding would remain consistent and meaningful across different sentence lengths, allowing the model to understand and process shorter or longer sentences effectively. \n\nIn summary, a consistent distance between two time-steps across sentences of various lengths is crucial for accurate positional encoding, effective model generalization, and overall performance in natural language processing tasks.\n\n1. 1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer.\n2. 2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase, quelle que soit sa longueur.\n3. 3. Dans les tâches de traitement du langage naturel, la position d'un mot dans une phrase porte souvent des informations contextuelles importantes.\n4. 4. Par exemple, le mot \"non\" peut changer le sens d'une phrase en fonction de son emplacement.\n5. 5. Si la distance entre les pas de temps n'est pas constante, le modèle peut mal interpréter la relation de position entre les mots, entraînant des prédictions ou interprétations inexactes.\n6. 6. De plus, maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs.\n7. 7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase, permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues.\n8. 8. En résumé, une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis, une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.528837",
                "duration": 5.392354,
                "cost": 0.0008635
              },
              "meta": {},
              "text": "Validated: [\"1. 1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer.\\n2. 2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase\", 'quelle que soit sa longueur.\\n3. 3. Dans les tâches de traitement du langage naturel', \"la position d'un mot dans une phrase porte souvent des informations contextuelles importantes.\\n4. 4. Par exemple\", 'le mot \"non\" peut changer le sens d\\'une phrase en fonction de son emplacement.\\n5. 5. Si la distance entre les pas de temps n\\'est pas constante', 'le modèle peut mal interpréter la relation de position entre les mots', 'entraînant des prédictions ou interprétations inexactes.\\n6. 6. De plus', 'maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs.\\n7. 7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase', 'permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues.\\n8. 8. En résumé', 'une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis', 'une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel.'] - Not validated: ['1', '2', '3', '4', '5', '6', '7', '8']",
              "human": 1.0,
              "auto": 1.25
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "Consistent distance between two time-steps across sentences of various lengths is important for several reasons:\n\n1. **Maintaining Contextual Relationships**: In natural language processing tasks, the distance between two time-steps (words in a sentence) can affect the contextual relationships between them. Consistent distance ensures that the model can capture and understand the relationships between words regardless of the sentence length.\n\n2. **Generalization**: By maintaining a consistent distance between time-steps, the model can generalize its learning across sentences of different lengths. This is crucial for the model to perform well on unseen data and tasks.\n\n3. **Information Flow**: Consistent distance helps in the smooth flow of information between time-steps. If the distance varies significantly across sentences, it can lead to information loss or confusion in the model's understanding of the text.\n\n4. **Model Stability**: Having a consistent distance between time-steps helps in stabilizing the model's training process. It ensures that the model learns to process and encode information in a consistent manner, leading to more stable and reliable predictions.\n\nOverall, consistent distance between time-steps is a key factor in ensuring that the model can effectively understand and process text data, regardless of the length or complexity of the sentences.",
              "prompt": {
                "meta": {},
                "user": "Explain the importance of consistent distance between two time-steps across sentences of various lengths.",
                "system": "Contexte :  One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective. \n\n Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. \n\n The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages. \n\n 84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM. \n\n The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights. \n\n With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column. \n\n Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD). \n\n 204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon? \n\n They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d. \n\n Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences. \n\n To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT. \n\n As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n 124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges. \n\n Next Sentence Prediction (NSP) Many downstream NLP tasks\nrequire understanding the relationship between two sentences, such as\nQuestion Answering (QA) and Natural Language Inference (NLI). Stan-\ndard language models do not pick up this type of knowledge. This moti-\nvatestheNSPtask,whereBERTisfedpairsofsentencesandpre-trained\nto predict if the second sentence should follow the ﬁrst one in a contin-\nuous context. As discussed earlier, the ﬁrst sentence is preﬁxed with the\n[CLS]token, then the two sentences are delimited by the special token\n[SEP]. During NSP task pre-training, the model is given sentence pairs\nwhere 50% of the time the second sentence comes after the ﬁrst sentence\nand the other 50% the second sentence is a random sentence from the\nfull training corpus. The self-attention of Transformer layers encourages \n\n Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. \n\n \n La question est Explain the importance of consistent distance between two time-steps across sentences of various lengths."
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:25:21.730568",
              "duration": 4.992496,
              "cost": 0.0022645
            },
            "meta": {},
            "text": "Consistent distance between two time-steps across sentences of various lengths is important for several reasons:\n\n1. **Maintaining Contextual Relationships**: In natural language processing tasks, the distance between two time-steps (words in a sentence) can affect the contextual relationships between them. Consistent distance ensures that the model can capture and understand the relationships between words regardless of the sentence length.\n\n2. **Generalization**: By maintaining a consistent distance between time-steps, the model can generalize its learning across sentences of different lengths. This is crucial for the model to perform well on unseen data and tasks.\n\n3. **Information Flow**: Consistent distance helps in the smooth flow of information between time-steps. If the distance varies significantly across sentences, it can lead to information loss or confusion in the model's understanding of the text.\n\n4. **Model Stability**: Having a consistent distance between time-steps helps in stabilizing the model's training process. It ensures that the model learns to process and encode information in a consistent manner, leading to more stable and reliable predictions.\n\nOverall, consistent distance between time-steps is a key factor in ensuring that the model can effectively understand and process text data, regardless of the length or complexity of the sentences.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1\n2\n3",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: Consistent distance between two time-steps across sentences of various lengths is important for several reasons:\n\n1. **Maintaining Contextual Relationships**: In natural language processing tasks, the distance between two time-steps (words in a sentence) can affect the contextual relationships between them. Consistent distance ensures that the model can capture and understand the relationships between words regardless of the sentence length.\n\n2. **Generalization**: By maintaining a consistent distance between time-steps, the model can generalize its learning across sentences of different lengths. This is crucial for the model to perform well on unseen data and tasks.\n\n3. **Information Flow**: Consistent distance helps in the smooth flow of information between time-steps. If the distance varies significantly across sentences, it can lead to information loss or confusion in the model's understanding of the text.\n\n4. **Model Stability**: Having a consistent distance between time-steps helps in stabilizing the model's training process. It ensures that the model learns to process and encode information in a consistent manner, leading to more stable and reliable predictions.\n\nOverall, consistent distance between time-steps is a key factor in ensuring that the model can effectively understand and process text data, regardless of the length or complexity of the sentences.\n\n1. 1. La distance constante entre deux pas de temps à travers des phrases de différentes longueurs est importante pour maintenir l'intégrité du codage de position dans les modèles de transformer.\n2. 2. Cette cohérence garantit que le modèle peut capturer avec précision les positions relatives des mots dans une phrase, quelle que soit sa longueur.\n3. 3. Dans les tâches de traitement du langage naturel, la position d'un mot dans une phrase porte souvent des informations contextuelles importantes.\n4. 4. Par exemple, le mot \"non\" peut changer le sens d'une phrase en fonction de son emplacement.\n5. 5. Si la distance entre les pas de temps n'est pas constante, le modèle peut mal interpréter la relation de position entre les mots, entraînant des prédictions ou interprétations inexactes.\n6. 6. De plus, maintenir une distance constante entre les pas de temps permet au modèle de mieux généraliser aux phrases de différentes longueurs.\n7. 7. Cela est dû au fait que le codage de position resterait cohérent et significatif quelle que soit la longueur de la phrase, permettant au modèle de comprendre et de traiter efficacement des phrases plus courtes ou plus longues.\n8. 8. En résumé, une distance constante entre deux pas de temps à travers des phrases de différentes longueurs est cruciale pour un codage de position précis, une généralisation efficace du modèle et des performances globales dans les tâches de traitement du langage naturel.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:32.929051",
                "duration": 0.725756,
                "cost": 0.000359
              },
              "meta": {},
              "text": "Validated: ['1\\n2\\n3'] - Not validated: ['1', '2', '3', '4', '5', '6', '7', '8']",
              "human": 1.0,
              "auto": 0.125
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Describe the computation process for each hidden unit in layer normalization."
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. En normalisation de couche, chaque unité cachée, notée hi, est calculée à l'aide de la formule hi = g * (hi - µ) / σ.\n2. Le paramètre g est la variable de gain qui peut être fixée à 1.\n3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées, représenté par H.\n4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne.\n5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1, réduisant le décalage de covariance, c'est-à-dire les dépendances de gradient entre chaque couche.\n6. Cela accélère la convergence car moins d'itérations sont nécessaires.",
          "prompt": {
            "meta": {},
            "user": "In layer normalization, each hidden unit, denoted as hi, is computed using the following formula:\n\nhi = g * (hi - µ) / σ\n\nHere, g is the gain variable which can be set to 1, µ is the mean of the hidden units, and σ is the standard deviation of the hidden units. The mean µ is calculated by summing all the hidden units and dividing by the total number of hidden units, represented as H. The standard deviation σ is calculated by taking the square root of the average of the squared deviations of the hidden units from their mean.\n\nThis computation ensures that each layer has a mean of 0 and a variance of 1, reducing the covariance shift, i.e., the gradient dependencies between each layer. This speeds up the convergence as fewer iterations are needed.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.854503",
          "duration": 7.494375,
          "cost": 0.000465
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. En normalisation de couche, chaque unité cachée, notée hi, est calculée à l'aide de la formule hi = g * (hi - µ) / σ."
          },
          {
            "meta": {},
            "text": "2. Le paramètre g est la variable de gain qui peut être fixée à 1."
          },
          {
            "meta": {},
            "text": "3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées, représenté par H."
          },
          {
            "meta": {},
            "text": "4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne."
          },
          {
            "meta": {},
            "text": "5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1, réduisant le décalage de covariance, c'est-à-dire les dépendances de gradient entre chaque couche."
          },
          {
            "meta": {},
            "text": "6. Cela accélère la convergence car moins d'itérations sont nécessaires."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.894689442284493,
              "Node id": "42603875-ed55-4bd4-a28e-0561e19696c9"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]."
          },
          {
            "meta": {
              "score": 11.805927276385447,
              "Node id": "29e99a45-e2fb-44d7-acdc-2a4e059b186b"
            },
            "text": "Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch."
          },
          {
            "meta": {
              "score": 11.219843598793993,
              "Node id": "88615c49-f37f-4a3b-b487-ea737f0c5aa1"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]."
          },
          {
            "meta": {
              "score": 11.150415383524095,
              "Node id": "d3ff4cb0-e8b6-43b0-95b0-f2712454f950"
            },
            "text": "For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers."
          },
          {
            "meta": {
              "score": 11.070592249153204,
              "Node id": "eeed8008-7c1d-4229-b168-1510628a3be8"
            },
            "text": "Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6."
          },
          {
            "meta": {
              "score": 10.940639948191937,
              "Node id": "f3c4218a-d4d3-448d-8a65-d3326120f4de"
            },
            "text": "˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3."
          },
          {
            "meta": {
              "score": 10.476838598983852,
              "Node id": "73353a5b-f1ab-48d4-a425-76e5821251ad"
            },
            "text": "maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2."
          },
          {
            "meta": {
              "score": 10.096178926222551,
              "Node id": "220fe6bb-2a69-4448-94db-8e250732feaf"
            },
            "text": "Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22)"
          },
          {
            "meta": {
              "score": 9.9471085667076,
              "Node id": "00b8b57c-feb3-46d9-866f-a74a3bd3ae92"
            },
            "text": "3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2."
          },
          {
            "meta": {
              "score": 9.811349069471598,
              "Node id": "81678cb8-fab6-467e-9077-98037c090d17"
            },
            "text": "3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6."
          },
          {
            "meta": {
              "score": 0.8212446676419476,
              "Node id": "b6c62e55-50ab-4289-a40a-2d5dd059bb1c"
            },
            "text": "32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4."
          },
          {
            "meta": {
              "score": 0.8207932496068016,
              "Node id": "b6acf5f6-6784-4264-9494-582ddd039cca"
            },
            "text": "[13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831."
          },
          {
            "meta": {
              "score": 0.8178942380187862,
              "Node id": "7fe31ed3-5465-46c9-82a2-f1f9cc1ea56d"
            },
            "text": "Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4)"
          },
          {
            "meta": {
              "score": 0.811845893022498,
              "Node id": "71afa33f-010b-4f74-ae2b-dfdfdea31289"
            },
            "text": "[13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "In layer normalization, each hidden unit, denoted as hi, is computed using the following formula:\n\nhi = g * (hi - µ) / σ\n\nHere, g is the gain variable which can be set to 1, µ is the mean of the hidden units, and σ is the standard deviation of the hidden units. The mean µ is calculated by summing all the hidden units and dividing by the total number of hidden units, represented as H. The standard deviation σ is calculated by taking the square root of the average of the squared deviations of the hidden units from their mean.\n\nThis computation ensures that each layer has a mean of 0 and a variance of 1, reducing the covariance shift, i.e., the gradient dependencies between each layer. This speeds up the convergence as fewer iterations are needed.",
              "prompt": {
                "meta": {},
                "user": "Describe the computation process for each hidden unit in layer normalization.",
                "system": "Contexte :  2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. \n\n Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch. \n\n 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]. \n\n For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. \n\n Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6. \n\n ˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3. \n\n maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2. \n\n Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22) \n\n 3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2. \n\n 3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6. \n\n 32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4. \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831. \n\n Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4) \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020). \n\n \n La question est Describe the computation process for each hidden unit in layer normalization."
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:25:34.241763",
              "duration": 12.506435,
              "cost": 0.10338
            },
            "meta": {},
            "text": "In layer normalization, each hidden unit, denoted as hi, is computed using the following formula:\n\nhi = g * (hi - µ) / σ\n\nHere, g is the gain variable which can be set to 1, µ is the mean of the hidden units, and σ is the standard deviation of the hidden units. The mean µ is calculated by summing all the hidden units and dividing by the total number of hidden units, represented as H. The standard deviation σ is calculated by taking the square root of the average of the squared deviations of the hidden units from their mean.\n\nThis computation ensures that each layer has a mean of 0 and a variance of 1, reducing the covariance shift, i.e., the gradient dependencies between each layer. This speeds up the convergence as fewer iterations are needed.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. En normalisation de couche, chaque unité cachée, notée hi, est calculée à l'aide de la formule hi = g * (hi - µ) / σ.\n2. 2. Le paramètre g est la variable de gain qui peut être fixée à 1.\n3. 3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées, représenté par H.\n4. 4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne.\n5. 5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1, réduisant le décalage de covariance, c'est-à-dire les dépendances de gradient entre chaque couche.\n6. 6. Cela accélère la convergence car moins d'itérations sont nécessaires.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In layer normalization, each hidden unit, denoted as hi, is computed using the following formula:\n\nhi = g * (hi - µ) / σ\n\nHere, g is the gain variable which can be set to 1, µ is the mean of the hidden units, and σ is the standard deviation of the hidden units. The mean µ is calculated by summing all the hidden units and dividing by the total number of hidden units, represented as H. The standard deviation σ is calculated by taking the square root of the average of the squared deviations of the hidden units from their mean.\n\nThis computation ensures that each layer has a mean of 0 and a variance of 1, reducing the covariance shift, i.e., the gradient dependencies between each layer. This speeds up the convergence as fewer iterations are needed.\n\n1. 1. En normalisation de couche, chaque unité cachée, notée hi, est calculée à l'aide de la formule hi = g * (hi - µ) / σ.\n2. 2. Le paramètre g est la variable de gain qui peut être fixée à 1.\n3. 3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées, représenté par H.\n4. 4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne.\n5. 5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1, réduisant le décalage de covariance, c'est-à-dire les dépendances de gradient entre chaque couche.\n6. 6. Cela accélère la convergence car moins d'itérations sont nécessaires.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.538100",
                "duration": 4.075737,
                "cost": 0.000603
              },
              "meta": {},
              "text": "Validated: ['1. 1. En normalisation de couche', 'chaque unité cachée', 'notée hi', \"est calculée à l'aide de la formule hi = g * (hi - µ) / σ.\\n2. 2. Le paramètre g est la variable de gain qui peut être fixée à 1.\\n3. 3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées\", \"représenté par H.\\n4. 4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne.\\n5. 5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1\", 'réduisant le décalage de covariance', \"c'est-à-dire les dépendances de gradient entre chaque couche.\\n6. 6. Cela accélère la convergence car moins d'itérations sont nécessaires.\"] - Not validated: ['1', '2', '3', '4', '5', '6']",
              "human": 1.0,
              "auto": 1.1666666666666667
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In layer normalization, the computation process for each hidden unit can be described as follows:\n\n1. Calculate the mean (µ) of all the hidden units in the layer:\n   µ = 1/H * Σ(i=1 to H) hi\n\n2. Calculate the standard deviation (σ) of the hidden units in the layer:\n   σ = √(1/H * Σ(i=1 to H) (hi - µ)^2)\n\n3. Compute the normalized value for each hidden unit (hi) using the following formula:\n   hi_normalized = g * (hi - µ) / σ\n\n   where:\n   - hi_normalized is the normalized value of the hidden unit hi\n   - g is the gain variable (usually set to 1)\n   - µ is the mean calculated in step 1\n   - σ is the standard deviation calculated in step 2\n\n4. The normalized values are then passed through an activation function (e.g., ReLU) to introduce non-linearity and enhance the representation of the hidden units.\n\n5. The normalized and activated hidden units are then combined with the original hidden units using a residual connection, and the resulting values are passed through another normalization step to ensure stability and faster convergence.\n\nOverall, layer normalization helps to reduce the covariance shift between layers, speeds up convergence by reducing gradient dependencies, and ensures that each layer has a mean of 0 and a unit variance.",
              "prompt": {
                "meta": {},
                "user": "Describe the computation process for each hidden unit in layer normalization.",
                "system": "Contexte :  2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. \n\n Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch. \n\n 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]. \n\n For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. \n\n Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6. \n\n ˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3. \n\n maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2. \n\n Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22) \n\n 3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2. \n\n 3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6. \n\n 32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4. \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831. \n\n Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4) \n\n [13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020). \n\n \n La question est Describe the computation process for each hidden unit in layer normalization."
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:25:39.925265",
              "duration": 5.680022,
              "cost": 0.0019909999999999997
            },
            "meta": {},
            "text": "In layer normalization, the computation process for each hidden unit can be described as follows:\n\n1. Calculate the mean (µ) of all the hidden units in the layer:\n   µ = 1/H * Σ(i=1 to H) hi\n\n2. Calculate the standard deviation (σ) of the hidden units in the layer:\n   σ = √(1/H * Σ(i=1 to H) (hi - µ)^2)\n\n3. Compute the normalized value for each hidden unit (hi) using the following formula:\n   hi_normalized = g * (hi - µ) / σ\n\n   where:\n   - hi_normalized is the normalized value of the hidden unit hi\n   - g is the gain variable (usually set to 1)\n   - µ is the mean calculated in step 1\n   - σ is the standard deviation calculated in step 2\n\n4. The normalized values are then passed through an activation function (e.g., ReLU) to introduce non-linearity and enhance the representation of the hidden units.\n\n5. The normalized and activated hidden units are then combined with the original hidden units using a residual connection, and the resulting values are passed through another normalization step to ensure stability and faster convergence.\n\nOverall, layer normalization helps to reduce the covariance shift between layers, speeds up convergence by reducing gradient dependencies, and ensures that each layer has a mean of 0 and a unit variance.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1\n2\n3\n5",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In layer normalization, the computation process for each hidden unit can be described as follows:\n\n1. Calculate the mean (µ) of all the hidden units in the layer:\n   µ = 1/H * Σ(i=1 to H) hi\n\n2. Calculate the standard deviation (σ) of the hidden units in the layer:\n   σ = √(1/H * Σ(i=1 to H) (hi - µ)^2)\n\n3. Compute the normalized value for each hidden unit (hi) using the following formula:\n   hi_normalized = g * (hi - µ) / σ\n\n   where:\n   - hi_normalized is the normalized value of the hidden unit hi\n   - g is the gain variable (usually set to 1)\n   - µ is the mean calculated in step 1\n   - σ is the standard deviation calculated in step 2\n\n4. The normalized values are then passed through an activation function (e.g., ReLU) to introduce non-linearity and enhance the representation of the hidden units.\n\n5. The normalized and activated hidden units are then combined with the original hidden units using a residual connection, and the resulting values are passed through another normalization step to ensure stability and faster convergence.\n\nOverall, layer normalization helps to reduce the covariance shift between layers, speeds up convergence by reducing gradient dependencies, and ensures that each layer has a mean of 0 and a unit variance.\n\n1. 1. En normalisation de couche, chaque unité cachée, notée hi, est calculée à l'aide de la formule hi = g * (hi - µ) / σ.\n2. 2. Le paramètre g est la variable de gain qui peut être fixée à 1.\n3. 3. La moyenne µ des unités cachées est calculée en additionnant toutes les unités cachées et en divisant par le nombre total d'unités cachées, représenté par H.\n4. 4. L'écart type σ est calculé en prenant la racine carrée de la moyenne des écarts au carré des unités cachées par rapport à leur moyenne.\n5. 5. Cette computation garantit que chaque couche a une moyenne de 0 et une variance de 1, réduisant le décalage de covariance, c'est-à-dire les dépendances de gradient entre chaque couche.\n6. 6. Cela accélère la convergence car moins d'itérations sont nécessaires.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:31.620507",
                "duration": 0.94486,
                "cost": 0.000323
              },
              "meta": {},
              "text": "Validated: ['1\\n2\\n3\\n5'] - Not validated: ['1', '2', '3', '4', '5', '6']",
              "human": 1.0,
              "auto": 0.16666666666666666
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il.\n2. Si Tjl est égal à 1, alors le calcul est basé sur une formule spécifique, sinon il est basé sur Ac_jl.\n3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés.\n4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention.\n5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH.\n6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs.\n7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs.\n8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence.",
          "prompt": {
            "meta": {},
            "user": "In the context of Transformer modifications, the attention with queries in a cluster is computed using the following formula:\n\nAt_il = { ˆmjexp(qiklT) / ∑L r=1Tjrexp(qikrT), if Tjl= 1, Ac_jl, otherwise (5.68)\n\nHere, ˆmj = ∑L i=1TijAc_ij and T ∈ {0,1}C×L. If Tij= 1, then ki is one of the top-k keys for in cluster j. \n\nThis formula is used in the context of clustered attention, a method to avoid self-attention’s O(L2·dk+ L2·dv) time complexity that linearizes the self-attention weight computation by clustering LSH hashed queries with the k-means clustering algorithm. The attention is computed for each of the top-k keys for a cluster with the queries in that cluster. The context vectors (weighted average of the values) of the clustered attention are then computed and used as the value matrix. This makes the complexity of the clustered attention calculation to O(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence length.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.862884",
          "duration": 7.830011,
          "cost": 0.0005065
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il."
          },
          {
            "meta": {},
            "text": "2. Si Tjl est égal à 1, alors le calcul est basé sur une formule spécifique, sinon il est basé sur Ac_jl."
          },
          {
            "meta": {},
            "text": "3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés."
          },
          {
            "meta": {},
            "text": "4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention."
          },
          {
            "meta": {},
            "text": "5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH."
          },
          {
            "meta": {},
            "text": "6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs."
          },
          {
            "meta": {},
            "text": "7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs."
          },
          {
            "meta": {},
            "text": "8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.575331584198825,
              "Node id": "36f3a298-c1cf-4a0d-90a5-77b25cbb4d18"
            },
            "text": "xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3."
          },
          {
            "meta": {
              "score": 12.549960937550692,
              "Node id": "23f67807-4b96-4ca7-8371-871e1f914657"
            },
            "text": "Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength."
          },
          {
            "meta": {
              "score": 10.688723477572681,
              "Node id": "48c273de-9a9e-45bf-9e21-cf08bf826bb8"
            },
            "text": "And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism."
          },
          {
            "meta": {
              "score": 10.681555878426526,
              "Node id": "90e8f749-838b-451d-a13a-ac93390bbad4"
            },
            "text": "The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length."
          },
          {
            "meta": {
              "score": 10.605552605416822,
              "Node id": "4f39317d-42da-424c-9d67-b7d18955e7a3"
            },
            "text": "5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3."
          },
          {
            "meta": {
              "score": 10.163372119062643,
              "Node id": "809d4326-8b22-4042-9f9e-868233ac8b04"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors."
          },
          {
            "meta": {
              "score": 10.103381782187256,
              "Node id": "0c37c0e5-cb03-4777-bba3-6e59042efb3c"
            },
            "text": "It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69)"
          },
          {
            "meta": {
              "score": 9.796252643393663,
              "Node id": "f29f7c5a-de71-4aa3-bca5-a44ef91809d6"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1."
          },
          {
            "meta": {
              "score": 9.77760490548345,
              "Node id": "8cf81edb-72e6-49af-9168-a748f923b19a"
            },
            "text": "The Longformer model,\ndiscussed in section 5.2.2.1, and Big Bird model, discussed in section\n5.2.2.4arebothexamplesofattentionwithpriors,sinceeachusesspeciﬁc\nattention patterns, like sliding window attention in sections 5.2.2.1 and\n5.2.2.4. We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages."
          },
          {
            "meta": {
              "score": 9.77760490548345,
              "Node id": "f4a73259-5653-412e-9661-46a07d807a8c"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance."
          },
          {
            "meta": {
              "score": 0.8702448508660161,
              "Node id": "dbef7c45-afcb-49c7-9727-5e32aa59f253"
            },
            "text": "2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space."
          },
          {
            "meta": {
              "score": 0.867720650285506,
              "Node id": "a4b31702-0072-4a00-9652-559ead64ddc6"
            },
            "text": "So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations."
          },
          {
            "meta": {
              "score": 0.8649182127027872,
              "Node id": "ce3bc842-f0b7-4b54-a931-df718c08a513"
            },
            "text": "Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing."
          },
          {
            "meta": {
              "score": 0.8620009796859537,
              "Node id": "3ea1c086-0d1f-412c-bf2d-ce4681401ba4"
            },
            "text": "K)V,∈RL1×dv×h(5.59)\nwhereαis the attention logits, A(Q, K)are the attention weights,\nandC(Q,K,V)is the “context” vector representing the output of the\nhattention heads prior to concatenation of the attention heads and the\nﬁnal projection layer.\nPartitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv."
          },
          {
            "meta": {
              "score": 0.8618002561275505,
              "Node id": "a3efb851-2ce8-4193-a5bf-2765bc4d69e6"
            },
            "text": "Partitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv.\nProjecting the attention logits Next, the attention logits αare\nprojected with a linear layer that mixes the query-key attention heads\nwith the attention logit/weight heads, Wα∈Rhk×h, and the attention"
          },
          {
            "meta": {
              "score": 0.8610330836831214,
              "Node id": "aa1d2f89-79fa-47b0-ab27-d008fbf1bfcd"
            },
            "text": "The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "In the context of Transformer modifications, the attention with queries in a cluster is computed using the following formula:\n\nAt_il = { ˆmjexp(qiklT) / ∑L r=1Tjrexp(qikrT), if Tjl= 1, Ac_jl, otherwise (5.68)\n\nHere, ˆmj = ∑L i=1TijAc_ij and T ∈ {0,1}C×L. If Tij= 1, then ki is one of the top-k keys for in cluster j. \n\nThis formula is used in the context of clustered attention, a method to avoid self-attention’s O(L2·dk+ L2·dv) time complexity that linearizes the self-attention weight computation by clustering LSH hashed queries with the k-means clustering algorithm. The attention is computed for each of the top-k keys for a cluster with the queries in that cluster. The context vectors (weighted average of the values) of the clustered attention are then computed and used as the value matrix. This makes the complexity of the clustered attention calculation to O(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence length.",
              "prompt": {
                "meta": {},
                "user": "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications.",
                "system": "Contexte :  xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3. \n\n Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength. \n\n And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. \n\n The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length. \n\n 5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. \n\n It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69) \n\n 4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n The Longformer model,\ndiscussed in section 5.2.2.1, and Big Bird model, discussed in section\n5.2.2.4arebothexamplesofattentionwithpriors,sinceeachusesspeciﬁc\nattention patterns, like sliding window attention in sections 5.2.2.1 and\n5.2.2.4. We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. \n\n 2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space. \n\n So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. \n\n Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing. \n\n K)V,∈RL1×dv×h(5.59)\nwhereαis the attention logits, A(Q, K)are the attention weights,\nandC(Q,K,V)is the “context” vector representing the output of the\nhattention heads prior to concatenation of the attention heads and the\nﬁnal projection layer.\nPartitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv. \n\n Partitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv.\nProjecting the attention logits Next, the attention logits αare\nprojected with a linear layer that mixes the query-key attention heads\nwith the attention logit/weight heads, Wα∈Rhk×h, and the attention \n\n The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs. \n\n \n La question est Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:25:53.390732",
              "duration": 13.460856,
              "cost": 0.12669
            },
            "meta": {},
            "text": "In the context of Transformer modifications, the attention with queries in a cluster is computed using the following formula:\n\nAt_il = { ˆmjexp(qiklT) / ∑L r=1Tjrexp(qikrT), if Tjl= 1, Ac_jl, otherwise (5.68)\n\nHere, ˆmj = ∑L i=1TijAc_ij and T ∈ {0,1}C×L. If Tij= 1, then ki is one of the top-k keys for in cluster j. \n\nThis formula is used in the context of clustered attention, a method to avoid self-attention’s O(L2·dk+ L2·dv) time complexity that linearizes the self-attention weight computation by clustering LSH hashed queries with the k-means clustering algorithm. The attention is computed for each of the top-k keys for a cluster with the queries in that cluster. The context vectors (weighted average of the values) of the clustered attention are then computed and used as the value matrix. This makes the complexity of the clustered attention calculation to O(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence length.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il.\n2. 2. Si Tjl est égal à 1, alors le calcul est basé sur une formule spécifique, sinon il est basé sur Ac_jl.\n3. 3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés.\n4. 4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention.\n5. 5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH.\n6. 6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs.\n7. 7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs.\n8. 8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the context of Transformer modifications, the attention with queries in a cluster is computed using the following formula:\n\nAt_il = { ˆmjexp(qiklT) / ∑L r=1Tjrexp(qikrT), if Tjl= 1, Ac_jl, otherwise (5.68)\n\nHere, ˆmj = ∑L i=1TijAc_ij and T ∈ {0,1}C×L. If Tij= 1, then ki is one of the top-k keys for in cluster j. \n\nThis formula is used in the context of clustered attention, a method to avoid self-attention’s O(L2·dk+ L2·dv) time complexity that linearizes the self-attention weight computation by clustering LSH hashed queries with the k-means clustering algorithm. The attention is computed for each of the top-k keys for a cluster with the queries in that cluster. The context vectors (weighted average of the values) of the clustered attention are then computed and used as the value matrix. This makes the complexity of the clustered attention calculation to O(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence length.\n\n1. 1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il.\n2. 2. Si Tjl est égal à 1, alors le calcul est basé sur une formule spécifique, sinon il est basé sur Ac_jl.\n3. 3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés.\n4. 4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention.\n5. 5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH.\n6. 6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs.\n7. 7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs.\n8. 8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.542533",
                "duration": 4.367195,
                "cost": 0.0006555
              },
              "meta": {},
              "text": "Validated: [\"1. 1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il.\\n2. 2. Si Tjl est égal à 1\", 'alors le calcul est basé sur une formule spécifique', \"sinon il est basé sur Ac_jl.\\n3. 3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés.\\n4. 4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention.\\n5. 5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH.\\n6. 6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs.\\n7. 7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs.\\n8. 8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence.\"] - Not validated: ['1', '2', '3', '4', '5', '6', '7', '8']",
              "human": 1.0,
              "auto": 0.375
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the context of Transformer modifications, the formula for computing the attention with queries in a cluster involves a process known as Clustered Attention. This method aims to linearize the self-attention weight computation to improve efficiency.\n\nHere is an overview of the formula and the steps involved in computing the attention with queries in a cluster:\n\n1. **Clustering Query Vectors**: \n   - Initially, each query vector is hashed using Locality-Sensitive Hashing (LSH).\n   - The hashed queries are then grouped into C clusters using the k-means clustering algorithm. The distance metric used for k-means is the Hamming distance.\n   - The centroid of the jth cluster is calculated using the formula: qcj = ∑i=1L Sij qi / ∑i=1L Sij, where S is a matrix partitioning the query vectors into C non-overlapping clusters.\n\n2. **Query Centroids and Clustered Attention Matrix**:\n   - The centroid queries are grouped into Qc, a matrix of centroid vectors.\n   - The real query matrix is substituted with the query centroid matrix, Qc, to compute the clustered attention matrix using the formula: Ac = softmax(QcKT / √(dk)), where dk is the dimension of the keys.\n\n3. **Output Computation**:\n   - The clustered attention weights obtained from the above calculation can be used to compute the output of the attention mechanism.\n   - The context vectors (weighted average of the values) of the clustered attention are used as the value matrix, denoted as V̂ = AtV.\n\n4. **Complexity**:\n   - The complexity of the clustered attention calculation is linear in the sequence length and is given by O(CL·dk + LC·dv + kL·max(dk, dv)).\n\nIn summary, the formula for computing the attention with queries in a cluster involves clustering the query vectors, calculating centroid queries, computing the clustered attention matrix, and using it to derive the output of the attention mechanism. This approach helps optimize the self-attention computation in Transformers for improved efficiency.",
              "prompt": {
                "meta": {},
                "user": "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications.",
                "system": "Contexte :  xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3. \n\n Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength. \n\n And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. \n\n The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length. \n\n 5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. \n\n It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69) \n\n 4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1. \n\n The Longformer model,\ndiscussed in section 5.2.2.1, and Big Bird model, discussed in section\n5.2.2.4arebothexamplesofattentionwithpriors,sinceeachusesspeciﬁc\nattention patterns, like sliding window attention in sections 5.2.2.1 and\n5.2.2.4. We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. \n\n We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. \n\n 2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space. \n\n So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. \n\n Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing. \n\n K)V,∈RL1×dv×h(5.59)\nwhereαis the attention logits, A(Q, K)are the attention weights,\nandC(Q,K,V)is the “context” vector representing the output of the\nhattention heads prior to concatenation of the attention heads and the\nﬁnal projection layer.\nPartitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv. \n\n Partitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv.\nProjecting the attention logits Next, the attention logits αare\nprojected with a linear layer that mixes the query-key attention heads\nwith the attention logit/weight heads, Wα∈Rhk×h, and the attention \n\n The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs. \n\n \n La question est Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:26:01.788483",
              "duration": 8.394632,
              "cost": 0.002496
            },
            "meta": {},
            "text": "In the context of Transformer modifications, the formula for computing the attention with queries in a cluster involves a process known as Clustered Attention. This method aims to linearize the self-attention weight computation to improve efficiency.\n\nHere is an overview of the formula and the steps involved in computing the attention with queries in a cluster:\n\n1. **Clustering Query Vectors**: \n   - Initially, each query vector is hashed using Locality-Sensitive Hashing (LSH).\n   - The hashed queries are then grouped into C clusters using the k-means clustering algorithm. The distance metric used for k-means is the Hamming distance.\n   - The centroid of the jth cluster is calculated using the formula: qcj = ∑i=1L Sij qi / ∑i=1L Sij, where S is a matrix partitioning the query vectors into C non-overlapping clusters.\n\n2. **Query Centroids and Clustered Attention Matrix**:\n   - The centroid queries are grouped into Qc, a matrix of centroid vectors.\n   - The real query matrix is substituted with the query centroid matrix, Qc, to compute the clustered attention matrix using the formula: Ac = softmax(QcKT / √(dk)), where dk is the dimension of the keys.\n\n3. **Output Computation**:\n   - The clustered attention weights obtained from the above calculation can be used to compute the output of the attention mechanism.\n   - The context vectors (weighted average of the values) of the clustered attention are used as the value matrix, denoted as V̂ = AtV.\n\n4. **Complexity**:\n   - The complexity of the clustered attention calculation is linear in the sequence length and is given by O(CL·dk + LC·dv + kL·max(dk, dv)).\n\nIn summary, the formula for computing the attention with queries in a cluster involves clustering the query vectors, calculating centroid queries, computing the clustered attention matrix, and using it to derive the output of the attention mechanism. This approach helps optimize the self-attention computation in Transformers for improved efficiency.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the context of Transformer modifications, the formula for computing the attention with queries in a cluster involves a process known as Clustered Attention. This method aims to linearize the self-attention weight computation to improve efficiency.\n\nHere is an overview of the formula and the steps involved in computing the attention with queries in a cluster:\n\n1. **Clustering Query Vectors**: \n   - Initially, each query vector is hashed using Locality-Sensitive Hashing (LSH).\n   - The hashed queries are then grouped into C clusters using the k-means clustering algorithm. The distance metric used for k-means is the Hamming distance.\n   - The centroid of the jth cluster is calculated using the formula: qcj = ∑i=1L Sij qi / ∑i=1L Sij, where S is a matrix partitioning the query vectors into C non-overlapping clusters.\n\n2. **Query Centroids and Clustered Attention Matrix**:\n   - The centroid queries are grouped into Qc, a matrix of centroid vectors.\n   - The real query matrix is substituted with the query centroid matrix, Qc, to compute the clustered attention matrix using the formula: Ac = softmax(QcKT / √(dk)), where dk is the dimension of the keys.\n\n3. **Output Computation**:\n   - The clustered attention weights obtained from the above calculation can be used to compute the output of the attention mechanism.\n   - The context vectors (weighted average of the values) of the clustered attention are used as the value matrix, denoted as V̂ = AtV.\n\n4. **Complexity**:\n   - The complexity of the clustered attention calculation is linear in the sequence length and is given by O(CL·dk + LC·dv + kL·max(dk, dv)).\n\nIn summary, the formula for computing the attention with queries in a cluster involves clustering the query vectors, calculating centroid queries, computing the clustered attention matrix, and using it to derive the output of the attention mechanism. This approach helps optimize the self-attention computation in Transformers for improved efficiency.\n\n1. 1. La formule pour calculer l'attention avec les requêtes dans un cluster est donnée par At_il.\n2. 2. Si Tjl est égal à 1, alors le calcul est basé sur une formule spécifique, sinon il est basé sur Ac_jl.\n3. 3. La formule implique l'utilisation de l'exponentielle des produits scalaires des requêtes avec les clés.\n4. 4. La méthode de l'attention groupée vise à réduire la complexité temporelle de l'auto-attention.\n5. 5. Elle utilise un algorithme de clustering k-means pour regrouper les requêtes hachées LSH.\n6. 6. Les vecteurs de contexte de l'attention groupée sont calculés comme une moyenne pondérée des valeurs.\n7. 7. Ces vecteurs de contexte sont ensuite utilisés comme matrice de valeurs.\n8. 8. La complexité de calcul de l'attention groupée est linéaire en fonction de la longueur de la séquence.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:31.918931",
                "duration": 0.591148,
                "cost": 0.00038449999999999997
              },
              "meta": {},
              "text": "Validated: ['1'] - Not validated: ['2', '3', '4', '5', '6', '7', '8']",
              "human": 1.0,
              "auto": 0.125
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement.\n2. La variable \"embedding\" représente la sortie du modèle, qui est la valeur prédite.\n3. C'est le résultat de la transformation des données d'entrée à travers le modèle.\n4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings).\n5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\n6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\n7. Plus la perte est faible, meilleures sont les prédictions du modèle.",
          "prompt": {
            "meta": {},
            "user": "The \"embedding\" and \"label\" variables in the compute_loss function are used to calculate the loss of the model during training. \n\nThe \"embedding\" variable represents the output of the model, which is the predicted value. It is the result of passing the input data through the model.\n\nThe \"label\" variable represents the actual or true value. It is the ground truth against which the model's predictions (embeddings) are compared.\n\nThe compute_loss function calculates the difference between the predicted values (embeddings) and the actual values (labels). This difference is the loss, which the model tries to minimize during training. The lower the loss, the better the model's predictions are.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.871038",
          "duration": 7.269023,
          "cost": 0.000401
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement."
          },
          {
            "meta": {},
            "text": "2. La variable \"embedding\" représente la sortie du modèle, qui est la valeur prédite."
          },
          {
            "meta": {},
            "text": "3. C'est le résultat de la transformation des données d'entrée à travers le modèle."
          },
          {
            "meta": {},
            "text": "4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings)."
          },
          {
            "meta": {},
            "text": "5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels)."
          },
          {
            "meta": {},
            "text": "6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement."
          },
          {
            "meta": {},
            "text": "7. Plus la perte est faible, meilleures sont les prédictions du modèle."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.301418302431337,
              "Node id": "7ee4658c-a143-4de9-8078-e7405518d914"
            },
            "text": "It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen-"
          },
          {
            "meta": {
              "score": 8.855261836837013,
              "Node id": "51dcc40d-cee3-4879-ad5e-11170b41c9f9"
            },
            "text": "introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model."
          },
          {
            "meta": {
              "score": 8.460994466625664,
              "Node id": "62167cd1-6e7e-45ef-acc3-ced424e81933"
            },
            "text": "int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself.val_acc = load_metric(\"accuracy\")\nself.test_acc = load_metric(\"accuracy\")\nself.hidden_dims = hidden_dims\nself."
          },
          {
            "meta": {
              "score": 8.298149546834619,
              "Node id": "cd48c63a-41a1-4d4b-851e-4faae43f1b65"
            },
            "text": "test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself."
          },
          {
            "meta": {
              "score": 8.193119930452319,
              "Node id": "e0b73afa-cb32-4ecd-9e73-d2385a331490"
            },
            "text": "Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,"
          },
          {
            "meta": {
              "score": 7.837219559631017,
              "Node id": "9e740d7e-564b-4fc1-aa6c-91378f334fd5"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3."
          },
          {
            "meta": {
              "score": 7.704405625003449,
              "Node id": "23ffb7e1-77af-44a8-95e2-8954bcdad405"
            },
            "text": "Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation."
          },
          {
            "meta": {
              "score": 7.608788988112662,
              "Node id": "d82acd08-b72f-4710-a9b8-8b9bfd6c55d3"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi."
          },
          {
            "meta": {
              "score": 7.589658902098048,
              "Node id": "6e9bdc67-bfb4-4ef6-9f55-c264870f2141"
            },
            "text": "The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks."
          },
          {
            "meta": {
              "score": 7.491095935564173,
              "Node id": "320e251f-3770-4a1b-9cd9-dddf6bfd69c3"
            },
            "text": "Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]."
          },
          {
            "meta": {
              "score": 0.7968698898480336,
              "Node id": "4ecc67e6-7882-485c-a941-8dc080d9e49e"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer."
          },
          {
            "meta": {
              "score": 0.7955607060498485,
              "Node id": "052b2829-cd0d-4168-afff-d19d077f7e0e"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers."
          },
          {
            "meta": {
              "score": 0.7954309202797077,
              "Node id": "e2e70800-ae76-45ed-9b95-67a9382671f4"
            },
            "text": "The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not."
          },
          {
            "meta": {
              "score": 0.7954287901425803,
              "Node id": "4e9aff89-8546-496e-b763-9ae3b6e5437f"
            },
            "text": "86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi."
          },
          {
            "meta": {
              "score": 0.7905206221475966,
              "Node id": "45e0b6ae-3c15-4e0c-a791-594dfc75f0b1"
            },
            "text": "Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C)."
          },
          {
            "meta": {
              "score": 0.789363757024328,
              "Node id": "7c326dc7-aa3b-425e-aa54-3168d380ce3d"
            },
            "text": "(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. This populates\nthe gradients back through the model to embedded inputs, showing the\nresulting distribution. Finally, we repeat this process for each generated\ntoken and visualize the resulting matrix."
          },
          {
            "meta": {
              "score": 0.7867543927520035,
              "Node id": "da38881c-51f6-4a28-a672-39fe549188bf"
            },
            "text": "found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token."
          },
          {
            "meta": {
              "score": 0.7851974496687317,
              "Node id": "0acc2499-1923-4bff-b291-0664b77d21cc"
            },
            "text": "Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block."
          },
          {
            "meta": {
              "score": 0.7841058371142566,
              "Node id": "353195f9-e021-4813-a3c5-32b17ab57167"
            },
            "text": "The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs."
          },
          {
            "meta": {
              "score": 0.7839676997004138,
              "Node id": "91c22950-6e38-4b59-859d-a11fef91eab3"
            },
            "text": "The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The \"embedding\" and \"label\" variables in the compute_loss function are used to calculate the loss of the model during training. \n\nThe \"embedding\" variable represents the output of the model, which is the predicted value. It is the result of passing the input data through the model.\n\nThe \"label\" variable represents the actual or true value. It is the ground truth against which the model's predictions (embeddings) are compared.\n\nThe compute_loss function calculates the difference between the predicted values (embeddings) and the actual values (labels). This difference is the loss, which the model tries to minimize during training. The lower the loss, the better the model's predictions are.",
              "prompt": {
                "meta": {},
                "user": "What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?",
                "system": "Contexte :  It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen- \n\n introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. \n\n int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself.val_acc = load_metric(\"accuracy\")\nself.test_acc = load_metric(\"accuracy\")\nself.hidden_dims = hidden_dims\nself. \n\n test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself. \n\n Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5, \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. \n\n Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi. \n\n The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks. \n\n Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers. \n\n The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not. \n\n 86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi. \n\n Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C). \n\n (a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. This populates\nthe gradients back through the model to embedded inputs, showing the\nresulting distribution. Finally, we repeat this process for each generated\ntoken and visualize the resulting matrix. \n\n found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token. \n\n Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. \n\n \n La question est What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:26:09.588907",
              "duration": 7.796176,
              "cost": 0.13662000000000002
            },
            "meta": {},
            "text": "The \"embedding\" and \"label\" variables in the compute_loss function are used to calculate the loss of the model during training. \n\nThe \"embedding\" variable represents the output of the model, which is the predicted value. It is the result of passing the input data through the model.\n\nThe \"label\" variable represents the actual or true value. It is the ground truth against which the model's predictions (embeddings) are compared.\n\nThe compute_loss function calculates the difference between the predicted values (embeddings) and the actual values (labels). This difference is the loss, which the model tries to minimize during training. The lower the loss, the better the model's predictions are.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement.\n2. 2. La variable \"embedding\" représente la sortie du modèle, qui est la valeur prédite.\n4. 4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings).\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\n7. 7. Plus la perte est faible, meilleures sont les prédictions du modèle.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The \"embedding\" and \"label\" variables in the compute_loss function are used to calculate the loss of the model during training. \n\nThe \"embedding\" variable represents the output of the model, which is the predicted value. It is the result of passing the input data through the model.\n\nThe \"label\" variable represents the actual or true value. It is the ground truth against which the model's predictions (embeddings) are compared.\n\nThe compute_loss function calculates the difference between the predicted values (embeddings) and the actual values (labels). This difference is the loss, which the model tries to minimize during training. The lower the loss, the better the model's predictions are.\n\n1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement.\n2. 2. La variable \"embedding\" représente la sortie du modèle, qui est la valeur prédite.\n3. 3. C'est le résultat de la transformation des données d'entrée à travers le modèle.\n4. 4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings).\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\n7. 7. Plus la perte est faible, meilleures sont les prédictions du modèle.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.547878",
                "duration": 3.83553,
                "cost": 0.000492
              },
              "meta": {},
              "text": "Validated: ['1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l\\'entraînement.\\n2. 2. La variable \"embedding\" représente la sortie du modèle', 'qui est la valeur prédite.\\n4. 4. La variable \"label\" représente la valeur réelle ou vraie', \"servant de référence pour comparer les prédictions du modèle (embeddings).\\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\\n7. 7. Plus la perte est faible\", 'meilleures sont les prédictions du modèle.'] - Not validated: ['1', '2', '3', '4', '5', '6', '7']",
              "human": 1.0,
              "auto": 0.5714285714285714
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "In the compute_loss function, the \"embedding\" variable represents the embeddings of the text examples, which are generated by the embed_text function. These embeddings capture the contextualized information of the text examples and are used as input to the model for training and prediction.\n\nOn the other hand, the \"label\" variable represents the labels associated with the text examples. These labels are converted from their original form to integer values using the label2int dictionary. The model uses these labels to calculate the loss during training, comparing the predicted outputs with the actual labels to update the model parameters through backpropagation.\n\nOverall, the \"embedding\" and \"label\" variables play crucial roles in the compute_loss function by providing the input embeddings and target labels necessary for training the model and computing the loss function.",
              "prompt": {
                "meta": {},
                "user": "What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?",
                "system": "Contexte :  It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen- \n\n introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. \n\n int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself.val_acc = load_metric(\"accuracy\")\nself.test_acc = load_metric(\"accuracy\")\nself.hidden_dims = hidden_dims\nself. \n\n test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself. \n\n Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5, \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. \n\n Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. \n\n For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi. \n\n The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks. \n\n Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer. \n\n 174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers. \n\n The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not. \n\n 86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi. \n\n Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C). \n\n (a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. This populates\nthe gradients back through the model to embedded inputs, showing the\nresulting distribution. Finally, we repeat this process for each generated\ntoken and visualize the resulting matrix. \n\n found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token. \n\n Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. \n\n The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. \n\n \n La question est What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:26:13.332806",
              "duration": 3.741821,
              "cost": 0.0023745
            },
            "meta": {},
            "text": "In the compute_loss function, the \"embedding\" variable represents the embeddings of the text examples, which are generated by the embed_text function. These embeddings capture the contextualized information of the text examples and are used as input to the model for training and prediction.\n\nOn the other hand, the \"label\" variable represents the labels associated with the text examples. These labels are converted from their original form to integer values using the label2int dictionary. The model uses these labels to calculate the loss during training, comparing the predicted outputs with the actual labels to update the model parameters through backpropagation.\n\nOverall, the \"embedding\" and \"label\" variables play crucial roles in the compute_loss function by providing the input embeddings and target labels necessary for training the model and computing the loss function.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement.\n4. 4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings).\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: In the compute_loss function, the \"embedding\" variable represents the embeddings of the text examples, which are generated by the embed_text function. These embeddings capture the contextualized information of the text examples and are used as input to the model for training and prediction.\n\nOn the other hand, the \"label\" variable represents the labels associated with the text examples. These labels are converted from their original form to integer values using the label2int dictionary. The model uses these labels to calculate the loss during training, comparing the predicted outputs with the actual labels to update the model parameters through backpropagation.\n\nOverall, the \"embedding\" and \"label\" variables play crucial roles in the compute_loss function by providing the input embeddings and target labels necessary for training the model and computing the loss function.\n\n1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l'entraînement.\n2. 2. La variable \"embedding\" représente la sortie du modèle, qui est la valeur prédite.\n3. 3. C'est le résultat de la transformation des données d'entrée à travers le modèle.\n4. 4. La variable \"label\" représente la valeur réelle ou vraie, servant de référence pour comparer les prédictions du modèle (embeddings).\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\n7. 7. Plus la perte est faible, meilleures sont les prédictions du modèle.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:31.394300",
                "duration": 2.613275,
                "cost": 0.00043099999999999996
              },
              "meta": {},
              "text": "Validated: ['1. 1. Les variables \"embedding\" et \"label\" dans la fonction compute_loss sont utilisées pour calculer la perte du modèle pendant l\\'entraînement.\\n4. 4. La variable \"label\" représente la valeur réelle ou vraie', \"servant de référence pour comparer les prédictions du modèle (embeddings).\\n5. 5. La fonction compute_loss calcule la différence entre les valeurs prédites (embeddings) et les valeurs réelles (labels).\\n6. 6. Cette différence est la perte que le modèle tente de minimiser pendant l'entraînement.\"] - Not validated: ['1', '2', '3', '4', '5', '6', '7']",
              "human": 1.0,
              "auto": 0.2857142857142857
            }
          }
        ]
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does the attention mechanism address the issue of long-distance associations in language processing?"
      },
      "facts": {
        "llm_answer": {
          "meta": {},
          "text": "1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents.\n2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur.\n3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée.\n4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage.\n5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN.",
          "prompt": {
            "meta": {},
            "user": "The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out the less relevant ones. It allows each time step in the decoder to be influenced differently by the hidden variables in the encoder. This means that the decoder can access information from any point in the input sequence, not just the last output of the encoder. This allows the model to maintain and utilize long-distance associations in the language, overcoming the limitations of traditional RNN-based encoder-decoder architectures.",
            "system": "Génère un minimum de phrases numérotées courtes et simples qui décrivent ce paragraphe.\n        Chaque phrase doit être indépendante et aucune phrase ne doit contenir la même information qu'une autre phrase.\n        Les phrases ne doivent pas contenir de référence au document source ni à sa page.\n        Les phrases doivent être compréhensibles seules et donc ne pas contenir de référence aux autres phrases ni nécessiter les autres phrases pour être comprises."
          },
          "name": "gpt-3.5-turbo",
          "full_name": "gpt-3.5-turbo-0125",
          "timestamp": "2024-05-09T02:21:03.881532",
          "duration": 8.572537,
          "cost": 0.000328
        },
        "meta": {},
        "items": [
          {
            "meta": {},
            "text": "1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents."
          },
          {
            "meta": {},
            "text": "2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur."
          },
          {
            "meta": {},
            "text": "3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée."
          },
          {
            "meta": {},
            "text": "4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage."
          },
          {
            "meta": {},
            "text": "5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN."
          }
        ]
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 15.088257255171966,
              "Node id": "0642bd7d-8458-42ad-8c5a-6fefaa7995af"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 14.217418862384083,
              "Node id": "71f6e6db-e039-46e5-b7f6-f5d815055c79"
            },
            "text": "Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies."
          },
          {
            "meta": {
              "score": 13.62478934034465,
              "Node id": "85bcde0e-c11c-41c6-9d15-37891db52a82"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 13.524611706449237,
              "Node id": "3a2c5014-8f05-4801-b4be-8e46a890b7f4"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 9.19032166547316,
              "Node id": "d50a0e58-fda2-4fc4-936c-25d15a81815b"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers."
          },
          {
            "meta": {
              "score": 9.097174963358942,
              "Node id": "2a7799e4-7cfe-440d-83e4-29cc5e0152d9"
            },
            "text": "Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig."
          },
          {
            "meta": {
              "score": 8.779105935768126,
              "Node id": "1b85a16b-98bb-47d2-8d63-93e6654fc76b"
            },
            "text": "Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts."
          },
          {
            "meta": {
              "score": 8.538131798058107,
              "Node id": "3152f463-2385-406b-b2ac-57c01d709e7a"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space."
          },
          {
            "meta": {
              "score": 8.496952258796714,
              "Node id": "86b6331f-7b20-46a1-af02-99b0657764d0"
            },
            "text": "For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig."
          },
          {
            "meta": {
              "score": 8.496952258796714,
              "Node id": "2133fdfd-a446-4a02-8e70-6ac58a825b02"
            },
            "text": "In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]."
          },
          {
            "meta": {
              "score": 0.8365384116275556,
              "Node id": "7388a63e-0f53-4627-aed4-3065b05fbaff"
            },
            "text": "Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations."
          },
          {
            "meta": {
              "score": 0.8324825023815255,
              "Node id": "15e2e361-2f42-4bed-b5c3-5a84cd6eb0b1"
            },
            "text": "The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]."
          },
          {
            "meta": {
              "score": 0.8280695759827673,
              "Node id": "9f31a899-bc47-4b62-b7a2-255a1124ddc9"
            },
            "text": "Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8)"
          },
          {
            "meta": {
              "score": 0.8249380841339582,
              "Node id": "46ea1f2d-0457-4dc9-b357-898ab0663a7f"
            },
            "text": "Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment."
          },
          {
            "meta": {
              "score": 0.8229529780045685,
              "Node id": "d3f527c5-3391-4393-a503-936de230a06a"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights."
          },
          {
            "meta": {
              "score": 0.822904227229679,
              "Node id": "7dcd214a-1856-4dbc-b1af-f6f31c8c3a64"
            },
            "text": "It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the \"heterogeneous\"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order."
          },
          {
            "meta": {
              "score": 0.8223131940073807,
              "Node id": "6ae11509-009f-423f-a3a1-4d8e649b3d90"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?"
          },
          {
            "meta": {
              "score": 0.8221930109187461,
              "Node id": "24a4d5c0-f074-4452-826d-38ce0ee55a4f"
            },
            "text": "Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues."
          },
          {
            "meta": {
              "score": 0.8212745372487479,
              "Node id": "67344d4a-fa90-42fd-a454-e900cf282307"
            },
            "text": "7.4 ATTENTION AND EXPLANATION\nAs discussed in the previous section, one of the emerging patterns, es-\npecially in NLP, is to associate the magnitude of the attention weights\nwith the inputs and use it to interpret the model behavior. Next, we dis-\ncuss few papers and the research that impacts how one views attention\nmechanisms and their contribution towards explainability.\n7.4.1 Attention is Not an Explanation\nIn this paper, Jain and Wallace try to ask fundamental questions on\nattention and their interpretations [132]. For example, when we create\nan attention map as shown in Fig. 7.4 that correlates attention weights\ndirectly to the input tokens or weights, the impact of many transforma-\ntions or computations such as intermediate hidden states, query vectors,\nattention techniques is not taken into account. The paper poses two cru-\ncial questions—(i) do the attention heat maps reveal the importance of\nwords/tokens?"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": [
          {
            "llm_answer": {
              "meta": {},
              "text": "The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out the less relevant ones. It allows each time step in the decoder to be influenced differently by the hidden variables in the encoder. This means that the decoder can access information from any point in the input sequence, not just the last output of the encoder. This allows the model to maintain and utilize long-distance associations in the language, overcoming the limitations of traditional RNN-based encoder-decoder architectures.",
              "prompt": {
                "meta": {},
                "user": "How does the attention mechanism address the issue of long-distance associations in language processing?",
                "system": "Contexte :  As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations. \n\n The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. \n\n Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8) \n\n Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the \"heterogeneous\"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n 7.4 ATTENTION AND EXPLANATION\nAs discussed in the previous section, one of the emerging patterns, es-\npecially in NLP, is to associate the magnitude of the attention weights\nwith the inputs and use it to interpret the model behavior. Next, we dis-\ncuss few papers and the research that impacts how one views attention\nmechanisms and their contribution towards explainability.\n7.4.1 Attention is Not an Explanation\nIn this paper, Jain and Wallace try to ask fundamental questions on\nattention and their interpretations [132]. For example, when we create\nan attention map as shown in Fig. 7.4 that correlates attention weights\ndirectly to the input tokens or weights, the impact of many transforma-\ntions or computations such as intermediate hidden states, query vectors,\nattention techniques is not taken into account. The paper poses two cru-\ncial questions—(i) do the attention heat maps reveal the importance of\nwords/tokens? \n\n \n La question est How does the attention mechanism address the issue of long-distance associations in language processing?"
              },
              "name": "gpt-4",
              "full_name": "gpt-4-0613",
              "timestamp": "2024-04-22T09:26:19.909896",
              "duration": 6.574717,
              "cost": 0.13014
            },
            "meta": {},
            "text": "The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out the less relevant ones. It allows each time step in the decoder to be influenced differently by the hidden variables in the encoder. This means that the decoder can access information from any point in the input sequence, not just the last output of the encoder. This allows the model to maintain and utilize long-distance associations in the language, overcoming the limitations of traditional RNN-based encoder-decoder architectures.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. 1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents.\n2. 2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur.\n3. 3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée.\n4. 4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage.\n5. 5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN.",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out the less relevant ones. It allows each time step in the decoder to be influenced differently by the hidden variables in the encoder. This means that the decoder can access information from any point in the input sequence, not just the last output of the encoder. This allows the model to maintain and utilize long-distance associations in the language, overcoming the limitations of traditional RNN-based encoder-decoder architectures.\n\n1. 1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents.\n2. 2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur.\n3. 3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée.\n4. 4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage.\n5. 5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:27.564581",
                "duration": 4.143403,
                "cost": 0.000425
              },
              "meta": {},
              "text": "Validated: [\"1. 1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents.\\n2. 2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur.\\n3. 3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée.\\n4. 4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage.\\n5. 5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN.\"] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          },
          {
            "llm_answer": {
              "meta": {},
              "text": "The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out less relevant ones. In traditional RNN-based encoder-decoder architectures, complete information about the source sentence is compressed and encoded in a single context variable used by the decoder. This compression can lead to a loss of information, especially when dealing with complex sentence structures and long-distance associations in language.\n\nWith the attention mechanism, each time step's hidden variables on the encoder side are available and carry information to be used by the decoder network. This means that each time step in the decoder can be influenced differently by the hidden variables in the encoder, allowing for a more nuanced understanding of the input sequence.\n\nBy selectively attending to different parts of the input sequence based on their relevance to the current decoding step, the attention mechanism enables the model to capture long-distance associations more effectively. This selective focus helps in addressing the inefficiencies of compressing complex language structures into a single vector and allows the model to better retain and utilize important information throughout the decoding process.",
              "prompt": {
                "meta": {},
                "user": "How does the attention mechanism address the issue of long-distance associations in language processing?",
                "system": "Contexte :  As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve \n\n Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. \n\n 2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize. \n\n The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. \n\n Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts. \n\n Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space. \n\n For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. \n\n In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]. \n\n Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations. \n\n The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. \n\n Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8) \n\n Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. \n\n It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the \"heterogeneous\"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order. \n\n The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks? \n\n Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues. \n\n 7.4 ATTENTION AND EXPLANATION\nAs discussed in the previous section, one of the emerging patterns, es-\npecially in NLP, is to associate the magnitude of the attention weights\nwith the inputs and use it to interpret the model behavior. Next, we dis-\ncuss few papers and the research that impacts how one views attention\nmechanisms and their contribution towards explainability.\n7.4.1 Attention is Not an Explanation\nIn this paper, Jain and Wallace try to ask fundamental questions on\nattention and their interpretations [132]. For example, when we create\nan attention map as shown in Fig. 7.4 that correlates attention weights\ndirectly to the input tokens or weights, the impact of many transforma-\ntions or computations such as intermediate hidden states, query vectors,\nattention techniques is not taken into account. The paper poses two cru-\ncial questions—(i) do the attention heat maps reveal the importance of\nwords/tokens? \n\n \n La question est How does the attention mechanism address the issue of long-distance associations in language processing?"
              },
              "name": "gpt-3.5-turbo",
              "full_name": "gpt-3.5-turbo-0125",
              "timestamp": "2024-04-22T09:26:25.013146",
              "duration": 5.101276,
              "cost": 0.0023825
            },
            "meta": {},
            "text": "The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out less relevant ones. In traditional RNN-based encoder-decoder architectures, complete information about the source sentence is compressed and encoded in a single context variable used by the decoder. This compression can lead to a loss of information, especially when dealing with complex sentence structures and long-distance associations in language.\n\nWith the attention mechanism, each time step's hidden variables on the encoder side are available and carry information to be used by the decoder network. This means that each time step in the decoder can be influenced differently by the hidden variables in the encoder, allowing for a more nuanced understanding of the input sequence.\n\nBy selectively attending to different parts of the input sequence based on their relevance to the current decoding step, the attention mechanism enables the model to capture long-distance associations more effectively. This selective focus helps in addressing the inefficiencies of compressing complex language structures into a single vector and allows the model to better retain and utilize important information throughout the decoding process.",
            "eval": {
              "llm_answer": {
                "meta": {},
                "text": "1. Oui\n2. Oui\n4. Oui\n5. Oui",
                "prompt": {
                  "meta": {},
                  "user": "Réponse: The attention mechanism addresses the issue of long-distance associations in language processing by selectively focusing on specific elements while filtering out less relevant ones. In traditional RNN-based encoder-decoder architectures, complete information about the source sentence is compressed and encoded in a single context variable used by the decoder. This compression can lead to a loss of information, especially when dealing with complex sentence structures and long-distance associations in language.\n\nWith the attention mechanism, each time step's hidden variables on the encoder side are available and carry information to be used by the decoder network. This means that each time step in the decoder can be influenced differently by the hidden variables in the encoder, allowing for a more nuanced understanding of the input sequence.\n\nBy selectively attending to different parts of the input sequence based on their relevance to the current decoding step, the attention mechanism enables the model to capture long-distance associations more effectively. This selective focus helps in addressing the inefficiencies of compressing complex language structures into a single vector and allows the model to better retain and utilize important information throughout the decoding process.\n\n1. 1. L'attention permet de se concentrer sélectivement sur des éléments spécifiques tout en filtrant les moins pertinents.\n2. 2. Chaque étape de temps dans le décodeur peut être influencée de manière différente par les variables cachées dans l'encodeur.\n3. 3. Le décodeur peut ainsi accéder à des informations provenant de n'importe quel point de la séquence d'entrée.\n4. 4. Cela permet au modèle de maintenir et d'utiliser des associations à longue distance dans le langage.\n5. 5. Cela surmonte les limitations des architectures encodeur-décodeur traditionnelles basées sur les RNN.",
                  "system": "Tu dois dire pour chaque fait numérotés 1, 2, 3...s'il est présent dans la Réponse.\n        Si le fait 1 est présent dans la réponse, renvoie 1. Si le fait 2 est présent dans la réponse, renvoie 2 etc...\n        Si le fait est vrai mais qu'il n'est pas présent dans la réponse, tu ne dois pas le renvoyer."
                },
                "name": "gpt-3.5-turbo",
                "full_name": "gpt-3.5-turbo-0125",
                "timestamp": "2024-05-09T02:22:31.718390",
                "duration": 0.8038009999999999,
                "cost": 0.000265
              },
              "meta": {},
              "text": "Validated: ['1. Oui\\n2. Oui\\n4. Oui\\n5. Oui'] - Not validated: ['1', '2', '3', '4', '5']",
              "human": 1.0,
              "auto": 0.2
            }
          }
        ]
      }
    }
  ]
}