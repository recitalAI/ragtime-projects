{
  "meta": {},
  "items": [
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 33.192873923114384,
              "Node id": "1ede6fba-99a5-4386-85d2-51195c2dbb8d"
            },
            "text": "Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered"
          },
          {
            "meta": {
              "score": 32.31338546294065,
              "Node id": "1c3c9ed7-2404-4905-89db-a2dd1b81257c"
            },
            "text": "LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]."
          },
          {
            "meta": {
              "score": 13.918003112997573,
              "Node id": "9b16b3d0-7c3a-4f0f-9119-673e5957766c"
            },
            "text": "The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al."
          },
          {
            "meta": {
              "score": 12.423515634033453,
              "Node id": "3764f50f-0b95-49f6-85ba-c1bf9134e147"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]."
          },
          {
            "meta": {
              "score": 12.178324830270618,
              "Node id": "fac4d19a-cd71-47be-a3fb-a9d1c2895013"
            },
            "text": "The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]."
          },
          {
            "meta": {
              "score": 10.797278377526439,
              "Node id": "41cc97ff-2ca6-44bd-aba4-e31b6e0305a9"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780."
          },
          {
            "meta": {
              "score": 10.724321494808025,
              "Node id": "b8d8db2a-d05d-49fb-a8b9-0900103cfe62"
            },
            "text": "Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116."
          },
          {
            "meta": {
              "score": 10.31485843860074,
              "Node id": "80615568-7cf7-4ce4-888e-0d03b43c5828"
            },
            "text": "4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications."
          },
          {
            "meta": {
              "score": 9.95443231280626,
              "Node id": "59efa2ae-3a60-4d21-8c8f-a15aa6764f5e"
            },
            "text": "Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its"
          },
          {
            "meta": {
              "score": 9.619967240570077,
              "Node id": "153fe2d1-6c12-4026-88c0-735ef783889b"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp."
          },
          {
            "meta": {
              "score": 0.8536065377099088,
              "Node id": "eb4074a6-b0df-4fc9-8b9f-491cf56609f6"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]."
          },
          {
            "meta": {
              "score": 0.8516590079316393,
              "Node id": "d90a8614-ed94-46ac-a778-0aae612acd06"
            },
            "text": "The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116]."
          },
          {
            "meta": {
              "score": 0.8516440071583733,
              "Node id": "51ef71e0-eac7-4397-8fb9-4f4a232f9f01"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 0.850891790726631,
              "Node id": "4a2257a1-9cb5-43a1-9c32-fcb20f82af58"
            },
            "text": "Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]."
          },
          {
            "meta": {
              "score": 0.8478846207766654,
              "Node id": "23ac04ee-7ae5-4eee-9605-635e4c25d813"
            },
            "text": "John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision."
          },
          {
            "meta": {
              "score": 0.8463647490109004,
              "Node id": "a5e43653-c66c-443c-aab7-ad3aaf0380d9"
            },
            "text": "In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.72677682330167,
              "Node id": "f29f7c5a-de71-4aa3-bca5-a44ef91809d6"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1."
          },
          {
            "meta": {
              "score": 13.706355088040688,
              "Node id": "8a7902a2-0e9b-4d82-848c-dfcbdf1729c1"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig."
          },
          {
            "meta": {
              "score": 13.628287519502955,
              "Node id": "13e8bdc9-f9a2-4bef-8b83-235ad11fede5"
            },
            "text": "2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5."
          },
          {
            "meta": {
              "score": 13.341388326569344,
              "Node id": "a9504729-676d-402d-b738-01fa06b9be62"
            },
            "text": "4.2 Data, Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5."
          },
          {
            "meta": {
              "score": 12.937946924929692,
              "Node id": "be86a5d7-7ee4-44c1-a235-ee953b3f835e"
            },
            "text": "1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2."
          },
          {
            "meta": {
              "score": 12.814474091335173,
              "Node id": "5f05066c-1caf-43c0-b1bb-9059e4b1f289"
            },
            "text": "122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention."
          },
          {
            "meta": {
              "score": 11.633077804218349,
              "Node id": "b870a302-5fa8-4f38-a305-2df7cd15f2d1"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24)."
          },
          {
            "meta": {
              "score": 11.48410504912319,
              "Node id": "cafb707f-a33e-481f-9a34-5c5bb3cef2de"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads."
          },
          {
            "meta": {
              "score": 11.394906470263596,
              "Node id": "824ddb89-1d89-4811-9486-bbd9ab4b1ccf"
            },
            "text": "Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two."
          },
          {
            "meta": {
              "score": 11.203583604034971,
              "Node id": "40fd90dc-32ff-4494-bf30-913a0d8653b3"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T."
          },
          {
            "meta": {
              "score": 0.8707926384987271,
              "Node id": "8ee90fff-a252-43fa-8d7a-f121bd1ddcad"
            },
            "text": "The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1."
          },
          {
            "meta": {
              "score": 0.8672500834344365,
              "Node id": "f8bac680-5751-42b4-85d2-e04f3a0abf90"
            },
            "text": "h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length."
          },
          {
            "meta": {
              "score": 0.8661099741373239,
              "Node id": "ac3cb608-b3d9-4ad8-9a90-ea4a47bfed3d"
            },
            "text": "The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6)"
          },
          {
            "meta": {
              "score": 0.8483832077236976,
              "Node id": "6b129587-0a89-4cad-8e8e-1e789c7f78fd"
            },
            "text": "self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1."
          },
          {
            "meta": {
              "score": 0.8440527473253598,
              "Node id": "96531517-9b4b-44b7-864a-918a49f6fad1"
            },
            "text": "Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings."
          },
          {
            "meta": {
              "score": 0.8427554618960998,
              "Node id": "614564f6-2dfd-497b-bf6a-784dd38bdfe1"
            },
            "text": "Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi."
          },
          {
            "meta": {
              "score": 0.8336442663122797,
              "Node id": "be802f8c-24b1-4524-bb05-768752c6e2f6"
            },
            "text": "As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 21.67859491404781,
              "Node id": "a845ca36-f23c-4b8b-a00d-161e5a6f8b38"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so."
          },
          {
            "meta": {
              "score": 20.213887337595324,
              "Node id": "56b627c1-09fd-43e5-b330-5e36e245c0c4"
            },
            "text": "When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake."
          },
          {
            "meta": {
              "score": 19.68196498115713,
              "Node id": "05641af3-836b-4a7a-a59b-ee02b3517149"
            },
            "text": "Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes."
          },
          {
            "meta": {
              "score": 18.46707611774621,
              "Node id": "6f2967ce-922d-4faa-acb0-d4f2a61ba1c3"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place ."
          },
          {
            "meta": {
              "score": 12.959481399310697,
              "Node id": "83ab4dc1-0fc2-4bf5-8788-7f5e2c9a6887"
            },
            "text": "Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 12.613909742935878,
              "Node id": "62e847ea-63e7-4e8c-ab4a-742d54966041"
            },
            "text": "\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well."
          },
          {
            "meta": {
              "score": 10.719691783146171,
              "Node id": "ce802fc5-45a3-4483-8760-a5ce8f584534"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants"
          },
          {
            "meta": {
              "score": 10.500479007799546,
              "Node id": "1b18a87e-ed0b-4489-bc45-ef4487815ed2"
            },
            "text": "Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out."
          },
          {
            "meta": {
              "score": 8.093326153317825,
              "Node id": "9787e40d-eb42-494b-a5fb-88c10053d0f5"
            },
            "text": "Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well."
          },
          {
            "meta": {
              "score": 7.878942352186918,
              "Node id": "c00306cb-9e1a-48b8-a3e8-e2f33f5cf745"
            },
            "text": "The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 0.7910490832512418,
              "Node id": "696503bd-002e-44df-9286-cf53514d4178"
            },
            "text": "\\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers."
          },
          {
            "meta": {
              "score": 0.790833628912363,
              "Node id": "a8680bb9-6b9c-4c88-a0c8-81403f9476c6"
            },
            "text": "I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly.\nThey kept the beer and wine flowing at our table at every\nvisit."
          },
          {
            "meta": {
              "score": 0.788961921515718,
              "Node id": "89d033f7-0bc4-4610-bc91-837b1d707407"
            },
            "text": "I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house."
          },
          {
            "meta": {
              "score": 0.7877402311259171,
              "Node id": "91650018-8067-4996-bf84-71d13a1a1316"
            },
            "text": "Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar."
          },
          {
            "meta": {
              "score": 0.787152620008759,
              "Node id": "1f63d7bd-2912-4039-9e75-8e3b268afabd"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither ."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How many patents does Kenneth L. Graham have related to natural language processing?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 17.63492779200936,
              "Node id": "0e39081e-4d01-48bf-b303-80e22fbed6ea"
            },
            "text": "He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry."
          },
          {
            "meta": {
              "score": 17.529961920015356,
              "Node id": "b3d10e8d-56ae-4f8d-8517-a2406c14d6ff"
            },
            "text": "Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii"
          },
          {
            "meta": {
              "score": 17.227868969332256,
              "Node id": "a9c276d3-5686-491d-8307-6e071876fb8d"
            },
            "text": "Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara"
          },
          {
            "meta": {
              "score": 16.506152704198495,
              "Node id": "f92b445b-5eb0-466d-8e25-faef2b186bf9"
            },
            "text": "Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr."
          },
          {
            "meta": {
              "score": 15.810136535838243,
              "Node id": "495dca8d-ee32-47a4-8066-9c73761b13ef"
            },
            "text": "He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling."
          },
          {
            "meta": {
              "score": 15.621927682657635,
              "Node id": "fc6213b4-f9ff-42ae-b0a5-550b62970007"
            },
            "text": "He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization."
          },
          {
            "meta": {
              "score": 13.450036307448087,
              "Node id": "a7be65c3-1b24-4bc4-ad36-e62ff8a1baa5"
            },
            "text": "To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara"
          },
          {
            "meta": {
              "score": 10.798034133567308,
              "Node id": "e16cd592-5179-4eb2-be0a-f747a951cfcf"
            },
            "text": "ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index."
          },
          {
            "meta": {
              "score": 9.956663296748097,
              "Node id": "1dbac9db-c1de-4e99-8b31-84c2944e8033"
            },
            "text": "Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529"
          },
          {
            "meta": {
              "score": 9.829018922265304,
              "Node id": "3f750bf2-4001-4654-9fe9-5e18d8c25033"
            },
            "text": "ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science)."
          },
          {
            "meta": {
              "score": 0.804144781716968,
              "Node id": "3764f50f-0b95-49f6-85ba-c1bf9134e147"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]."
          },
          {
            "meta": {
              "score": 0.8035498720326018,
              "Node id": "eb4074a6-b0df-4fc9-8b9f-491cf56609f6"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60]."
          },
          {
            "meta": {
              "score": 0.8008641036457432,
              "Node id": "51ef71e0-eac7-4397-8fb9-4f4a232f9f01"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 0.800118515793234,
              "Node id": "c2070aec-9728-40c8-8a8b-944b4b8fb29d"
            },
            "text": "[184]T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and\nS. Khudanpur ,Recurrent neural network based language model. ,\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017."
          },
          {
            "meta": {
              "score": 0.799116171875577,
              "Node id": "e35aee43-c372-4d24-ac21-c7bbe2a150a6"
            },
            "text": ",\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "In what scenarios is global attention beneficial in transformer models?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 7.752985807498689,
              "Node id": "42fc4ec3-e597-4a78-8a61-b70bb0496185"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?"
          },
          {
            "meta": {
              "score": 7.1114695913595805,
              "Node id": "b415d6c9-dcae-4920-8fe1-2191f55a4c35"
            },
            "text": "•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures. They are also rapidly employed in other applications such as\ncomputer vision and audio."
          },
          {
            "meta": {
              "score": 6.987768093224079,
              "Node id": "33d1ad41-d550-4a0c-a86e-5b8f25bdac58"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld."
          },
          {
            "meta": {
              "score": 6.914259124333967,
              "Node id": "f1e1385e-58c1-4f01-99b9-fe810ece5b9f"
            },
            "text": "•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld."
          },
          {
            "meta": {
              "score": 6.270287208150268,
              "Node id": "566c81c2-fd62-4011-9d9e-5e69665968e2"
            },
            "text": "Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 ."
          },
          {
            "meta": {
              "score": 6.06463537374279,
              "Node id": "2ba24716-60ba-450c-a149-454fffc2b197"
            },
            "text": "• ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction."
          },
          {
            "meta": {
              "score": 6.011153445665961,
              "Node id": "5067e6a6-4409-41d7-90eb-61fbf8203cee"
            },
            "text": "In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig."
          },
          {
            "meta": {
              "score": 5.888625552861361,
              "Node id": "74e3d63a-33c9-4899-ab82-25eea0b91fd0"
            },
            "text": "285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020)."
          },
          {
            "meta": {
              "score": 5.766390019131647,
              "Node id": "afe83c18-c464-43b4-aae9-c792f77294f8"
            },
            "text": "· · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions."
          },
          {
            "meta": {
              "score": 5.7195112006177835,
              "Node id": "15ad0c2f-8d48-47a4-b33c-934d20508fe1"
            },
            "text": "• · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G."
          },
          {
            "meta": {
              "score": 0.8545663019659953,
              "Node id": "9935e09f-48f6-4397-8a29-d6238ef14d5f"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier."
          },
          {
            "meta": {
              "score": 0.8508321905518534,
              "Node id": "e422aa6e-bf4d-4792-a11a-41e117f2c9e9"
            },
            "text": "3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights."
          },
          {
            "meta": {
              "score": 0.8496317358808926,
              "Node id": "4252db37-df5d-472e-8796-fdc98f630413"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention."
          },
          {
            "meta": {
              "score": 0.848532998629924,
              "Node id": "cc89dfa1-ae87-4de4-807f-ca80b3a22fad"
            },
            "text": "The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5."
          },
          {
            "meta": {
              "score": 0.8468740453352807,
              "Node id": "f0574687-24a9-47aa-ace7-cb8ad4737807"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals."
          },
          {
            "meta": {
              "score": 0.8466349950772598,
              "Node id": "c4f1af7e-3e97-4c63-a5a3-4c51de9ef39c"
            },
            "text": "Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion."
          },
          {
            "meta": {
              "score": 0.8433962149748828,
              "Node id": "2951269c-e690-4775-a98c-a06156800137"
            },
            "text": "208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent."
          },
          {
            "meta": {
              "score": 0.843218141686294,
              "Node id": "1b44970b-d6e3-4702-aee6-c366207a1ca7"
            },
            "text": "This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses. The Stanford Sentiment Treebank"
          },
          {
            "meta": {
              "score": 0.8428950217692884,
              "Node id": "e713e7e4-28c5-44e5-83d1-ab34c98770f4"
            },
            "text": "Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult."
          },
          {
            "meta": {
              "score": 0.8425468641952224,
              "Node id": "69d461e4-8491-45fd-9d34-34eb9fd921c1"
            },
            "text": "Furthermore,\nself-attention, the critical innovation in the transformers, helps in paral-\nlelizingthecomputationofper-symbolcontext-basedvectorsandcreates\na global receptive ﬁeld where the symbol gets information from all the\nsymbols. On the other hand, the absence of recurrent inductive bias of\nRNNs becomes an issue when solving tasks with inherent hierarchical\nstructures or when the lengths vary signiﬁcantly between the training\nand the unseen data the model predicts. Also, the number of sequen-\ntial computations in transformers is independent of the input size but\nonly dependent on the number of layers, making it computationally non-\nuniversal or Turing incomplete. Transformers apply the same amount of\ncomputation to all the inputs leading to ineﬃciencies in many cases\nwhere computations can be conditioned on the complexity.\nUniversaltransformers(UT)byDehghanietal.[69]isanextensionof\ntransformerswheretheparallelizabilityandglobalreceptiveﬁeldbeneﬁts\nget supplemented by the recurrent inductive bias of RNNs while being\ncomputationally universal."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.633073200985782,
              "Node id": "3b4fe435-db2b-4e63-a2e3-01a51991d8db"
            },
            "text": "One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective."
          },
          {
            "meta": {
              "score": 14.157910573717407,
              "Node id": "8914a6fc-05c7-48ea-8dd1-0e63cf8c79f5"
            },
            "text": "Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence."
          },
          {
            "meta": {
              "score": 12.99037835638875,
              "Node id": "110cdf47-4e8e-4436-bf47-cbb5740ffc7c"
            },
            "text": "The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages."
          },
          {
            "meta": {
              "score": 12.887587589012789,
              "Node id": "79158561-2dcf-4b2b-aed4-d794df305791"
            },
            "text": "84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM."
          },
          {
            "meta": {
              "score": 12.808474001416679,
              "Node id": "bd71f8f6-c904-40fe-bffb-032d4cd4ae85"
            },
            "text": "The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well."
          },
          {
            "meta": {
              "score": 12.53585035441094,
              "Node id": "85bcde0e-c11c-41c6-9d15-37891db52a82"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 12.427440302267886,
              "Node id": "ecfd5a0c-ae9d-43f1-b65f-a4a94e658a48"
            },
            "text": "The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights."
          },
          {
            "meta": {
              "score": 12.355960767022836,
              "Node id": "f5535185-9c30-4216-8da5-a820bfa7ce79"
            },
            "text": "With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column."
          },
          {
            "meta": {
              "score": 12.28304127645633,
              "Node id": "83564f46-55cf-46ea-8a73-d23d27ef0c3a"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD)."
          },
          {
            "meta": {
              "score": 12.094130687363325,
              "Node id": "eb120220-ab1b-4060-81f8-e90714516c7e"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          },
          {
            "meta": {
              "score": 0.8030843088666506,
              "Node id": "eeddff96-a911-4253-9c83-62b958e31732"
            },
            "text": "They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d."
          },
          {
            "meta": {
              "score": 0.801413571581106,
              "Node id": "fbdb3ee2-fe4d-4153-8c95-a41680c6d57a"
            },
            "text": "Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences."
          },
          {
            "meta": {
              "score": 0.7999707583572584,
              "Node id": "22d860c8-3a90-4d07-ab32-160e5f174c85"
            },
            "text": "To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT."
          },
          {
            "meta": {
              "score": 0.7968505870870304,
              "Node id": "0642bd7d-8458-42ad-8c5a-6fefaa7995af"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 0.7936029687045592,
              "Node id": "3a2c5014-8f05-4801-b4be-8e46a890b7f4"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 0.7918889589968077,
              "Node id": "9a97a3ca-f9f1-483b-820d-76753f4ea7d6"
            },
            "text": "124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges."
          },
          {
            "meta": {
              "score": 0.7917196288493183,
              "Node id": "22de9b71-c32b-4871-b03b-22143f840bec"
            },
            "text": "Next Sentence Prediction (NSP) Many downstream NLP tasks\nrequire understanding the relationship between two sentences, such as\nQuestion Answering (QA) and Natural Language Inference (NLI). Stan-\ndard language models do not pick up this type of knowledge. This moti-\nvatestheNSPtask,whereBERTisfedpairsofsentencesandpre-trained\nto predict if the second sentence should follow the ﬁrst one in a contin-\nuous context. As discussed earlier, the ﬁrst sentence is preﬁxed with the\n[CLS]token, then the two sentences are delimited by the special token\n[SEP]. During NSP task pre-training, the model is given sentence pairs\nwhere 50% of the time the second sentence comes after the ﬁrst sentence\nand the other 50% the second sentence is a random sentence from the\nfull training corpus. The self-attention of Transformer layers encourages"
          },
          {
            "meta": {
              "score": 0.7906167330118984,
              "Node id": "aaa3bfd9-a1e0-40d7-b46b-1394328f1147"
            },
            "text": "Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Describe the computation process for each hidden unit in layer normalization."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 11.894689442284493,
              "Node id": "42603875-ed55-4bd4-a28e-0561e19696c9"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]."
          },
          {
            "meta": {
              "score": 11.805927276385447,
              "Node id": "29e99a45-e2fb-44d7-acdc-2a4e059b186b"
            },
            "text": "Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch."
          },
          {
            "meta": {
              "score": 11.219843598793993,
              "Node id": "88615c49-f37f-4a3b-b487-ea737f0c5aa1"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]."
          },
          {
            "meta": {
              "score": 11.150415383524095,
              "Node id": "d3ff4cb0-e8b6-43b0-95b0-f2712454f950"
            },
            "text": "For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers."
          },
          {
            "meta": {
              "score": 11.070592249153204,
              "Node id": "eeed8008-7c1d-4229-b168-1510628a3be8"
            },
            "text": "Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6."
          },
          {
            "meta": {
              "score": 10.940639948191937,
              "Node id": "f3c4218a-d4d3-448d-8a65-d3326120f4de"
            },
            "text": "˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3."
          },
          {
            "meta": {
              "score": 10.476838598983852,
              "Node id": "73353a5b-f1ab-48d4-a425-76e5821251ad"
            },
            "text": "maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2."
          },
          {
            "meta": {
              "score": 10.096178926222551,
              "Node id": "220fe6bb-2a69-4448-94db-8e250732feaf"
            },
            "text": "Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22)"
          },
          {
            "meta": {
              "score": 9.9471085667076,
              "Node id": "00b8b57c-feb3-46d9-866f-a74a3bd3ae92"
            },
            "text": "3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2."
          },
          {
            "meta": {
              "score": 9.811349069471598,
              "Node id": "81678cb8-fab6-467e-9077-98037c090d17"
            },
            "text": "3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6.6.2.2 Graphs with edge attributes 175\n6."
          },
          {
            "meta": {
              "score": 0.8212446676419476,
              "Node id": "b6c62e55-50ab-4289-a40a-2d5dd059bb1c"
            },
            "text": "32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4."
          },
          {
            "meta": {
              "score": 0.8207932496068016,
              "Node id": "b6acf5f6-6784-4264-9494-582ddd039cca"
            },
            "text": "[13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831."
          },
          {
            "meta": {
              "score": 0.8178942380187862,
              "Node id": "7fe31ed3-5465-46c9-82a2-f1f9cc1ea56d"
            },
            "text": "Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4)"
          },
          {
            "meta": {
              "score": 0.811845893022498,
              "Node id": "71afa33f-010b-4f74-ae2b-dfdfdea31289"
            },
            "text": "[13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 12.575331584198825,
              "Node id": "36f3a298-c1cf-4a0d-90a5-77b25cbb4d18"
            },
            "text": "xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3."
          },
          {
            "meta": {
              "score": 12.549960937550692,
              "Node id": "23f67807-4b96-4ca7-8371-871e1f914657"
            },
            "text": "Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength."
          },
          {
            "meta": {
              "score": 10.688723477572681,
              "Node id": "48c273de-9a9e-45bf-9e21-cf08bf826bb8"
            },
            "text": "And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism."
          },
          {
            "meta": {
              "score": 10.681555878426526,
              "Node id": "90e8f749-838b-451d-a13a-ac93390bbad4"
            },
            "text": "The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length."
          },
          {
            "meta": {
              "score": 10.605552605416822,
              "Node id": "4f39317d-42da-424c-9d67-b7d18955e7a3"
            },
            "text": "5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3."
          },
          {
            "meta": {
              "score": 10.163372119062643,
              "Node id": "809d4326-8b22-4042-9f9e-868233ac8b04"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors."
          },
          {
            "meta": {
              "score": 10.103381782187256,
              "Node id": "0c37c0e5-cb03-4777-bba3-6e59042efb3c"
            },
            "text": "It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69)"
          },
          {
            "meta": {
              "score": 9.796252643393663,
              "Node id": "f29f7c5a-de71-4aa3-bca5-a44ef91809d6"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1."
          },
          {
            "meta": {
              "score": 9.77760490548345,
              "Node id": "8cf81edb-72e6-49af-9168-a748f923b19a"
            },
            "text": "The Longformer model,\ndiscussed in section 5.2.2.1, and Big Bird model, discussed in section\n5.2.2.4arebothexamplesofattentionwithpriors,sinceeachusesspeciﬁc\nattention patterns, like sliding window attention in sections 5.2.2.1 and\n5.2.2.4. We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages."
          },
          {
            "meta": {
              "score": 9.77760490548345,
              "Node id": "f4a73259-5653-412e-9661-46a07d807a8c"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance."
          },
          {
            "meta": {
              "score": 0.8702448508660161,
              "Node id": "dbef7c45-afcb-49c7-9727-5e32aa59f253"
            },
            "text": "2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space."
          },
          {
            "meta": {
              "score": 0.867720650285506,
              "Node id": "a4b31702-0072-4a00-9652-559ead64ddc6"
            },
            "text": "So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations."
          },
          {
            "meta": {
              "score": 0.8649182127027872,
              "Node id": "ce3bc842-f0b7-4b54-a931-df718c08a513"
            },
            "text": "Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing."
          },
          {
            "meta": {
              "score": 0.8620009796859537,
              "Node id": "3ea1c086-0d1f-412c-bf2d-ce4681401ba4"
            },
            "text": "K)V,∈RL1×dv×h(5.59)\nwhereαis the attention logits, A(Q, K)are the attention weights,\nandC(Q,K,V)is the “context” vector representing the output of the\nhattention heads prior to concatenation of the attention heads and the\nﬁnal projection layer.\nPartitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv."
          },
          {
            "meta": {
              "score": 0.8618002561275505,
              "Node id": "a3efb851-2ce8-4193-a5bf-2765bc4d69e6"
            },
            "text": "Partitioning the attention heads THA modiﬁes the attention\nmechanism in a few ways from that shown in (5.56)–(5.59). First, it\nchanges the attention head dimension of QandKto be the number\nof query-key attention head hk, and changes the attention head dimen-\nsion of Vto be the number of value attention heads hv. This happens\nby changing the dimension of the projection matrices that generate the\nquery, key, and value matrices from the input sequences. In other words,\n(5.56) becomes\nQ=X1Wq,∈RL1×dk×hk\nK=X2Wk,∈RL2×dk×hk\nV=X2Wv,∈RL2×dv×hv(5.60)\nwhere Wq,Wk∈Rd×dk×hk, and Wv∈Rd×dv×hv.\nProjecting the attention logits Next, the attention logits αare\nprojected with a linear layer that mixes the query-key attention heads\nwith the attention logit/weight heads, Wα∈Rhk×h, and the attention"
          },
          {
            "meta": {
              "score": 0.8610330836831214,
              "Node id": "aa1d2f89-79fa-47b0-ab27-d008fbf1bfcd"
            },
            "text": "The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.301418302431337,
              "Node id": "7ee4658c-a143-4de9-8078-e7405518d914"
            },
            "text": "It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen-"
          },
          {
            "meta": {
              "score": 8.855261836837013,
              "Node id": "51dcc40d-cee3-4879-ad5e-11170b41c9f9"
            },
            "text": "introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model."
          },
          {
            "meta": {
              "score": 8.460994466625664,
              "Node id": "62167cd1-6e7e-45ef-acc3-ced424e81933"
            },
            "text": "int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself.val_acc = load_metric(\"accuracy\")\nself.test_acc = load_metric(\"accuracy\")\nself.hidden_dims = hidden_dims\nself."
          },
          {
            "meta": {
              "score": 8.298149546834619,
              "Node id": "cd48c63a-41a1-4d4b-851e-4faae43f1b65"
            },
            "text": "test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself."
          },
          {
            "meta": {
              "score": 8.193119930452319,
              "Node id": "e0b73afa-cb32-4ecd-9e73-d2385a331490"
            },
            "text": "Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,"
          },
          {
            "meta": {
              "score": 7.837219559631017,
              "Node id": "9e740d7e-564b-4fc1-aa6c-91378f334fd5"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3."
          },
          {
            "meta": {
              "score": 7.704405625003449,
              "Node id": "23ffb7e1-77af-44a8-95e2-8954bcdad405"
            },
            "text": "Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation."
          },
          {
            "meta": {
              "score": 7.608788988112662,
              "Node id": "d82acd08-b72f-4710-a9b8-8b9bfd6c55d3"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi."
          },
          {
            "meta": {
              "score": 7.589658902098048,
              "Node id": "6e9bdc67-bfb4-4ef6-9f55-c264870f2141"
            },
            "text": "The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks."
          },
          {
            "meta": {
              "score": 7.491095935564173,
              "Node id": "320e251f-3770-4a1b-9cd9-dddf6bfd69c3"
            },
            "text": "Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]."
          },
          {
            "meta": {
              "score": 0.7968698898480336,
              "Node id": "4ecc67e6-7882-485c-a941-8dc080d9e49e"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer."
          },
          {
            "meta": {
              "score": 0.7955607060498485,
              "Node id": "052b2829-cd0d-4168-afff-d19d077f7e0e"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers."
          },
          {
            "meta": {
              "score": 0.7954309202797077,
              "Node id": "e2e70800-ae76-45ed-9b95-67a9382671f4"
            },
            "text": "The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not."
          },
          {
            "meta": {
              "score": 0.7954287901425803,
              "Node id": "4e9aff89-8546-496e-b763-9ae3b6e5437f"
            },
            "text": "86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi."
          },
          {
            "meta": {
              "score": 0.7905206221475966,
              "Node id": "45e0b6ae-3c15-4e0c-a791-594dfc75f0b1"
            },
            "text": "Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C)."
          },
          {
            "meta": {
              "score": 0.789363757024328,
              "Node id": "7c326dc7-aa3b-425e-aa54-3168d380ce3d"
            },
            "text": "(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value. This populates\nthe gradients back through the model to embedded inputs, showing the\nresulting distribution. Finally, we repeat this process for each generated\ntoken and visualize the resulting matrix."
          },
          {
            "meta": {
              "score": 0.7867543927520035,
              "Node id": "da38881c-51f6-4a28-a672-39fe549188bf"
            },
            "text": "found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token."
          },
          {
            "meta": {
              "score": 0.7851974496687317,
              "Node id": "0acc2499-1923-4bff-b291-0664b77d21cc"
            },
            "text": "Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block."
          },
          {
            "meta": {
              "score": 0.7841058371142566,
              "Node id": "353195f9-e021-4813-a3c5-32b17ab57167"
            },
            "text": "The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs."
          },
          {
            "meta": {
              "score": 0.7839676997004138,
              "Node id": "91c22950-6e38-4b59-859d-a11fef91eab3"
            },
            "text": "The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does the attention mechanism address the issue of long-distance associations in language processing?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 15.088257255171966,
              "Node id": "0642bd7d-8458-42ad-8c5a-6fefaa7995af"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 14.217418862384083,
              "Node id": "71f6e6db-e039-46e5-b7f6-f5d815055c79"
            },
            "text": "Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies."
          },
          {
            "meta": {
              "score": 13.62478934034465,
              "Node id": "85bcde0e-c11c-41c6-9d15-37891db52a82"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 13.524611706449237,
              "Node id": "3a2c5014-8f05-4801-b4be-8e46a890b7f4"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 9.19032166547316,
              "Node id": "d50a0e58-fda2-4fc4-936c-25d15a81815b"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers."
          },
          {
            "meta": {
              "score": 9.097174963358942,
              "Node id": "2a7799e4-7cfe-440d-83e4-29cc5e0152d9"
            },
            "text": "Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig."
          },
          {
            "meta": {
              "score": 8.779105935768126,
              "Node id": "1b85a16b-98bb-47d2-8d63-93e6654fc76b"
            },
            "text": "Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts."
          },
          {
            "meta": {
              "score": 8.538131798058107,
              "Node id": "3152f463-2385-406b-b2ac-57c01d709e7a"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space."
          },
          {
            "meta": {
              "score": 8.496952258796714,
              "Node id": "86b6331f-7b20-46a1-af02-99b0657764d0"
            },
            "text": "For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig."
          },
          {
            "meta": {
              "score": 8.496952258796714,
              "Node id": "2133fdfd-a446-4a02-8e70-6ac58a825b02"
            },
            "text": "In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]."
          },
          {
            "meta": {
              "score": 0.8365384116275556,
              "Node id": "7388a63e-0f53-4627-aed4-3065b05fbaff"
            },
            "text": "Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations."
          },
          {
            "meta": {
              "score": 0.8324825023815255,
              "Node id": "15e2e361-2f42-4bed-b5c3-5a84cd6eb0b1"
            },
            "text": "The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]."
          },
          {
            "meta": {
              "score": 0.8280695759827673,
              "Node id": "9f31a899-bc47-4b62-b7a2-255a1124ddc9"
            },
            "text": "Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8)"
          },
          {
            "meta": {
              "score": 0.8249380841339582,
              "Node id": "46ea1f2d-0457-4dc9-b357-898ab0663a7f"
            },
            "text": "Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment."
          },
          {
            "meta": {
              "score": 0.8229529780045685,
              "Node id": "d3f527c5-3391-4393-a503-936de230a06a"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights."
          },
          {
            "meta": {
              "score": 0.822904227229679,
              "Node id": "7dcd214a-1856-4dbc-b1af-f6f31c8c3a64"
            },
            "text": "It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the \"heterogeneous\"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order."
          },
          {
            "meta": {
              "score": 0.8223131940073807,
              "Node id": "6ae11509-009f-423f-a3a1-4d8e649b3d90"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?"
          },
          {
            "meta": {
              "score": 0.8221930109187461,
              "Node id": "24a4d5c0-f074-4452-826d-38ce0ee55a4f"
            },
            "text": "Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues."
          },
          {
            "meta": {
              "score": 0.8212745372487479,
              "Node id": "67344d4a-fa90-42fd-a454-e900cf282307"
            },
            "text": "7.4 ATTENTION AND EXPLANATION\nAs discussed in the previous section, one of the emerging patterns, es-\npecially in NLP, is to associate the magnitude of the attention weights\nwith the inputs and use it to interpret the model behavior. Next, we dis-\ncuss few papers and the research that impacts how one views attention\nmechanisms and their contribution towards explainability.\n7.4.1 Attention is Not an Explanation\nIn this paper, Jain and Wallace try to ask fundamental questions on\nattention and their interpretations [132]. For example, when we create\nan attention map as shown in Fig. 7.4 that correlates attention weights\ndirectly to the input tokens or weights, the impact of many transforma-\ntions or computations such as intermediate hidden states, query vectors,\nattention techniques is not taken into account. The paper poses two cru-\ncial questions—(i) do the attention heat maps reveal the importance of\nwords/tokens?"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    }
  ]
}