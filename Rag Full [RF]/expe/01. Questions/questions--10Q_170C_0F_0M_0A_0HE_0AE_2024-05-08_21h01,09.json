{
  "meta": {},
  "items": [
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What breakthrough paper was published by Hinton et al. in 2006, leading to the resurgence of deep learning?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 0.849446624783471,
              "Node id": "87d57433-0a01-4af3-b8df-b066ce9c1227",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "2"
            },
            "text": "John Hopﬁeld introduced “Hopﬁeld Networks”, one of the ﬁrst recur-\nrentneuralnetworks(RNNs)thatserveasacontent-addressablememory\nsystem [117].\nIn 1986, David Rumelhart, Geoﬀ Hinton, and Ronald Williams pub-\nlished the seminal work “Learning representations by back-propagating\nerrors” [217]. Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision."
          },
          {
            "meta": {
              "score": 0.8406410249743105,
              "Node id": "01ffa358-cd9e-44ce-bd87-d7ed74fd464e",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "2"
            },
            "text": "Their work conﬁrms how a multi-layered neural network\nusing many “hidden” layers can overcome the weakness of perceptrons\nin learning complex patterns with relatively simple training procedures.\nThe building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115]."
          },
          {
            "meta": {
              "score": 10.033043559478116,
              "Node id": "d175934b-c676-414d-b41a-1cafd07699b7",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "2"
            },
            "text": "The building blocks for this work had been laid down by various research\nover the years by S. Linnainmaa, P. Werbos, K. Fukushima, D. Parker,\nand Y. LeCun [164, 267,91,196,149].\nLeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al."
          },
          {
            "meta": {
              "score": 32.20370645157876,
              "Node id": "8ff44c56-a380-4d93-a2f8-1828a0cb120a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "2"
            },
            "text": "LeCun et al., through their research and implementation, led to the\nﬁrst widespread application of neural networks to recognize the hand-\nwrittendigitsusedbytheU.S.PostalService[150].Thisworkisacritical\nmilestone in deep learning history, proving the utility of convolution op-\nerations and weight sharing in learning the features in computer vision.\nBackpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]."
          },
          {
            "meta": {
              "score": 33.06928751560403,
              "Node id": "eb719f5e-ce96-4c5c-a304-5c88eeb60a60",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "2"
            },
            "text": "Backpropagation, the key optimization technique, encountered a\nnumber of issues such as vanishing gradients, exploding gradients, and\nthe inability to learn long-term information, to name a few [115].\nHochreiter and Schmidhuber, in their work,“Long short-term memory\n(LSTM)” architecture, demonstrated how issues with long-term depen-\ndenciescouldovercomeshortcomingsofbackpropagationovertime[116].\nHinton et al. published a breakthrough paper in 2006 titled “A fast\nlearning algorithm for deep belief nets”; it was one of the reasons for the\nresurgence of deep learning [113]. The research highlighted the eﬀective-\nness of layer-by-layer training using unsupervised methods followed by\nsupervised “ﬁne-tuning” to achieve state-of-the-art results in character\nrecognition. Bengio et al., in their seminal work following this, oﬀered"
          },
          {
            "meta": {
              "score": 0.8435697452576497,
              "Node id": "94305823-5ca7-481b-a20c-8d71d2ac1876",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "In their research, Bengio and LeCun emphasized the advantages of deep\nlearning through architectures such as convolutional neural networks\n(CNNs), restricted Boltzmann machines (RBMs), and deep belief net-\nworks(DBNs),andthroughtechniquessuchasunsupervisedpre-training\nwith ﬁne-tuning, thus inspiring the next wave of deep learning [28]. Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld."
          },
          {
            "meta": {
              "score": 0.8508451460962845,
              "Node id": "a713adb2-97f2-4269-bdf0-17481c7ad077",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Fei-\nFei Li, head of the artiﬁcial intelligence lab at Stanford University, along\nwith other researchers, launched ImageNet, which resulted in the most\nextensive collection of images and, for the ﬁrst time, highlighted the\nusefulness of data in learning essential tasks such as object recognition,\nclassiﬁcation, and clustering [70]. Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]."
          },
          {
            "meta": {
              "score": 0.8498259879778571,
              "Node id": "ca8e72a2-d0b8-4fe7-9338-d2dc10d670dc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 12.411095202864262,
              "Node id": "18e70a9c-f0fa-480f-81dc-14970ce023cc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al."
          },
          {
            "meta": {
              "score": 12.169105354847211,
              "Node id": "807d3b63-4b57-49dd-8e34-0b1cbeb95314",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]."
          },
          {
            "meta": {
              "score": 9.93723617444747,
              "Node id": "9fd1a20a-7259-4aa4-ae25-806af4adc1a7",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Sutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al. in-\ntroduced sequence-to-sequence learning as a generic neural framework\ncomprised of an encoder neural network processing inputs as a sequence\nand a decoder neural network predicting the outputs based on the in-\nput sequence states and the current output states [238]. As a result, the\nsequence-to-sequence framework became the core architecture for a wide\nrange of NLP tasks such as constituency parsing, named entity recogni-\ntion (NER), machine translation, question-answering, and summariza-\ntion, to name a few. Furthermore, even Google started replacing its"
          },
          {
            "meta": {
              "score": 10.245272152970955,
              "Node id": "ad6bf5b4-34ad-491b-a918-606969313a77",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "4"
            },
            "text": "4■Transformers for Machine Learning: A Deep Dive\nmonolithic phrase-based machine translation models with sequence-to-\nsequence neural machine translation models [272]. To overcome the bot-\ntleneck issues with the sequence-to-sequence framework, seminal work\nby Bahdanau et al. proposed the attention mechanism, which plays a\ncrucial role in transformers and their variants [17].\n1.2 TRANSFORMERS AND TAXONOMY\nThe transformer architecture [254] was introduced in 2017, in the paper\nAttention Is All You Need , for sequence-to-sequence problems. It was\nan alternative to using recurrent or convolutional layers. Since its in-\ntroduction, there’s been a wide variety of research into various ways to\nimproveuponthestandardtransformer.Twosurveys[163, 243]havecat-\negorized transformer-related papers. Transformer research has focused\non three things: architecture modiﬁcation, pre-training methods, and\napplications."
          },
          {
            "meta": {
              "score": 10.616871736097117,
              "Node id": "b78a062a-fd73-44a9-9e04-fd105a05a048",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "233"
            },
            "text": "Bibliography ■233\n[112]G. Hinton, O. Vinyals, and J. Dean ,Distilling the knowledge\nin a neural network , arXiv preprint arXiv:1503.02531, (2015).\n[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116."
          },
          {
            "meta": {
              "score": 10.688191189162236,
              "Node id": "304b1640-d8c3-4b21-8420-d4e273e62883",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "233"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780."
          },
          {
            "meta": {
              "score": 9.535817448199914,
              "Node id": "5b70a49c-0cde-47a0-86b2-c5e8aea473c2",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "233"
            },
            "text": "[113]G. E. Hinton, S. Osindero, and Y.-W. Teh ,A fast learn-\ning algorithm for deep belief nets , Neural Comput., 18 (2006),\npp. 1527–1554.\n[114]J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Sal-\nimans,Axial attention in multidimensional transformers , ArXiv,\nabs/1912.12180 (2019).\n[115]S. Hochreiter ,The vanishing gradient problem during learning\nrecurrent neural nets and problem solutions , International Journal\nof Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (1998),\npp. 107–116.\n[116]S. Hochreiter and J. Schmidhuber ,Long short-term mem-\nory, Neural Comput., 9 (1997), pp. 1735–1780.\n[117]J. J. Hopfield ,Neural networks and physical systems with emer-\ngent collective computational abilities , Proceedings of the National\nAcademy of Sciences of the United States of America, 79 (1982),\npp."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How is the value matrix generated in the self-attention block of Funnel-Transformer?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 13.28291175843535,
              "Node id": "83a193b4-e78d-4920-a14c-4cc5a947f88b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2."
          },
          {
            "meta": {
              "score": 13.563721741936497,
              "Node id": "6bc2cc7e-13f9-4a9a-a59a-cdc330efd19a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1."
          },
          {
            "meta": {
              "score": 13.660074502167584,
              "Node id": "f7c1cf80-4237-4554-ba78-8f5922c296bc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2."
          },
          {
            "meta": {
              "score": 0.8435613616809315,
              "Node id": "b3f3d39c-a672-496b-933a-5d0f630bc8ca",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "23"
            },
            "text": "Transformers: Basics and Introduction ■23\nFigure 2.10 Self-attention inputs mapped to query, keys, and values and\ngenerated output for each input.\nself-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings."
          },
          {
            "meta": {
              "score": 0.8483266562822943,
              "Node id": "f86f9476-fc5e-462a-a859-77c51e9e8cda",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "23"
            },
            "text": "self-attention. Fig. 2.10 shows how the input vectors, xi, are converted\nto the output vectors, zi, through the self-attention layer. Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1."
          },
          {
            "meta": {
              "score": 0.84298973763973,
              "Node id": "fc3e336d-4e78-4e05-8b11-fd70a7e60569",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "23"
            },
            "text": "Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi."
          },
          {
            "meta": {
              "score": 11.130192949475267,
              "Node id": "734823cd-3039-4323-8c9a-7e9cbd074f5e",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "110"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T."
          },
          {
            "meta": {
              "score": 13.678704446638772,
              "Node id": "20a5ed17-c771-4b74-aaf6-7aef70e24df5",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "110"
            },
            "text": "110■Transformers for Machine Learning: A Deep Dive\nFigure 5.1 Schematic architecture diagram for Funnel-Transformer’s en-\ncoder. Each layer represents a block composed of several transformer\nlayers with the same sequence length. Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig."
          },
          {
            "meta": {
              "score": 11.360661814621476,
              "Node id": "55bd53b6-c5fb-4eed-91fd-926ec778ac22",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "111"
            },
            "text": "Transformer Modiﬁcations ■111\nFigure 5.2 Shows how the pooling operation between Funnel-\nTransformer’s encoder layers aﬀect the input of the next layer. h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two."
          },
          {
            "meta": {
              "score": 10.639924543470778,
              "Node id": "635641d1-875d-47c8-9072-28fa1785d942",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "111"
            },
            "text": "h\nis the output of the layer before the pooling and h′is the output of the\npooling operation. The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length."
          },
          {
            "meta": {
              "score": 0.8694055576854897,
              "Node id": "5f4926cd-50f5-46b4-8399-b19930b2e4e9",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "111"
            },
            "text": "The query matrix for the next layer is constructed\nfrom the pooled output, h′. The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1."
          },
          {
            "meta": {
              "score": 0.8638961090470622,
              "Node id": "4a7d068f-0be3-4d68-96f8-3b2f7158332e",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "111"
            },
            "text": "The key and value matrices for the next\nlayer are made from the unpooled output, h.\nThe attention weight matrix of each attention head is (T′×T), which\nhas decreasing complexity for each successive layer. The output of multi-\nhead attention has the same dimensions as h′.\nBy constructing the query from the pooled sequence and the key\nand value from the unpooled sequence, the attention mechanism tries\nto learn how the pooled and unpooled sequences should best attend to\neach other to result in high quality compression. Funnel-Transformer\nuses mean pooling with stride and window size both set to two.\nDecoder To support token-level prediction tasks where the model\nneeds to produce a full output sequence, like machine translation,\nFunnel-Transformer has an optional decoder that upsamples the com-\npressed encoder output to a full sequence length. Mencoder layers will\nhave the output sequence h(M)that has length TM=T/2M−1. It will\nbe upsampled in a single step to h(up)= [h(up)\n1,...,h(up)\nT]by repeating\neach hidden vector 2M−1times:\nhup\ni=h(M)\ni//2N−1,∀i= 1,...,T (5.5)\nx//y =floor (x/y) (5.6)"
          },
          {
            "meta": {
              "score": 0.835392093720937,
              "Node id": "b58afe9d-c0e3-43ff-9551-e58058cf1abe",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "121"
            },
            "text": "As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices."
          },
          {
            "meta": {
              "score": 0.8365739332727254,
              "Node id": "d72d289e-4dd3-4557-938a-ad6320b01e99",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "121"
            },
            "text": "As we saw in section 2.4.2.1, the output of the attention mechanism\n(before the heads are concatenated) can be represented by\nAttn (Q,K,V) = softmax(\nQKT\n√dk(\nV, (5.23)\nwhere Q,K,Vare the query, key, and value matrices, respectively.\nEach is the result of transforming the input sequence into a diﬀerent\nvector space:\nQ=XWq,∈RL×d k\nK=XWk,∈RL×d k\nV=XWv,∈RL×d v(5.24)\nwheredkis the dimension of the query and key spaces and is typi-\ncally set to d, anddvis the value dimension. The matrices Wq,Wk∈\nRd×dk, and Wv∈Rd×dvare basically rotation matrices. Each row of\na query/key/value matrix corresponds to the query/key/value vector of\ntheithtoken:\nQ=)\n])q1\n...\nqL(\n⌊[,K=)\n])k1\n...\nkL(\n⌊[,V=)\n])v1\n...\nvL(\n⌊[ (5.25)\nNote that (5.24) can be adapted for the case of multi-head attention\nbetween two sequences, X1andX2, of lengths L1andL2, respectively."
          },
          {
            "meta": {
              "score": 12.856116710053211,
              "Node id": "43959e63-23ca-483f-99aa-802b6f679905",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "122"
            },
            "text": "122■Transformers for Machine Learning: A Deep Dive\nFor two sequences, the query matrix is formed from X1and the key and\nvalue matrices are formed from X2:\nQ=X1Wk,∈RL1×dk×h\nK=X2Wk,∈RL2×dk×h\nV=X2Wv,∈RL2×dv×h(5.26)\nwhere X1∈RL1×dandX2∈RL2×d. This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention."
          },
          {
            "meta": {
              "score": 11.51708538971759,
              "Node id": "927ad7a9-a701-499c-a33f-81b4de65e262",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "122"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads."
          },
          {
            "meta": {
              "score": 11.683991749768197,
              "Node id": "193ca206-fc70-43e4-a23b-e0adf21a57cf",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "122"
            },
            "text": "This is generally what happens in\na transformer decoder block. X1∈RL×d\nThe softmax portion of (5.23) is the attention weight matrix Aij:\nAij=exp(qikT\nj√dk)\n∑\nr∈Siexp(qikTr√dk), (5.27)\nwhereSiis the set of key positions that query qican attend to.\n5.2.1.1 Multi-head self-attention\nSo far, we have only discussed single-head self-attention. Multi-head at-\ntention is mainly partitioning the matrices shown above into hpieces,\nwherehis the number of attention heads.\nEach attention head has its own query/key/value that is obtained\nby breaking the single-head versions into hequally sized pieces, that are\nindexed by n= 1,...,h:\nQn=XW(q)\nn,∈RL×d k/h\nKn=XW(k)\nn,∈RL×d k/h\nVn=XW(v)\nn,∈RL×d v/h(5.28)\nThis does not mean that we now have hquery, key, and value ma-\ntrices, but that the matrices shown in (5.28) are a part of the matrices\nshown in (5.24)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does Mama Ricotta's differ from traditional Italian restaurants in terms of its location?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 0.7717620851487127,
              "Node id": "d47bc55a-2fb2-4200-8425-b959bae0e8a8",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "59"
            },
            "text": ">>> dump_topic_and_docs(\"Largest topic\", 0)\nLargest topic size: 349\n**** Representative reviews ****\nThis place makes me cringe! I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad."
          },
          {
            "meta": {
              "score": 0.7906606101592262,
              "Node id": "a3d5cec5-c757-410d-aebb-0955ee9b9dca",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "59"
            },
            "text": "I’ve dined here with large groups\nof friends when we needed to have a big table and they all\nwanted to be bursting full of cheap food and that is really\nthe only excuse to go to this place. \\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house."
          },
          {
            "meta": {
              "score": 0.7932253091439618,
              "Node id": "9f22dc13-b0df-4070-86b5-18dc00f64826",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "59"
            },
            "text": "\\n\\nOne reviewer\nmentioned the 90’s music and the goofy food art on the\nwalls. I could not agree more that this isso funny. Whoa\nand talk about noisy. This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful."
          },
          {
            "meta": {
              "score": 0.7925749251626345,
              "Node id": "96e92c8a-d275-4e4b-92ba-96a097c0370e",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "59"
            },
            "text": "This place isdeafening inside on a\nFriday orSaturday night, worse than a cafeteria. I think\nthat everyone with a City-Pass crams inthere insearch of\nthe best two-for-one deal on a massive mound of macaroni\nslathered indreadful red sauce and salty cheese. \\n\\nI\nactually ordered a salad as my main the last time that I\ndined there because I know how universally disappointing\nthe pasta dishes were and they actually screwed up a salad.\nI am not sure what on earth it was supposed to be, but they\ncalled it a chopped salad and it had a little M next to it\ninthe menu as ifit were a specialty of the house. I asked\nfor grilled chicken on top and received a dried out piece\nof leather sitting above a mess of lettuce, beans, nuts,\ncheese and peppers. Just plain salty and awful. Everything\nwas either from a can ora jar. \\n\\nI do agree with others\nwho have said that the service isvery fast and friendly."
          },
          {
            "meta": {
              "score": 21.821889901894327,
              "Node id": "4fd15bad-62a7-437f-949a-0d94d3d86b53",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so."
          },
          {
            "meta": {
              "score": 18.625772802808353,
              "Node id": "0e879724-0009-49ba-89a1-15140988f3aa",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "60■Transformers for Machine Learning: A Deep Dive\nshovel this stuff down. \\n\\nAh well, Arrivederci (no more)\nMama Ricotta\nI met up with friends for a birthday gathering at Frankie ’s.It\nwasmyfirst time and ,while Iusually don ’t go out of my\nwayfor Italian, I was very impressed with Frankie ’s.I\nfelt like Istepped back intime .The ambiance and decor\nseemed elegant from the 50sera ,yet the friendliness of\ntheserver and the atmosphere was casual .\\n\\nThe menu\ncontained everything you ’d expect on an Italian restaurant\nmenu and everything from the bread to the appetizer to the\nentree to the wine tasted delicious. Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place ."
          },
          {
            "meta": {
              "score": 19.836315427795356,
              "Node id": "2c2186c5-434a-49dc-9b3b-eaef0e301fd0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "Frankie ’sis\ndefinitely aplace you can take friends and family to\nimpress them ,but not spend afortune doing so.\nWhen you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes."
          },
          {
            "meta": {
              "score": 20.365767381645767,
              "Node id": "a787cf8a-4641-4fa7-923f-0b54d5df35e7",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "When you think ofanice Italian restaurant ,you don ’t think it\nwould come ina strip mall, but Mama Ricotta ’sbucks the\ntrend .Not only does the atmosphere &decor give the\nimpression ofanicer Italian place ,the food ispretty\ngood .\\n\\nWhile you may bethinking that this isadinner\nonly place ,this isactually areally popular lunch place .\nThere isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake."
          },
          {
            "meta": {
              "score": 0.7790768070042948,
              "Node id": "42d88392-44e2-43f0-87fb-cfc62fb82f6c",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither ."
          },
          {
            "meta": {
              "score": 11.07185621081788,
              "Node id": "27f43994-21d5-4318-a3f6-3de9b1445817",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "60"
            },
            "text": "There isusually aline during lunch ,but itmoves pretty\nquickly ,especially ifthe outside seating isopen .While\nthefood can beatad onthe pricey side ,Ihave yet to\nhave ameal Ihaven ’t been happy with. They have plenty of\nselections for all Italian lovers so don ’texpect just the\nobvious options .\\n\\nI’d suggest this place as more of a\ndinner place, mainly because of the prices along with the\nportion sizes. If you lunch it here, it may be a long\nafternoon at work trying to stay awake. And with their wine\nselection, making this a date destination isn ’tabad idea\neither .\n[(’italian ’,0.010707434311063687) ,\n(’pasta ’,0.007218630048706305) ,\n(’sauce ’,0.004690392541116093) ,\n(’it was ’,0.003576349729937027) ,\n(’food ’,0.0035416017180294685) ,\n(’restaurant ’,0.0034094836517629345) ,\n(’salad ’,0.003321322452779836) ,\n(’olive ’,0.0032739980714160824) ,\n(’bread ’,0.0032417620081978916) ,\n(’italian food ’,0.0031995754647714428) ]\nListing 3.10 Largest topic: Italian food restaurants"
          },
          {
            "meta": {
              "score": 7.964734235859355,
              "Node id": "2c8003fa-8dd3-4854-b9cd-89f9673f4731",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "98"
            },
            "text": "The dataset consists of 560K highly polar Yelp\nreviews for training and 38K reviews for testing. Original Yelp reviews\ntake numerical score from 1 to 5 stars. This dataset is constructed by\ngrouping the 1 and 2 stars reviews into the negative sentiment class and\nthe 3 and 4 stars reviews into the positive sentiment class.\nOur model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 8.178749373992616,
              "Node id": "3d09ca08-badf-44f2-a8a6-1194d0447ed5",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "98"
            },
            "text": "Our model will use the Multilingual Universal Sentence Encoder\n(mUSE) [49, 284] for feature generation. mUSE is a Transformer en-\ncoder trained such that text which is in diﬀerent languages, but has\nsimilarmeaning,willresultinasimilarencoding.Thisisanalogoustothe\nway two words with similar meaning (and usage) will have similar word\nembeddings. mUSE supports 16 languages: Arabic, Chinese-simpliﬁed,\nChinese-traditional,English,French,German,Italian,Japanese,Korean,\nDutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian.\nIn this case study, we’ll use TensorFlow Hub to load the mUSE\nmodel, Huggingface Datasets to load the Yelp Polarity dataset, and Py-\nTorch Lightning for make training a bit simpler. mUSE internally uses\nTensorFlow Text for tokenization, so we install that as well."
          },
          {
            "meta": {
              "score": 13.261237232281601,
              "Node id": "d1740bd4-fa5d-4b1c-9352-d4b0bd78c9ac",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "105"
            },
            "text": "Multilingual Transformer Architectures ■105\nfor t, best_index, score_pair in zip(text, preds, scores):\nresults.append({\n\"text \": t,\n\"label\" :\"positive \"ifbest_index == 1 else\n\"negative\" ,\n\"score\" : score_pair[best_index]\n})\nreturn results\npredict([\" I love that restaurant!\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian."
          },
          {
            "meta": {
              "score": 12.911942495668779,
              "Node id": "84aedb9a-ca13-429a-8dd7-06c3d0a63a71",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "105"
            },
            "text": "\", \"I hate italian food.\"])\n#>> [{\"label\": ’positive’, \"score\": 0.99751616, \"text\": ’I love\nthat restaurant!’},\n# {\"label \": ’negative’, \"score\": 0.9791407, \"text\": ’I hate\nitalian food.’}]\nListing 4.6 Load best model and run inference\nSince we used USEm embeddings, we should be able to predict sen-\ntiment for non-English languages. Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well."
          },
          {
            "meta": {
              "score": 7.615978505566927,
              "Node id": "1370ac05-84c5-4d1b-9392-7ab350a5010e",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "105"
            },
            "text": "Let’s try it out. As mentioned ear-\nlier, USEm supports 16 languages: Arabic, Chinese-simpliﬁed, Chinese-\ntraditional, English, French, German, Italian, Japanese, Korean, Dutch,\nPolish, Portuguese, Spanish, Thai, Turkish, Russian. In Listing 4.7, we\ncompare sentiment predictions between pairs of languages, ﬁnding that\neven though our model was trained on a small subset of the Yelp Po-\nlarity training set, it can still perform well. We also ﬁnd that the model\ncan make accurate predictions for at least one language that is notone\nof the 16 supported languages. We use input text in four languages in\nListing 4.7: English, German, Italian, and Finnish.\nfrom pprint import PrettyPrinter\npp = PrettyPrinter()\n# English vs. German\nenglish_text = \"Our server was horrid. He messed up the order\nand didn’t even apologize when he spilled wine on my\nsister ’s hair!\"\ngerman_translation = \"Unser Server war schrecklich."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How many patents does Kenneth L. Graham have related to natural language processing?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 17.214856849443983,
              "Node id": "ca81ed20-3c26-44e7-92c3-de5fb37c29cb",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "iii"
            },
            "text": "Transformers for \nMachine Learning\nA Deep Dive\nUday Kamath\nKenneth L. Graham\nWael Emara"
          },
          {
            "meta": {
              "score": 9.784587976889709,
              "Node id": "bf33728f-e72a-420b-92b2-be21afaebc70",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "iv"
            },
            "text": "First edition published 2022\nby CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\nand by CRC Press\n4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\nCRC Press is an imprint of Taylor & Francis Group, LLC© 2022 Uday Kamath, Kenneth L. Graham and Wael Emara Reasonable efforts have been made to publish reliable data and information, but the author and pub-\nlisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S."
          },
          {
            "meta": {
              "score": 9.908551002565755,
              "Node id": "65836846-d694-4a3b-80da-57d0c18ebc34",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "iv"
            },
            "text": "ISBN: 978-0-367-77165-2 (hbk)\nISBN: 978-0-367-76734-1 (pbk)\n \nISBN: 978-1-003-17008-2 (ebk)\nDOI: 10.1201/9781003170082\nTypeset in Latin Modern  font \nby KnowledgeWorks Global Ltd.\nPublisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science)."
          },
          {
            "meta": {
              "score": 10.035695361087736,
              "Node id": "f5b2ba7a-cc35-42a5-b0ff-e68f7fe1b346",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "iv"
            },
            "text": "Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.Library of Congress Cataloging-in-Publication Data\nNames: Kamath, Uday, author. \nTitle: Transformers for machine learning : a deep dive / Uday Kamath, Kenneth L. Graham, Wael Emara. Description: First edition. | Boca Raton : CRC Press, 2022. | Includes bibliographical references and index. Identifiers: LCCN 2021059529 | ISBN 9780367771652 (hardback) | ISBN 9780367767341 (paperback) | ISBN 9781003170082 (ebook) Subjects: LCSH: Neural networks (Computer science). | Computational intelligence. | Machine learning. Classification: LCC QA76.87 .K354 2022 | DDC 006.3/2--dc23/eng/20220218 LC record available at https://lccn.loc.gov/2021059529"
          },
          {
            "meta": {
              "score": 13.500371670213266,
              "Node id": "838932df-8d49-4343-8c6d-801938d79087",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "v"
            },
            "text": "To all the researchers and frontline COVID workers\nfor their extraordinary service.\n– Uday Kamath, Kenneth L. Graham,\nand Wael Emara\nTo my parents Krishna and Bharathi, my wife\nPratibha, the kids Aaroh and Brandy, my family and\nfriends for their support.\n–Uday Kamath\nTo my wife Alyson, to my mother, my in-laws, my\nfamily and friends, thank you for the support and your\nwillingness to sacriﬁce your time with me.\n–Kenneth L. Graham\nTo my wife Noha, my parents Ali and Zainab, my\nsister Wesam, my extended family and friends, thank\nyou all for being there for me all the time.\n–Wael Emara"
          },
          {
            "meta": {
              "score": 0.7960131983858421,
              "Node id": "e76c49c4-fd0c-4280-bbfc-aa6cf0bad127",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "Authors\nUday Kamath has spent more than two decades developing analyt-\nics products and combines this experience with learning in statistics,\noptimization, machine learning, bioinformatics, and evolutionary com-\nputing. He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh."
          },
          {
            "meta": {
              "score": 0.7943225724403119,
              "Node id": "8db581bb-1a1f-4a8a-84c3-1c3a577cbb11",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "Authors\nUday Kamath has spent more than two decades developing analyt-\nics products and combines this experience with learning in statistics,\noptimization, machine learning, bioinformatics, and evolutionary com-\nputing. He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare."
          },
          {
            "meta": {
              "score": 15.86325838979507,
              "Node id": "f1b8d2f7-4b36-416f-a07e-7a264448989a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "He has contributed to many journals, conferences, and books, is\nthe author of XAI: An Introduction to Interpretable XAI, Deep Learn-\ning for NLP and Speech Recognition, Mastering Java Machine Learning,\nand Machine Learning: End-to-End Guide for Java Developers . He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling."
          },
          {
            "meta": {
              "score": 15.676481307518271,
              "Node id": "1ed8e551-4290-44b3-adc5-7136f4e587c4",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "He held\nmany senior roles: chief analytics oﬃcer for Digital Reasoning, advisor\nfor Falkonry, and chief data scientist for BAE Systems Applied Intelli-\ngence. Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization."
          },
          {
            "meta": {
              "score": 16.553552640793875,
              "Node id": "a20f956c-d603-43ff-89c2-935e44eb9edd",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "Dr. Kamath has many patents and has built commercial products\nusing AI in domains such as compliance, cybersecurity, ﬁnancial crime,\nand bioinformatics. He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr."
          },
          {
            "meta": {
              "score": 17.62463580891227,
              "Node id": "57ebbc4c-8754-464a-812d-8411efaf83c6",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "He currently works as the chief analytics oﬃcer for\nSmarsh. He is responsible for data science, research of analytical prod-\nucts employing deep learning, transformers, explainable AI, and modern\ntechniques in speech and text for the ﬁnancial domain and healthcare.\nKenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry."
          },
          {
            "meta": {
              "score": 17.52087786802487,
              "Node id": "31444a24-2143-4acf-b1fc-6db5421ce9a7",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xxiii"
            },
            "text": "Kenneth L. Graham has two decades experience solving quantita-\ntive problems in multiple domains, including Monte Carlo simulation,\nNLP, anomaly detection, cybersecurity, and behavioral proﬁling. For the\npast ten years, he has focused on building scalable solutions in NLP for\ngovernment and industry, including entity coreference resolution, text\nclassiﬁcation, active learning, automatic speech recognition, and tempo-\nral normalization. He currently works at AppFolio as a senior machine\nlearning engineer. Dr. Graham has ﬁve patents for his work in natural\nlanguage processing, seven research publications, and a PhD in con-\ndensed matter physics.\nWael Emara has two decades of experience in academia and indus-\ntry. He has a PhD in computer engineering and computer science with\nemphasis on machine learning and artiﬁcial intelligence. His techni-\ncal background and research spans signal and image processing, com-\nputer vision, medical imaging, social media analytics, machine learning,\nxxiii"
          },
          {
            "meta": {
              "score": 0.7982950434054092,
              "Node id": "ca8e72a2-d0b8-4fe7-9338-d2dc10d670dc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Improvements in computer hardware,\nprimarily through GPUs, increasing the throughput by almost 10×ev-\nery ﬁve years, and the existence of a large amount of data to learn from\nresulted in a paradigm shift in the ﬁeld. Instead of hand-engineered fea-\ntures that were the primary focus for many sophisticated applications,\nby learning from a large volume of training data, where the necessary\nfeatures emerge, the deep learning network became the foundation for\nmany state-of-the-art techniques.\nMikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al."
          },
          {
            "meta": {
              "score": 0.803318040854103,
              "Node id": "18e70a9c-f0fa-480f-81dc-14970ce023cc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "3"
            },
            "text": "Mikolov et al. and Graves proposed language models using RNNs\nand long short-term memory, which later became the building blocks for\nmany natural language processing (NLP) architectures [184, 97]. The\nresearch paper by Collobert and Weston was instrumental in demon-\nstrating many concepts such as pre-trained word embeddings, CNNs for\ntext, and sharing of the embedding matrix for multi-task learning [60].\nMikolovetal.furtherimprovedtheeﬃciencyoftrainingthewordembed-\ndings proposed by Bengio et al. by eliminating the hidden layer and for-\nmulatinganapproximateobjectiveforlearninggivingriseto“word2vec”,\nan eﬃcient large-scale implementation of word embeddings [185, 183].\nSutskever’s research, which proposed a Hessian-free optimizer to train\nRNNs eﬃciently on long-term dependencies, was a breakthrough in re-\nviving the usage of RNNs, especially in NLP [237]. Sutskever et al."
          },
          {
            "meta": {
              "score": 0.7950647003625774,
              "Node id": "f19bff41-a64c-421a-bba0-6481df2e1c72",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "240"
            },
            "text": ",\nin INTERSPEECH, T. Kobayashi, K. Hirose, and S. Nakamura,\neds., ISCA, 2010, pp. 1045–1048.\n[185]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean ,Distributed representations of words and phrases and\ntheir compositionality , in Advances in Neural Information Process-\ningSystems26,C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahra-\nmani, and K. Q. Weinberger, eds., Curran Associates, Inc., 2013,\npp. 3111–3119.\n[186]M. Minsky and S. A. Papert ,Perceptrons: An introduction to\ncomputational geometry , MIT press, 2017.\n[187]G. Montavon, S. Lapuschkin, A. Binder, W. Samek,\nand K.-R."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "In what scenarios is global attention beneficial in transformer models?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 0.8515776502836343,
              "Node id": "4bf3631c-a08f-460c-a88c-914112e218b1",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xvii"
            },
            "text": "Foreword\nRenowned AI pioneer and Nobel laureate Herbert Simon underscored\n“attention” as the most valuable resource of the information econ-\nomy, as necessary to allocate attention eﬃciently among the over-\nabundance of information resources. Having written the foundational\npaper on meaning-aware AI and recently having served as MIT-\nPrinceton-USAF-AFRL AI Faculty-SME, I had the privilege of pub-\nlishing by invitation in the same journal’s special issue of ASQ, and of\nbeing the Malcolm Baldrige National Quality Award administrator, as\nwell as being ranked along with Dr. Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention."
          },
          {
            "meta": {
              "score": 0.8474977441881683,
              "Node id": "dd405aa2-a9c1-482b-b82a-05afeb3440db",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xvii"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier."
          },
          {
            "meta": {
              "score": 0.8481569026094764,
              "Node id": "571452e5-5f62-44ba-a06e-2963656cf4bd",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xvii"
            },
            "text": "Simon in the same global academic\ncitation impact studies.\nGiven the above background, I am thrilled to share with you the\nmost thorough and up-to-date compendium of research, practices, case\nstudies, and applications available today that can provide the best ROI\non the latest AI technological advances on transformers inspired by the\npaper, “Attention is All You Need.” Since Google introduced transformer\narchitecture in 2017, transformers have provided exponential improve-\nments in context-focused realization toward meaning-aware AI as deep\n(neuralnetwork)learningmodelsbaseduponattentionmechanismssuch\nas dot-product attention and multi-head attention. Resulting advances\nin enhanced parallel processing of sequential data have made eﬃcient\ncontext sensitive and hence more “meaningful” for ever-larger datasets\nand much more feasible than earlier.\nCovering the latest advances in neural network architectures related\nto transformers spanning applications such as Natural Language Pro-\ncessing (NLP), speech recognition, time series analysis, and computer\nvision and domain-speciﬁc models spanning science, medicine, and ﬁ-\nnance, the book aims to meet the theoretical, research, application, and\npractical needs across academia and industry for multiple audiences in-\ncluding postgraduate students and researchers, undergraduate students,\nindustry practitioners, and professionals."
          },
          {
            "meta": {
              "score": 7.93187230666987,
              "Node id": "6a406521-eda5-4a1c-95e7-30b78873c3a3",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xx"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?"
          },
          {
            "meta": {
              "score": 7.157218271744327,
              "Node id": "3ddf53db-e7bc-435e-8069-d487e923bb39",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xx"
            },
            "text": "xx■Preface\n•Acomprehensivereferencebookfor detailedexplanationsforevery\nalgorithm and technique related to transformers.\n•Over 60transformer architectures covered in a comprehensive\nmanner.\n•A book for understanding how to apply the transformer techniques\nin diﬀerent NLP applications, speech, time series, and computer\nvision.\n•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld."
          },
          {
            "meta": {
              "score": 7.412455702600983,
              "Node id": "34418f4c-d622-4c80-afd5-e5cf47242d62",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xx"
            },
            "text": "•Practical tips and tricks for each architecture and how to use it in\nthe real world.\n•Hands-on case studies providing practical insights to real-world\nscenarios in diverse topics such as machine translation, topic min-\ning, zero-shot multilingual classiﬁcation, sentiment analysis, au-\ntomatic speech recognition, and text classiﬁcation/categorization\nare covered in suﬃcient detail from the task, process, and analysis\nperspective, all ready to run in Google Colab.\nWHO IS THIS BOOK WRITTEN FOR?\nThe theoretical explanations of the state-of-the-art transformer archi-\ntectures will appeal to postgraduate students and researchers (academic\nandindustry)asitwillprovideasingle-entrypointwithdeepdiscussions\nof a quickly moving ﬁeld. The practical hands-on case studies and code\nwill appeal to undergraduate students, practitioners, and professionals\nas it allows for quick experimentation and lowers the barrier to entry\ninto the ﬁeld.\nTransformers are already a cornerstone for NLP deep learning archi-\ntectures."
          },
          {
            "meta": {
              "score": 6.000373361027249,
              "Node id": "5058ba58-4ef1-47a4-9124-03fa48c97930",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "135"
            },
            "text": "• ⌊] ⌊]]· · • • · · • · · · · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction."
          },
          {
            "meta": {
              "score": 5.709711595879051,
              "Node id": "c0765be1-f555-4139-8e9d-e17fdc83e8d3",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "135"
            },
            "text": "· · · ⌊⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊]· · • • · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions."
          },
          {
            "meta": {
              "score": 5.488169675441036,
              "Node id": "b3d03e21-33fd-432a-a3df-8137d66a002f",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "135"
            },
            "text": "• · · • · · · · · ⌊] ⌊)· · • • · · • · · · · · [\n· · • • · · • · · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig."
          },
          {
            "meta": {
              "score": 5.652332362959962,
              "Node id": "f0b8e79c-c2f9-45b8-abaa-48884409e5e1",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "135"
            },
            "text": "· · · · ·\nFigure 5.11 Global attention adjacency matrix for the internal trans-\nformer construction, for L= 12andG= 3,4,7. Rowicorresponds\nto queryi. Columns with a •are keys that query iattends to and·\nrepresents a lack of attention (a missing edge).\nGlobal attention Big Bird also allows some tokens to attend to all\ntokens in the sequence. These global tokens are also attended to by all\ntokens. Big Bird uses two types of global tokens: internal transformer\nconstruction and external transformer construction.\nIn the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]."
          },
          {
            "meta": {
              "score": 5.918041825022973,
              "Node id": "65fd314d-54e5-47da-8439-cd5f0ca41a7b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "135"
            },
            "text": "In the internal transformer construction, a subset of the Lvertex,G,\narepromotedtoglobaltokens.Thusthequeriesorkeysinthosepositions\nattend to all other positions. Here, A(i,:) =A(:,i) = 1,∀i∈G. The\nexpanded adjacency matrix Bis shown in Fig. 5.11.\nThe external transformer construction adds gadditional tokens to\ntheexisting Ltokens.Theadditionaltokensareglobal.Examplesinclude\nspecial tokens used in transformers, like [CLS]. This essentially creates a\nnew adjacency matrix, B, that includes the special tokens by prepending\ngrows and columns onto A. Here,B(i,:) =B(:,i) = 1, wherei=\n1,...,g, andB(g+i,g+j) =A(i,j), whereiandj= 1,...,L. The\nexpanded adjacency matrix Bis shown in Fig. 5.12.\nFinally,anexampleadjacencymatrixforthecombinationofrandom,\nsliding window, and global attention (external construction) is shown in\nFig. 5.13 ."
          },
          {
            "meta": {
              "score": 7.377423992720805,
              "Node id": "54647c80-0ec6-48cc-b455-98299b3067fa",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "136"
            },
            "text": "136■Transformers for Machine Learning: A Deep Dive\n)\n]]]]]]]]]]]]]]]]]]]]])••••••••••••\n••••••••••••\n••••••••••••\n•••·········\n•••·········\n•••·········\n•••·········\n•••·········\n•••·········\n•••·········\n•••·········\n•••·········(\n⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊⌊[\nFigure 5.12 Global attention adjacency matrix for the external trans-\nformer construction,"
          },
          {
            "meta": {
              "score": 0.8546779966612428,
              "Node id": "f2e3eb51-afa8-4e1e-b88e-8a49fdbdeb9d",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "137"
            },
            "text": "3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space.\nTalking-Heads Attention (THA) also partitions the attention heads\ninto three types: heads for the queries and keys, heads for the value, and\nheads for the attention logits and attention weights."
          },
          {
            "meta": {
              "score": 0.8430463233962346,
              "Node id": "92c0a977-8a9a-4bc7-b1e4-512801337b94",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "172"
            },
            "text": "Note that, for a single patch, the spatial and temporal attention\nare both linear. Spatial is O(N)and temporal is O(F), so the divided\nscales asO(N+F). The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc."
          },
          {
            "meta": {
              "score": 0.8496144375730136,
              "Node id": "2695aa44-e946-4fd7-b57b-67e20a85abd0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "172"
            },
            "text": "The joint attention is O(N·F).\nThe paper ﬁnds that in many cases, spatial attention is more im-\nportant than temporal attention. But, there are cases where the tem-\nporal attention is very important. Another ﬁnding is that the divided\nspace-time attention is able to learn more than the full, joint space-time\nattention because the divided case treats them as two separate attention\nmechanisms, and thus it has twice the parameters and can learn more, in\nprincipal. Because of this, the recommended attention method is divided\nspace-time attention.\n6.6 GRAPH TRANSFORMERS\nCan transformers be applied to graph datasets? When a transformer\nuses a full attention mechanism, meaning it has no hard-coded sparsity,\nit treats an input sequence as a fully-connected graph. This is true for\ntext, images, videos, etc. We saw this for text data with Big Bird in\nsection 5.2.2.4, for images with Vision Transformer in section 6.2.1, and\nwith video for TimeSformer in section 6.5."
          },
          {
            "meta": {
              "score": 0.8473438380457511,
              "Node id": "7585a066-f69c-4f9d-95d9-90f635e41ba8",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "188"
            },
            "text": "Saliency maps of attention on image or text highlights\nthe parts of the input that are important from the model per-\nspective for decision-making (classiﬁcation, recognition, question-\nanswering, etc.), with the output mimicking how trained hu-\nmans associate a focus-based mechanism as a form of explana-\ntion [195, 127, 254,106,154].\n2.Safety. When deployed in applications that directly or indi-\nrectly impact human life, the transformer-based models should be\ndeemed safe. One can qualify safety in terms of the (i) consistent\nand deterministic behavior, i.e., given the same input, the out-\nput remains the same every time, (ii) robust and reliable under\nstandard and exceptional conditions, and (iii) the ability to guard\nagainst choices that negatively impact society in general [191].\n3.Trust. Dependable models are the ones that do not need valida-\ntion."
          },
          {
            "meta": {
              "score": 0.8441900565260425,
              "Node id": "2f33dca8-9c2a-4e11-99f4-94760f6d404d",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "206"
            },
            "text": "2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights. Based on the F1 scores\non all six classiﬁcation datasets comparing the uniform and learned at-\ntention weights, the news datasets show no variations and hence are not\nused for the subsequent two analyses."
          },
          {
            "meta": {
              "score": 0.8444130129532256,
              "Node id": "199cc21d-5362-48a0-8c19-7f0e7d163579",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "208"
            },
            "text": "208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent."
          },
          {
            "meta": {
              "score": 0.844148872249445,
              "Node id": "97a82132-5049-46ca-bd9b-ff9600b5c044",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "208"
            },
            "text": "Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent.\n7.5 QUANTIFYING ATTENTION FLOW\nAs discussed in the previous two sections, correlating the attention\nweights to inputs for explanation in a simple BiLSTM with a single\nattention layer before the output itself is an open research topic. In\ntransformers with self-attention, multiple attention heads, and many at-\ntention layers in the encoder, the problem becomes even more diﬃcult."
          },
          {
            "meta": {
              "score": 5.782519650948618,
              "Node id": "d57b609e-917d-4c9c-a133-452dd525242f",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "246"
            },
            "text": "285–286.\n[240]S. Tan, R. Caruana, G. Hooker, P. Koch, and A. Gordo ,\nLearning global additive explanations for neural nets using model\ndistillation , arXiv preprint arXiv:1801.08640, (2018).\n[241]Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and\nC. Zheng ,Synthesizer: Rethinking self-attention in transformer\nmodels, ArXiv, abs/2005.00743 (2021).\n[242]Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan ,\nSparse Sinkhorn attention , in ICML, 2020.\n[243]Y. Tay, M. Dehghani, D. Bahri, and D. Metzler ,Eﬃcient\ntransformers: A survey , ArXiv, abs/2009.06732 (2020)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the importance of consistent distance between two time-steps across sentences of various lengths."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 0.791777579799372,
              "Node id": "5c02d225-60c0-4d69-8bac-cb5e98077e45",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 12.574537045125032,
              "Node id": "31c00bb6-14f2-4944-912c-55d6123b7151",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 0.7950897516178046,
              "Node id": "71591f35-f5f6-455a-9076-4f280f08e6e3",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 13.915879984177455,
              "Node id": "d8122dd0-a824-45a4-b415-24c40e404656",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "21"
            },
            "text": "Transformers: Basics and Introduction ■21\nFigure 2.8 Positional encoding for 8positions with dimensionality 3.\nknown as positional encoding. One can derive various requirements for\neﬀective positional encodings. They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective."
          },
          {
            "meta": {
              "score": 0.8057054331261521,
              "Node id": "935a9672-2c06-4d0d-b1c4-c83a0ea91f9a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "21"
            },
            "text": "They are\n1. Unique encoding value for each time-step (word in the sentence).\n2. Consistentdistancebetweentwotime-stepsacrosssentencesofvar-\nious lengths.\n3. Encoding results are generalized independent of the length of the\nsentence.\n4. The encoding is deterministic.\nOne trivial way of accomplishing all the requirements for positional en-\ncoding is to use binary representation. Fig. 2.8 highlights how with a\nvector of size or depth 3, we can generate 8positional encodings using\nbinary values that meet all the requirements given above. The represen-\ntation of each bit as grey (0) and white (1) shows how each position is\ndiﬀerent and has a constant diﬀerence. Using binary values is very costly\nfrom a memory perspective.\nIf the length of the sentence is given by land the embedding di-\nmension/depth is given by d, positional encoding Pis a2-d matrix of\nsame dimension, i.e., P∈Rl×d."
          },
          {
            "meta": {
              "score": 0.798816138168541,
              "Node id": "191f655a-bf91-46e0-8c66-c18a5425796a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "28"
            },
            "text": "Then we tokenize the sentences, convert the words to token IDs, and\nappend <bos> and <eos> IDs to the beginning and end of the token\nID sequences. Padding the variable-length sequences to the maximum\nobserved length in the batch using the <pad> token ensures a ﬁxed-\nsize tensor for training and evaluation.\nThe total of 135,842language pairs after ﬁltering reduce to 131,951\nand we further split it into 80% training, 10% validation and 10% test\ndata, i.e., 105,460, 13,308, and 13,183respectively.\nFigs. 2.13 and 2.14 show the distribution plots as histograms for En-\nglish/French and joint distribution. Most of the sentences in the parallel\ncorpus are between 4and8tokens/words length.\nFigure 2.13 SentencelengthdistributionforEnglishandFrenchsentences."
          },
          {
            "meta": {
              "score": 0.7894445281510666,
              "Node id": "b7342bcc-4234-4482-9a13-1fffd10141d9",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "75"
            },
            "text": "Next Sentence Prediction (NSP) In the context of sentence-level\npre-training tasks, NSP assists the model in learning associations be-\ntween phrases [71]. It is a binary sentence pair classiﬁcation problem\nthat learns to identify consecutive sentences. For two sentences xandy,\nthe[CLS]token vector representing the aggregate representation of the\ntwo sentences (x,y)is passed to the Sigmoid layer to obtain the proba-\nbility of being consecutive sentences. To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora."
          },
          {
            "meta": {
              "score": 0.7928337976417346,
              "Node id": "d99d5047-927d-4585-b141-a1117b2170ee",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "75"
            },
            "text": "To prepare for training, the phrase\npairs are created such that ﬁfty percent of the occurrences are consec-\nutive and the remaining ﬁfty percent are not consecutive. Pre-training\nthe model at the sentence level is beneﬁcial in downstream tasks like\nquestion answering (QA) , natural language inference (NLI), and se-\nmantic text similarity (STS), which need sentence pairs as input. Let\nl∈{1, 0}represents two sentences (x,y)being consecutive or not, NSP\nloss is deﬁned as follows:\nL(x,y)\nNSP =−logP (l|x,y) (4.2)\n4.1.2.2 Cross-Lingual Language Model (XLM)\nCross-Lingual Language Model (XLM) [146] is an improvement over\nthe mBERT architecture by learning from monolingual and parallel cor-\npora. For learning from monolingual data XLM uses the standard MLM\npre-training task used by mBERT."
          },
          {
            "meta": {
              "score": 0.7851190544692803,
              "Node id": "7afb36a2-f1ae-4bec-9d6e-ec7218d8f6df",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "82"
            },
            "text": "82■Transformers for Machine Learning: A Deep Dive\nencouraged to minimize the distance between Ax→yandAy→xduring\ntraining.\nCross-lingual Sentence Alignment (CLSA) CLSA [121] is de-\nsigned to enforce the alignment of sentence representations across lan-\nguagesusingparalleldata.Let MandPbethemonolingualandparallel\ndata,respectively.Foreachparallelsentencepair (x,y),distinctsentence\nembeddings for both xandyare generated separately, namely cxand\ncy, respectively. cxandcyare obtained by averaging the last layer’s\nembeddings of the encoder. The embedding alignment takes place by\nencouraging the model to predict the correct translation yfor an input\nsentencexby using the following loss function:\nL(x,y)\nCLSA =−logecT\nxcy\n∑\ny′∈M∪PecTxcy′(4.9)\nwherey′inL(x,y)\nCLSAcan be any sentence and with no language restriction."
          },
          {
            "meta": {
              "score": 12.809942197159412,
              "Node id": "a388ffa3-3735-4336-b0f8-1a398e0a58a0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "84"
            },
            "text": "84■Transformers for Machine Learning: A Deep Dive\nFigure 4.5 Illustration of BTMLM [194] pre-training task. The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM."
          },
          {
            "meta": {
              "score": 12.732628781586214,
              "Node id": "b7e348e0-543b-4885-b8f1-5c9a4f26a967",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "84"
            },
            "text": "The ﬁrst step\non the left is where a pre-trained CAMLM is used to generate pseudo-\nparallel sentences. The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well."
          },
          {
            "meta": {
              "score": 12.360040559388196,
              "Node id": "e4247ffa-de78-4cf9-a77f-b75c4bd63211",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "84"
            },
            "text": "The second step on the right is where the generated\npseudo-parallel sentences are then used to further train the CAMLM.\nconsists of two steps; the ﬁrst step generates pseudo-parallel data from\na given monolingual corpus. ERNIE-M [194] constructs pseudo-parallel\nsentences by ﬁrst pre-training the model using CAMLM and then adding\nplaceholder masks to the end of the original monolingual sentence to\nshow the location and language that the model should generate. The\nsecond step masks the tokens in the original monolingual sentence, then\nconcatenates it with the created pseudo-parallel sentence. Finally, the\nmodel should predict the masked tokens. Fig. 4.5shows the two steps of\nBTMLM.\nDue to their superior performance in XLM architecture, the mMLM\nand TLM pre-training tasks are employed as part of training ERNIE-\nM as well. ERNIE-M is trained with monolingual and parallel corpora\nwith 96 languages and is initialized with XLM-R weights."
          },
          {
            "meta": {
              "score": 0.789114731256071,
              "Node id": "ffc8c082-e682-42bd-98a6-6c86ac0101cf",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "124"
            },
            "text": "124■Transformers for Machine Learning: A Deep Dive\nlengthincreases.Forexample,ifthesequencelengthdoubles,theamount\nof time needed to compute and store the attention weights will increase\nfourfold.\n5.2.2 Reducing Complexity of Self-Attention\nThis section discusses several transformer models that reduce the com-\nputational complexity of multi-head self-attention.\n5.2.2.1 Longformer\nWhen calculating self-attention (omitting the causal requirement for the\nself-attention between the encoder and decoder blocks) there are usually\nno restrictions on which positions in the sequence can attend to each\nother. This means that, in principle, the matrix of attention weights for\nevery head could be dense. When viewed as a graph, it corresponds to a\nfully-connected, weighted bipartite graph. If the sequence has Ltokens,\nthen there would be L(L−1)/2edges. Longformer [25] changes this by\nrestricting which positions can attend to each other according to speciﬁc\npatterns."
          },
          {
            "meta": {
              "score": 0.7870889008501416,
              "Node id": "60b8acfc-45e1-4501-810e-da5d738c4e5d",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "162"
            },
            "text": "There is no gradient updating or ﬁne-tuning.\nThe model is assessed on 12 NLP datasets.\nResults showed that as the number of model parameters increases,\nthe model needs fewer demonstrations to learn how to perform a task\n(and hence to reach a given accuracy target). When few-shot is used,\nmodel performance increases more quickly with model size, suggesting\nlarger models are better at in-context learning.\nEvaluation and conditioning the model For few-shot, Kexamples were\nrandomly drawn from the training set (or dev set if there was no labeled\ntraining set) to be used as conditioning, separated by 1–2 newlines. K\nvaried from 0 to the size of the context window. Some examples just\nhave a description of the task used as conditioning. For multiple-choice\ntasks,Kexamples of context with the correct choice were used as con-\nditioning and one additional example of context only was the item to\nbe completed. As with GPT-2, the language model likelihood score for\neach possible completion was scored."
          },
          {
            "meta": {
              "score": 11.950473400468121,
              "Node id": "1abfbe46-43d2-4629-9a5a-043f72977669",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "164"
            },
            "text": "In the standard transformer [254], the residual\nconnection was added prior to LayerNorm .\nThroughaseriesofexperimentswithViT,[78]demonstratesthatthe\ninductive biases introduced by CNNs are useful for small datasets, but\nnot for larger ones. With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important."
          },
          {
            "meta": {
              "score": 12.220310890149005,
              "Node id": "206da6b2-3775-4856-b1a2-76833b59527b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "164"
            },
            "text": "With larger datasets the model can learn the relevant\ncorrelations on its own, as has been shown for various Transformers.\nViT also shows that the spatial relationship between patches (distance\ninside the image) is learned by the positional encodings. Patches that\nare close to each other end up with similar positional encodings. The\ntwo-dimensional spatial correlations are also learned by the positional\nencodings,i.e.,patchesinthesameroworcolumnhavesimilarpositional\nencodings.Theexperimentsalsodemonstratedthathard-codingthetwo-\ndimensional structure of the image patches into the positional encodings\ndoes not improve quality. This is likely because building inductive biases\ninto a model as versatile as a transformer prevents it from learning on\nits own what is or is not important.\nLastly, the Vision Transformer investigates a modiﬁcation to the self-\nattention mechanism, axial attention [126, 114]. Axial attention, where\nattention is between patches in the same row or the same column."
          },
          {
            "meta": {
              "score": 12.959541157419865,
              "Node id": "ea0e22f3-9d92-4182-9da5-f5f46813cfed",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "165"
            },
            "text": "The length of the sequence, T, is the\nnumber of time steps in the audio. Some spans of in the sequence of\nspeech representations are then masked.\nThe encodings are able to be learned because the speech is decom-\nposed into discrete speech units akin to the WordPiece tokens used as\ninputs into a text Transformer. The speech units are a ﬁnite set of dis-\ncrete units of the audio sequence and are shorter than phonemes (they’re\n25 ms in length). The latent speech encodings are analogous to the em-\nbeddings learned in the initial embedding layer in a text transformer.\nThese masked encodings are passed into a transformer to build con-\ntextualized representations. A contrastive loss function [219, 250] lets\nthe wav2vec 2.0 transformer learn the relative importance of the speech\nunits.\nNote that the discrete speech units also enable cross-lingual train-\ning, where the model learns which units are only used for a particular\nlanguage and which units are used across multiple languages."
          },
          {
            "meta": {
              "score": 11.883181366006006,
              "Node id": "64214022-ddc2-4875-ac05-4461a9417dda",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "204"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          },
          {
            "meta": {
              "score": 12.156723860543643,
              "Node id": "ebe938e7-9a64-4c8d-bcb8-0316f7dd3112",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "207"
            },
            "text": "Interpretability and Explainability Techniques for Transformers ■207\n(SST) is a borderline case and shows a small diﬀerence as compared to\nthe MIMIC (III) and IMDB dataset.\n7.4.2.2 Searching for adversarial models\nTo ﬁnd attention weight distributions that mimic the base model pre-\ndictions, the authors propose a model-consistent training protocol for\nﬁnding adversarial attention distributions through a combined parame-\nterization that holds for all training examples. The two measures they\nemploy for the adversarial training are Total Variation Distance (TVD)\nand Jensen-Shannon Divergence (JSD)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Describe the computation process for each hidden unit in layer normalization."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 10.725948186659043,
              "Node id": "26a67481-b37f-497e-a2ac-01b9bf0b8a38",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xiii"
            },
            "text": "Contents ■xiii\n6.2 COMPUTER VISION 163\n6.2.1 Vision Transformer 163\n6.3 AUTOMATIC SPEECH RECOGNITION 164\n6.3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6."
          },
          {
            "meta": {
              "score": 9.529403399126764,
              "Node id": "cbaccf22-4855-41ac-9dd9-5f41178c8f8c",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xiii"
            },
            "text": "3.1 Wav2vec 2.0 165\n6.3.2 Speech2Text2 165\n6.3.3 HuBERT: Hidden Units BERT 166\n6.4 MULTIMODAL AND MULTITASKING TRANSFORMER 166\n6.4.1 Vision-and-Language BERT (VilBERT) 167\n6.4.2 Uniﬁed Transformer (UniT) 168\n6.5 VIDEO PROCESSING WITH TIMESFORMER 169\n6.5.1 Patch Embeddings 169\n6.5.2 Self-Attention 170\n6.5.2.1 Spatiotemporal self-attention 171\n6.5.2.2 Spatiotemporal attention blocks 171\n6.6 GRAPH TRANSFORMERS 172\n6.6.1 Positional Encodings in a Graph 173\n6.6.1.1 Laplacian positional encodings 173\n6.6.2 Graph Transformer Input 173\n6.6.2.1 Graphs without edge attributes 174\n6."
          },
          {
            "meta": {
              "score": 8.787829127545507,
              "Node id": "095f82b1-acd4-48bc-9ef8-e1c02866c875",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv."
          },
          {
            "meta": {
              "score": 10.447249361042964,
              "Node id": "e4bfda1e-7b0e-49d2-bd8c-d093f5cc0dd5",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "maskedAttention (Q,K,V) = softmax(QKT+M√dk)\nV(2.20)\n2.4.2.4 Encoder-decoder multi-head attention\nIn the decoder side there is a need to learn the attention relationship\nbetween the entire source input and the target output at a given time.\nTherefore, the query vectors from the target sequence (before a given\ntime) and the keys and values from the entire input sequence of the\nencoder are passed to the self-attention layer in the decoder as shown in\nFig. 2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2."
          },
          {
            "meta": {
              "score": 11.841630912291016,
              "Node id": "d8264704-9d6e-483e-bfcc-9e6cf8aafe6d",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]."
          },
          {
            "meta": {
              "score": 11.17852679960571,
              "Node id": "b551f1b8-f040-4c62-8889-dc21d16dc62a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "2.7.\n2.4.3 Residuals and Layer Normalization\nSimilar to ResNets, the inputs, X, are short circuited to the out-\nput,Z, and both are added and passed through layer normalization\naddAndNorm (X+Z)[105]. Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130]."
          },
          {
            "meta": {
              "score": 11.754472301575257,
              "Node id": "bc718b1f-dd06-4ed1-a6f5-828c940c8377",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "Layer normalization ensures each layer to\nhave 0mean and a unit ( 1) variance.\nFor each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch."
          },
          {
            "meta": {
              "score": 11.097354492074137,
              "Node id": "2ade33bd-9c79-41c9-9b32-a658c564f0ee",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "For each hidden unit, hi, we can compute\nhi=g\nσ(hi−µ) (2.21)\nwheregis the gain variable (can be set to 1), µis the mean given by\n1\nH∑H\ni=1hiandσis the standard deviation given by∑\n1\nH(hi−µ)2.\nLayer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers."
          },
          {
            "meta": {
              "score": 10.071033255622178,
              "Node id": "dd9c79eb-478f-4783-b559-bb611303d216",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "26"
            },
            "text": "Layer normalization reduces the covariance shift , i.e., the gradient\ndependencies between each layer, and therefore speeds up the conver-\ngence as fewer iterations are needed [13]. This is related to batch nor-\nmalization, where batch normalization happens at one hidden unit level\nand a 0mean and a unit ( 1) variance is achieved on that one batch [130].\nAdvantage of layer normalization is that it works independent of the\nbatch size, i.e., can give a single example, small batch or a large batch.\n2.4.4 Positionwise Feed-forward Networks\nBoth encoder and decoder contain a fully connected feed-forward net-\nwork after the attention sub layers. For each position, similar linear\ntransformations with a ReLU activation in between is performed.\nFFN (x) = max(0,xW 1+b1)W2+b2 (2.22)"
          },
          {
            "meta": {
              "score": 0.8167136650436424,
              "Node id": "76be3954-fa06-4139-9da2-38cb74c2194b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "110"
            },
            "text": "Shows three pooling operations\nbetween blocks, with each decreasing the sequence length of the output\nby half.\nbefore the pooling operation. This is shown in Fig. 5.1. If the output of a\ngiven layer is h, then the output of the pooling layer is h′=Pooling (h),\nwhere h∈RT×dandh′∈RT′×d, for someT′<T.\nh′is used to construct the query and the residual connection for the\nself-attention block, his used for the key and value vectors:\nQ=h′WQ,∈RT′×dk(5.1)\nK=hWK,∈RT×dk(5.2)\nV=hWV,∈RT×dv(5.3)\nThis relationship between the unpooled and pooled outputs and the\nquery, key, and value matrices of the next layer is shown in Fig. 5.2. The\noutput of the (n+ 1)stlayer is then\nh(n+1)=LayerNorm (h′(n)+multihead (Q(h′(n)),K(h(n)),V(h(n)))\n(5.4)"
          },
          {
            "meta": {
              "score": 10.91298746988286,
              "Node id": "c28027bd-55dc-4ee0-a760-9e07d5b30487",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "175"
            },
            "text": "˜h(l+1)\niispassedintotheremainderofthetransformerlayerasfollows,\nresulting in the output of the transformer layer, h(l+1)\ni:\nh(l+1)\ni =Norm (h′(l+1)\ni+h′′(l+1)\ni) (6.23)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.24)\nh′′(l+1)\ni =W(l)\n2ReLU(\nW(l)\n1h′(l+1)\ni)\n(6.25)\nwhere W(l)\n1∈R2d×d,W(l)\n2∈Rd×2d, andNormcan be layer normal-\nization or batch normalization. The structure of the Graph Transformer\ndescribed in (6.23)–(6.25) is shown in Fig. 6.3."
          },
          {
            "meta": {
              "score": 0.817263682794774,
              "Node id": "3ca318ef-ae63-4a4c-8244-5cf68557c84a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "176"
            },
            "text": "31)\nh′(l+1)\ni =Norm (h(l)\ni+˜h(l+1)\ni) (6.32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization."
          },
          {
            "meta": {
              "score": 0.8172548289157651,
              "Node id": "5c97006f-aa17-4a3b-8572-15c8c24a7add",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "176"
            },
            "text": "32)\nh′′(l+1)\ni =W(l)\nn,2ReLU(\nW(l)\nn,1h′(l+1)\ni)\n(6.33)\ne(l+1)\nij =Norm (e′(l+1)\nij+e′′(l+1)\nij) (6.34)\ne′(l+1)\nij =Norm (e(l)\nij+˜ e(l+1)\nij) (6.35)\ne′′(l+1)\nij =W(l)\ne,2ReLU(\nW(l)\ne,1e′(l+1)\nij)\n(6.36)\nwhere W(l)\nn,1,W(l)\ne,1∈R2d×d,W(l)\nn,2,W(l)\ne,2∈Rd×2d, andNormcan be layer\nnormalization or batch normalization. Subscripts nandeare for nodes\nand edges, respectively. This is shown schematically in Fig. 6.4."
          },
          {
            "meta": {
              "score": 0.8071053614374204,
              "Node id": "802c2499-3815-406e-9a07-ace3c3ff4177",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "222"
            },
            "text": "[13]L.J.Ba,J.R.Kiros,andG.E.Hinton ,Layer normalization ,\nCoRR, abs/1607.06450 (2016).\n[14]S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek ,On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propagation , PloS\none, 10 (2015), p. e0130140.\n[15]D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Müller ,How to explain indi-\nvidual classiﬁcation decisions , The Journal of Machine Learning\nResearch, 11 (2010), pp. 1803–1831.\n[16]A. Baevski, H. Zhou, A. rahman Mohamed, and M. Auli ,\nwav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations , ArXiv, abs/2006.11477 (2020)."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "Explain the formula for computing the attention with queries in a cluster in the context of Transformer modifications."
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 9.58532942138775,
              "Node id": "83a193b4-e78d-4920-a14c-4cc5a947f88b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "Tools, and Libraries 98\n4.4.3 Experiments, Results, and Analysis 98\n4.4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2."
          },
          {
            "meta": {
              "score": 9.804075392553274,
              "Node id": "6bc2cc7e-13f9-4a9a-a59a-cdc330efd19a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "4.3.1 Data preprocessing 99\n4.4.3.2 Experiments 101\nChapter 5■Transformer Modiﬁcations 109\n5.1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1."
          },
          {
            "meta": {
              "score": 9.879273946534337,
              "Node id": "f7c1cf80-4237-4554-ba78-8f5922c296bc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xi"
            },
            "text": "1 TRANSFORMER BLOCK MODIFICATIONS 109\n5.1.1 Lightweight Transformers 109\n5.1.1.1 Funnel-transformer 109\n5.1.1.2 DeLighT 112\n5.1.2 Connections between Transformer Blocks 114\n5.1.2.1 RealFormer 114\n5.1.3 Adaptive Computation Time 115\n5.1.3.1 Universal transformers (UT) 115\n5.1.4 RecurrenceRelationsbetweenTransformerBlocks 116\n5.1.4.1 Transformer-XL 116\n5.1.5 Hierarchical Transformers 120\n5.2 TRANSFORMERS WITH MODIFIED MULTI-HEAD\nSELF-ATTENTION 120\n5.2.1 Structure of Multi-Head Self-Attention 120\n5.2.1.1 Multi-head self-attention 122\n5.2.1.2 Space and time complexity 123\n5.2.2 Reducing Complexity of Self-Attention 124\n5.2.2.1 Longformer 124\n5.2.2."
          },
          {
            "meta": {
              "score": 12.532986974237497,
              "Node id": "4bf27fe8-f2bb-429c-86e0-2ef24fe293c4",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "xii"
            },
            "text": "xii■Contents\n5.2.5 Prototype Queries 140\n5.2.5.1 Clustered attention 140\n5.2.6 Compressed Key-Value Memory 141\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention 141\n5.2.7 Low-Rank Approximations 143\n5.2.7.1 Linformer 143\n5.3 MODIFICATIONS FOR TRAINING TASK EFFICIENCY 145\n5.3.1 ELECTRA 145\n5.3.1.1 Replaced token detection 145\n5.3.2 T5 146\n5.4 TRANSFORMER SUBMODULE CHANGES 146\n5.4.1 Switch Transformer 146\n5.5 CASE STUDY: SENTIMENT ANAL YSIS 148\n5.5.1 Goal 148\n5.5.2 Data, Tools, and Libraries 148\n5.5.3 Experiments, Results, and Analysis 150\n5.5.3.1 Visualizing attention head weights 150\n5.5.3."
          },
          {
            "meta": {
              "score": 0.8606031414156504,
              "Node id": "660b82b2-13b1-4fc0-80b7-a5693694502c",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "24"
            },
            "text": "The logical ﬂow of all the computations carried out for each token i\nfrom input to output is demonstrated in Fig. 2.11.\nInstead of a vector computation for each token i, input matrix\nX∈Rl×dwherelis the maximum length of the sentence and dis\nthe dimension of the inputs, combines with each of the query, key, and\nvalue matrices as a single computation given by\nattention(Q, K,V) = softmax(QKT\n√dk)\nV (2.17)\n2.4.2.2 Multi-head attention\nInstead of a single self-attention head, there can be hparallel self-\nattention heads; this is known as multi-head attention. In the original\ntransformer paper, the authors used h= 8heads. Multi-head attention\nprovides diﬀerent subspace representations instead of just a single rep-\nresentation for the inputs, which helps capture diﬀerent aspects of the\nsame inputs. It also helps the model expand the focus to diﬀerent posi-\ntions."
          },
          {
            "meta": {
              "score": 0.8615272726089345,
              "Node id": "9663d105-a99d-44af-9e84-e7f28e49f074",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "123"
            },
            "text": "Transformer Modiﬁcations ■123\nFigure 5.6 The query matrix, Q, can be partitioned into hheads, as de-\nscribed in (5.28).\nNote that for the multihead case, QnKT\nnis divided by√dk/hinstead of√dk. This change account s for the change in eﬀective dimension of the\nquery and key spaces to dk/h. The attention heads are then combined\nas described in (2.19).\n5.2.1.2 Space and time complexity\nComputing the attention weight matrix described in section 5.27 takes\nO(L2·dk)matrix multiplications and computing the context vector in\nsection 5.23 needsO(L2·dv)matrix multiplications, so the time com-\nplexity of self-attention is O(L2·dk+L2·dv).\nConsider a single input sequence of Ltokens and that the query,\nkey, and value share the same dimensionality, so dk=dv=dmodel."
          },
          {
            "meta": {
              "score": 0.867041289313354,
              "Node id": "31f1d49d-60cb-4cd7-8f03-bf43d905c066",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "127"
            },
            "text": "Transformer Modiﬁcations ■127\nRecall that in the scaled dot-product attention, the query, key, and\nvalue matrices are the result of transforming the matrix of dmodel-\ndimensional input vectors into queries and keys of dimension dkand\nvalues of dimension dv.\nIn the equation for A, the computationally expensive term is the\nproduct QKT, moreover, once the softmax function is applied, only the\nlargest terms along each dmodeldimension are important. This means\nthat for each query vector in Q, we only need the keys in Kthat are\nclosest to it. To make this easier, they set Q=K, meaning that for\neach query vector, we only need to ﬁnd the closest queries. This is an\napproximate nearest neighbors problem, so we can use locality-sensitive\nhashing (LSH).\nLocality-sensitive hashing Locality-sensitive hashing, or LSH, was\nintroduced in 1998, in [129] as a method of approximate similarity search\nbased on hashing."
          },
          {
            "meta": {
              "score": 0.860855527869792,
              "Node id": "9c31197f-4731-4dcb-97a5-bf1d096c58bc",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "132"
            },
            "text": "132■Transformers for Machine Learning: A Deep Dive\nAij= exp(\nqikT\nj√dk)\n, so thatDij=δij∑\nr∈SiAir. ThusAttn(Q, K,V) =\nD−1AV. The queries and keys can be scaled so that the key dimension\nis absorbed into them, Aijis simply exp(qikT\nj).\nSinceAijdepends on the inner product of the query and key vectors,\nit is a measure of similarity between qiandkj. These attention weights\ncan be approximated using the FAVOR+ algorithm:\nAij=⟨φ(qT\ni)Tφ(kT\nj)⟩, (5.52)\nwheremapping φmaps Q,K∈RL×d ktoQ′,K′∈RL×r,respectively,\nwithr >0."
          },
          {
            "meta": {
              "score": 0.8674035968770041,
              "Node id": "81dc0e6d-e5fd-47f1-86b3-39b27da289e5",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "137"
            },
            "text": "So, instead of Lqueries and keys, there will be L/b\nqueries and keys. This modiﬁes each attention pattern discussed above\nin relatively simple ways:\n1.Random attention The random number of keys for a query to\nattend to, r, becomes the random number of key blocks that a\nquery block attends to.\n2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations."
          },
          {
            "meta": {
              "score": 0.8691834429931428,
              "Node id": "d03204c9-565c-49b9-8126-a119df7fca58",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "137"
            },
            "text": "2.Sliding window attention The query block iattends to key\nblocksi−(w−1)/2throughi+ (w−1)/2.\n3.Globalattention Thedeﬁnitionofglobalattentionisunchanged,\nexcept it is deﬁned in terms of blocks rather than sequence posi-\ntion.\n5.2.3 Improving Multi-Head-Attention\nNow we focus on some of the ways the attention mechanism has been\nchanged to improve performance of the transformer.\n5.2.3.1 Talking-heads attention\nVaswani et al. [254] showed that multi-head attention allows the trans-\nformer to perform h(number of attention heads) separate attention\ncalculations. Talking-Heads Attention [227] instead allows the atten-\ntion heads to share information. It works by adding two linear layers\nthat project the product of the query and key matrices, QKT(at-\ntention logits), into a new space and projects the attention weights,\nSoftmax(QKT), into a new space."
          },
          {
            "meta": {
              "score": 9.841120139494414,
              "Node id": "a7ab0207-158f-4abc-b325-da5f3abfc108",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "140"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance."
          },
          {
            "meta": {
              "score": 10.259118589500408,
              "Node id": "ca9522b4-e006-4e3c-879e-db79d6454f98",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "140"
            },
            "text": "We also discussed another example of biasing attention with\npriors in section 5.1.2.1, the Realformer.\n5.2.5 Prototype Queries\n5.2.5.1 Clustered attention\nClustered attention [256] is a method to avoid self-attention’s O(L2·dk+\nL2·dv)time complexity that linearizes the self-attention weight compu-\ntation by clustering LSH hashed queries with the k-means clustering\nalgorithm. And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors."
          },
          {
            "meta": {
              "score": 10.776820541130977,
              "Node id": "9581c310-d233-4b65-810c-b71d60839196",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "140"
            },
            "text": "And using the query centroids as the queries to compute the\nattention matrix.\nClustering query vectors Clustered attention happens in two\nstages. First, each query vector is hashed with locality-sensitive hash-\ning. The hashed queries are then grouped into Cclusters with k-means.\nThe distance metric used for k-means is the Hamming distance. The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism."
          },
          {
            "meta": {
              "score": 10.755348577432148,
              "Node id": "0ebf7077-aaa4-4bcb-bc40-355d8bebb928",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "140"
            },
            "text": "The\ncentroid of the jthcluster is given by\nqc\nj=∑L\ni=1Sijqi∑L\ni=1Sij(5.66)\nwhere qc\njis the centroid of the jthcluster and the matrix S∈\n{0,1}L×Cpartitions the query vectors into Cnon-overlapping clusters,\nso ifSij= 1, then qiis in cluster j. The centroid queries are grouped\nintoQc,∈RC×dk, the matrix of centroid vectors. We can then substitute\nthe real query matrix with the query centroid matrix, Qcand compute\nthe clustered attention matrix:\nAc= softmax(\nQcKT\n√(dk)(\n,∈RC×L(5.67)\nYou can stop here and just use the clustered attention weights to\ncompute the output of the attention mechanism. This calculation has\ntime complexity of O(CL·dk+LC·dv), which is explicitly linear in\nthe sequence length."
          },
          {
            "meta": {
              "score": 12.634138332866613,
              "Node id": "52ebe9f3-73c5-4aef-a158-06ac85a025c2",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "141"
            },
            "text": "Transformer Modiﬁcations ■141\neach of the Cclusters. And, for each of the top-k keys for a cluster,\ncompute the attention with the queries in that cluster:\nAt\nil={ ˆmjexp(\nqiklT)\n∑L\nr=1Tjrexp(\nqikrT),ifTjl= 1\nAc\njl, otherwise(5.68)\nwhere ˆmj=∑L\ni=1TijAc\nijandT∈{0,1}C×L: ifTij= 1, then kiis one of\nthe top-k keys for in cluster j.\nThen compute the context vectors (weighted average of the values)\nof the clustered attention and use it as the value matrix: ˆV=AtV,∈\nRL×d v. This makes the complexity of the clustered attention calculation\ntoO(CL·dk+LC·dv+kLmax(dk,dv)), which is linear in the sequence\nlength."
          },
          {
            "meta": {
              "score": 10.165457404346592,
              "Node id": "bde0ac02-2d01-4b6e-ab4c-700249c2c0d0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "141"
            },
            "text": "It performs better\non GLUE that RoBERTa, but not SQuAD, where it is slightly worse.\nAs number of clusters increases, the approximation becomes more\naccurate.Itconvergesuptotwiceasfastasthestandardtransformer,for\nlongsequencelengthsand,forshortsequencelengths,clusteredattention\nisnotfaster than the standard transformer.\n5.2.6 Compressed Key-Value Memory\n5.2.6.1 Luna: Linear Uniﬁed Nested Attention\nLuna [177], which stands for Linear Uniﬁed Nested Attention, replaces\ntheattentionweightcomputationineachattentionheadwithtwonested\nlinear attention computations using an extra, learnable, input sequence\nthat learns to encode contextual information: P∈Rl×d, wherelis the\nlength of the sequence.\nAs discussed earlier, the output of an attention head between a query\nsequence, X∈Rn×dand a context sequence, C∈Rm×d, can be written\nas\nY=Attn(X, C) = softmax(\nXWq(CWk)T\n√\ndk/h(\nCV,∈Rn×d(5.69)"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "What is the role of the \"embedding\" and \"label\" variables in the compute_loss function?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 7.241078126933564,
              "Node id": "fc3e336d-4e78-4e05-8b11-fd70a7e60569",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "23"
            },
            "text": "Each input\nvector, xi, generates three diﬀerent vectors: the query, key, and value,\n(qi,ki,vi,). The query, key, and value vectors are obtained by projecting\nthe input vector, xi, at timeion the learnable weight matrices Wq,Wk,\nandWvtoget qi,ki,andvi,respectively.Thesequery/key/valueweight\nmatrices are randomly initialized and the weights are jointly learned\nfrom the training process. For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi."
          },
          {
            "meta": {
              "score": 7.781283925661214,
              "Node id": "d9c9ac53-1b64-4159-92c9-c1bf343e043f",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "23"
            },
            "text": "For the ﬁrst attention layer of the encoder\nand decoder, the inputs are the summation of the word embeddings and\npositional encodings.\nSimilar to the attention discussion in section 2.3 where we discussed\nthe query, key, and values, and how they impact the ﬁnal attention\nscores, the self-attention has all three vectors generated for every input\nand the following are their key roles:\n1. The role of the query vector of token i,qi, is to combine with every\nother key vectors∑l\nj=0qikjTto inﬂuence the weights for its own\noutput, zi.\n2. Theroleofthekeyvectoroftoken i,ki,istobematchedwithevery\nother query vectors to get similarity with query and to inﬂuence\nthe output through query-key product scoring.\n3. The role of the value vector of token i,vi, is extracting information\nby combining with the output of the query-key scores to get the\noutput vector zi."
          },
          {
            "meta": {
              "score": 0.78143028902952,
              "Node id": "85279826-f1f2-437c-a5ca-e056db47b411",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "35"
            },
            "text": "Transformers: Basics and Introduction ■35\n(a) Loss.\n (b) Perplexity.\nFigure 2.16 Attention-based seq2seq loss and perplexity on training and\nvalidation sets.\nThe outputs help visualizing and diagnosing issues in the data and the\nmodel. For example, Fig. 2.17(a) shows how English word “going” pays\nattention to “je” and “vais” and similarly how the “store” word pays\nattention to “au”, “magasin”, “.” and “<eos>”.\n2.5.3.3 Transformer\nThe Listing 2.6 shows transformer model wrapping the PyTorch trans-\nformer block."
          },
          {
            "meta": {
              "score": 0.7837358298849552,
              "Node id": "d2b37954-a0fe-45dc-9973-cc8bf56cb644",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "38"
            },
            "text": "The validation loss plateau’s at a value less than 2 in epoch 20,\ncomparing to the value around 2.5 in the attention mechanism. Also,\nthe perplexity of attention is almost double of the transformer model in\nthe validation set.\n(a) Loss curves.\n (b) Perplexity measures.\nFigure 2.18 Transformer loss and perplexity on training and validation\nsets.\nFig. 2.19 shows comparative performance of attention-based and\ntransformer-based models on the same test dataset. Perplexity of trans-\nformers is almost three times less than that of attention proving the\nbeneﬁts of the architecture in the real-world translation problem.\n2.5.3.5 Explainability\nWe can use the gradient values for the input sequence to illustrate each\ngenerated token’s dependence on each input token. We start by perform-\ning a forward pass on embedded inputs. We then take the token with\nthe highest logit value (in the same way as greedy decoding earlier), and\nperform a backward pass from the highest logit value."
          },
          {
            "meta": {
              "score": 7.495850454278617,
              "Node id": "a58b2714-1026-438a-ad44-912a9ca030d5",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "52"
            },
            "text": "However, the last layer is too close\nto the target functions (i.e., masked language model and next sentence\nprediction) during pre-training, therefore may be biased to those targets.\n3.4.2 BERTology\nThe great push BERT has provided to the NLP ﬁeld triggered much\nresearch into understanding how it works and the type of knowledge it\nextracts through massive pre-training. BERTology aims to answer some\nof the questions about why BERT performs well on so many NLP tasks.\nSome of the topics addressed by BERTology include the type of knowl-\nedge learned by BERT and where it is represented. In general there\nthree types of knowledge BERT acquires: Syntactic knowledge, Seman-\ntic knowledge, World knowledge.\nBERT representations of syntactic knowledge are hierarchical rather\nthan linear, i.e., they include a syntactic tree structure in addition to\nthe word order information. Additionally, BERT embeddings store infor-\nmation about speech segments, grammatical chunks, and roles."
          },
          {
            "meta": {
              "score": 0.793121526658526,
              "Node id": "b0e3ff13-1368-4634-a6ec-ff6975fd53c0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "86"
            },
            "text": "86■Transformers for Machine Learning: A Deep Dive\nFigure 4.6 Illustration of Language-agnostic BERT Sentence Embedding\n(LaBSE) architecture [88].\nwhereφis the scoring function of the similarity between the representa-\ntions ofxiandyi\nDuring training P(yi|xi)is approximated by sampling negatives,\nyn, from translation pairs in the same batch:\nPapprox (yi|xi) =eφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.12)\nTherefore, for parallel source and target pairs (xi,yi), the model can be\noptimized using the log-likelihood objective [283]:\nLs=−1\nNN\ni=1logeφ(xi,yi)\neφ(xi,yi)+∑N\nn=1,n̸=ieφ(xi,yn)(4.13)\nFor eachxi, the lossLsaims to identify the correct yi."
          },
          {
            "meta": {
              "score": 0.781464175090916,
              "Node id": "1cfb5afc-399a-42c6-866a-161490a2723b",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "87"
            },
            "text": "φ′(xi,yj) ={\nφ(xi,yj)−mifi=j\nφ(xi,yj) ifi̸=j(4.15)\nApplyingφ′(xi,yj)to the bidirectional loss Ls, the additive margin\nbidirectional loss becomes as follows [283]:\nLams=−1\nNN\ni=1eφ(xi,yi)−m\neφ(xi,yi)−m+∑N\nn=1,n̸=ieφ(xi,yn)\nLams=Lams+L′\nams(4.16)\nAs mentioned earlier, LaBSE pre-trains its BERT based encoders\nwith Masked Language Model (MLM) and Translation Language Model\n(TLM) pre-training tasks using monolingual and parallel corpora, re-\nspectively. LaBSE trains transformer encoders using a three-stage pro-\ngressive stacking approach. For a Llayer encoder, it ﬁrst learning a\nmodel offracL 4layers, then fracL 2layers, and lastly all Llayers."
          },
          {
            "meta": {
              "score": 6.8761964967964015,
              "Node id": "062b5ff6-1382-42d2-a935-46396dc97e27",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "100"
            },
            "text": "100■Transformers for Machine Learning: A Deep Dive\n\"\"\"\nself.test_ds = load_dataset( ’yelp_polarity ’,\nsplit= \"test [:2%] \")\nself.train_ds = load_dataset( ’yelp_polarity ’,\nsplit= \"train [:2%] \")\nself.val_ds = load_dataset( ’yelp_polarity ’,\nsplit= \"train [99%:] \")\n#Map class labels toaninteger\nself.label_names = self.train_ds.unique( \"label \")\nlabel2int = { str(label): n for n, label in\nenumerate (self.label_names)}\nself.encoder = encoder_factory(label2int)\ndefsetup(self):\n#Compute embeddings inbatches ,tospeed things up\nself.train = self.train_ds. map(self.encoder,\nbatched=True, batch_size=self.batch_size)\nself.train.set_format( type =\"torch \",\ncolumns=[ \"embedding \",\"label \"],\noutput_all_columns=True)\nself.val = self.val_ds."
          },
          {
            "meta": {
              "score": 8.11447310694721,
              "Node id": "570c9cca-5f61-46d3-9c6c-e3ef21ea6b3a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "101"
            },
            "text": "Multilingual Transformer Architectures ■101\npin_memory=self.pin_memory)\ndef test_dataloader(self):\nreturn DataLoader(self.test,\nbatch_size=self.batch_size,\nnum_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ()."
          },
          {
            "meta": {
              "score": 8.371018051184967,
              "Node id": "712b5ef9-f1f9-470f-82fd-7ac113923761",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "101"
            },
            "text": "num_workers=self.num_workers)\ndef encoder_factory(label2int: Dict[str, int]):\n\"Returns a function that encodes each text example and each\nlabel \"\ndef encode(batch):\nbatch[ \"embedding\" ] = embed_text(batch[\"text\"])\nbatch[ \"label\" ] = [label2int[str(x)] for xin\nbatch[ \"label\" ]]\nreturn batch\nreturn encode\nListing 4.3 Load model and tokenizer\n4.4.3.2 Experiments\nNext, we deﬁne the model architecture in Listing 4.4.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_metric\nclass Model(pl.LightningModule):\ndef __init__(self,\nhidden_dims: List[int] = [768, 128],\ndropout_prob: float = 0.5,\nlearning_rate: float = 1e-3):\nsuper ().__init__()\nself.train_acc = load_metric(\"accuracy\")\nself.val_acc = load_metric(\"accuracy\")\nself.test_acc = load_metric(\"accuracy\")\nself."
          },
          {
            "meta": {
              "score": 0.7895326605445452,
              "Node id": "e38eec66-023a-4398-8b31-9c5e13d458c2",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "159"
            },
            "text": "Optimization was with\nstochastic gradient descent.\nSupervised ﬁne-tuning In this phase, the model is ﬁne-tuned on la-\nbeled, task-speciﬁc corpus, C, where each data point is a token sequence\nx= (x1,...,xm)and a class label y. The pre-trained decoder model is\nused as a feature generator for the labeled data and a fully-connected\nlinear layer, with softmax activation and weight matrix W, is appended\nto it and trained by minimizing a second negative log-likelihood\nL2(C) =−\n(x,y)logP(y|x;W) (6.2)\nRadford et al. found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C)."
          },
          {
            "meta": {
              "score": 0.7858819928130156,
              "Node id": "3e7aab7d-c565-452c-8ad4-14491f65331f",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "159"
            },
            "text": "found that the model converged faster and general-\nized better when the language modeling objective from the unsupervised\nphase, (6.1), was added to (6.2). So, the full objective was the weighted\nsumL2(C) +λL1(C).\nFormatting data for ﬁne-tuning Data for each of the four training tasks\nis formatted diﬀerently:\n•Textclassiﬁcationdatahasasimpleformat;eachinstanceisbrack-\neted with a start and an end token, so the input is formatted like\n[⟨s⟩,text,⟨/s⟩].\n•A natural language inference (NLI) instance has two parts, the\npremise,p, and the hypothesis, h. Labels can be entailment, con-\ntradiction, or neutral. The input is formatted like [⟨s⟩,p,$,h,⟨/s⟩],\nwhere $ is a delimiter token."
          },
          {
            "meta": {
              "score": 0.795371016592256,
              "Node id": "36eb2eda-62be-4ef2-8af3-cf963459956a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "174"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer."
          },
          {
            "meta": {
              "score": 0.7945945543525701,
              "Node id": "7efc995e-abd9-4eb5-9fc5-d9fa4bfd5c4f",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "174"
            },
            "text": "174■Transformers for Machine Learning: A Deep Dive\nembeddings, resulting in node embeddings for node i,˜h(0)\niand edge\nembeddings ˜ e(0)\nijbetween nodes iandj:\n˜h(0)\ni=A(0)αi+a(0)(6.16)\n˜ e(0)\nij=B(0)βij+b(0), (6.17)\nwhere A(0)∈Rd×dnandB(0)∈Rd×dnare the node and edge embedding\nmatrices, respectively, and a(0)andb(0)are bias terms for the nodes and\nedges, respectively. The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers."
          },
          {
            "meta": {
              "score": 0.7930966269186338,
              "Node id": "cb2b969e-8e1d-4489-97a8-07867fd7ea63",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "174"
            },
            "text": "The superscript (0)denotes that this is the input\nlayer.\nThe Laplacian positional encodings λialso get embedded into a\nd−dimensional space with an additional learnable embedding layer\nC(0)∈Rd×k, to generate Laplacian positional embeddings λ(0)\ni:\nλ(0)\ni=C(0)λi+c(0)(6.18)\nh(0)\ni=˜h(0)\ni+λ(0)\ni (6.19)\nNote that c(0)∈Rdis a bias term for the Laplacian positional embed-\nding, h(0)is the full node embedding, and Laplacian positional embed-\ndings are only computed for the input layer and are not used inside\nTransformer layers.\n6.6.2.1 Graphs without edge attributes\nThere are two ways to structure a graph Transformer, depending on\nwhether the graph has edge attributes or not."
          },
          {
            "meta": {
              "score": 7.823540886187196,
              "Node id": "bd9d4548-eaf3-468b-b32e-ec63ff631cbb",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "199"
            },
            "text": "Syntactic information probing tasks in-\nvestigates syntax-based properties, for example, “are the embeddings\nsensitive to word order?”, using a classiﬁcation dataset with bigrams\nshifted as positives and non-shifted as negatives. Finally, semantic infor-\nmation probing tasks investigate semantics-based attributes retained in\nthe embeddings, for example, “can the embeddings understand tenses?”,\nusing a tense classiﬁcation dataset where VBP/VBZ forms are labeled as\npresent and VBD as past tense. The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation."
          },
          {
            "meta": {
              "score": 7.730231036895169,
              "Node id": "1e9daaa6-e048-47a6-829f-d0695750f8b7",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "199"
            },
            "text": "The comprehensive experiments in this\nwork with diﬀerent architectures and downstream tasks provided great\ninsights into model architectures and their ability to preserve diﬀerent\nlinguistic properties.\nTenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks."
          },
          {
            "meta": {
              "score": 9.006104712154285,
              "Node id": "ca17a3c2-7e8d-4ff6-9034-0ede4d41e5ea",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "199"
            },
            "text": "Tenney et al. introduced “edge probing” to understand the hidden\nrepresentation in deep learning architectures such as ELMO, GPT and\nBERT [245]. It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model."
          },
          {
            "meta": {
              "score": 9.453853645776906,
              "Node id": "cee3f61b-9a54-40e6-9756-282e0d5eddd9",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "199"
            },
            "text": "It investigates the role of the word in each position to en-\ncode structural, syntactic, semantic, and even long-range phenomena by\nfreezing the layers and using a neural classiﬁer to train and test on vari-\nous tasks such as part-of-speech tagging (POS), constituent labeling, de-\npendency labeling, named entity labeling, semantic role labeling (SRL),\ncoreference, semantic proto-role and relation Classiﬁcation. They show\nthat contextualized embeddings improve over their non-contextualized\nequivalents, mostly on syntactic tasks compared to semantic tasks.\nTenney et al., in their work, further found that a model like\nBERT can rediscover linguistic information similar to a traditional NLP\npipeline in an interpretable and localizable way [244]. They discovered\nthe sequence: POS tagging, parsing, NER, semantic roles, coreference\nare part of the overall BERT model. They introduce two complemen-"
          },
          {
            "meta": {
              "score": 0.7826330830116266,
              "Node id": "64214022-ddc2-4875-ac05-4461a9417dda",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "204"
            },
            "text": "204■Transformers for Machine Learning: A Deep Dive\nThe experiments show a consistently low correlation between the atten-\ntion weights and feature importance scores across all the datasets, espe-\ncially for contextualized encoders .\n7.4.1.2 Counterfactual experiments\nTo validate the second hypothesis, the authors put forth two empirical\nquestions\n1. How much does the output change if the attention scores are ran-\ndomly permutated?\n2. Can we ﬁnd maximally diﬀerent attention that does not change\nthe output more than a predeﬁned threshold epsilon?"
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    },
    {
      "meta": {},
      "question": {
        "meta": {},
        "text": "How does the attention mechanism address the issue of long-distance associations in language processing?"
      },
      "facts": {
        "llm_answer": null,
        "meta": {},
        "items": []
      },
      "chunks": {
        "meta": {},
        "items": [
          {
            "meta": {
              "score": 14.093956143879094,
              "Node id": "04ffa189-391e-4ad7-8db2-abe523ae1952",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "Thus, the encoder and decoder are jointly trained, and the cross-entropy\nloss is used for optimization and is given by\nmax\nθ1\nNN\nn=1logpθ(y(n)|x(n)) (2.7)\nThe process of concatenating the <bos> and the original output se-\nquence, excluding the ﬁnal token, as the input to the decoder during\nthe training is called teacher forcing . The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies."
          },
          {
            "meta": {
              "score": 13.378858347300643,
              "Node id": "5c02d225-60c0-4d69-8bac-cb5e98077e45",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "The teacher forcing helps in ad-\ndressing the slow convergence and instability problems when training\nRNNs.\n2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]."
          },
          {
            "meta": {
              "score": 13.47661199650911,
              "Node id": "31c00bb6-14f2-4944-912c-55d6123b7151",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "2.2.4 Issues with RNN-Based Encoder-Decoder\nAsdescribedintheabovesection,completeinformationaboutthesource\nsentence is compressed and encoded in one context variable used by the\ndecoder component. As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize."
          },
          {
            "meta": {
              "score": 14.917086127550668,
              "Node id": "71591f35-f5f6-455a-9076-4f280f08e6e3",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "14"
            },
            "text": "As the input size increases, there will be a loss of\ninformation while compressing the input. The words in a sentence can\nalso have complex structure and long-distance associations based on the\nlanguage.Capturingthisinacompressedwayinasinglevectoralsoleads\nto ineﬃciencies. On the other hand, each time step’s hidden variables\non the encoder side are available and carry information to be used by\nthe decoder network. Each time step in the decoder can be inﬂuenced\ndiﬀerently by the hidden variables in the encoder. RNNs also have issues\nwith vanishing and explosive gradients [115]. One of the computational\nissues with RNNs is that the recurrence or dependence on previous time\nsteps makes the architecture very diﬃcult to parallelize.\n2.3 ATTENTION MECHANISM\n2.3.1 Background\nThe attention mechanism involves selectively focusing on speciﬁc ele-\nments while ﬁltering out the less relevant ones. The human optic nerve"
          },
          {
            "meta": {
              "score": 0.8247079495959836,
              "Node id": "d6a2630c-2138-451d-854b-c402d19288de",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "Transformers: Basics and Introduction ■15\nreceives information in the order of billion bits per second, while the\nbrain’s capacity to process is far less. Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment."
          },
          {
            "meta": {
              "score": 0.8212419053491421,
              "Node id": "b4b1df26-7e1e-4d9b-8c6c-3c90194549d1",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "Visual attention, a form of atten-\ntion, involves orienting to and sustaining focus on a stimulus such as a\nperson or inanimate object or a speciﬁc task, thus enabling the brain’s\neﬃcient processing. Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately."
          },
          {
            "meta": {
              "score": 0.8222373326441093,
              "Node id": "2545b503-c8b2-4233-89b8-054c984e8939",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "Therefore, the attention mechanism has allowed hu-\nmans to focus on only a fraction of information of interest, thus enabling\noptimum resource usage, leading to better survival and growth.\nThe “father of American psychology”, William James, created a two-\ncomponent framework to explain the visual attention mechanism [133].\nIn this framework, the spotlight of attention uses both nonvolitional\n(involuntary) and volitional (voluntary) cues to bias the sensory input.\nThenonvolitional cue is involuntary and is based on the saliency and\nnoticeability of targets in the environment. In contrast, the volitional\ncue is based on the subject’s voluntary eﬀort to focus on the target de-\nliberately. For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues."
          },
          {
            "meta": {
              "score": 8.413729143069162,
              "Node id": "7e977f24-babd-4bce-98f4-6d865589a95d",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "For example, drawing attention to speciﬁc objects by coloring\nthem diﬀerently or attending to a crying baby are nonvolitional cues.\nIn contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig."
          },
          {
            "meta": {
              "score": 8.413729143069162,
              "Node id": "aa69c8cc-2a43-4919-9048-8df73d202483",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "In contrast, attending to speciﬁc text for answering question or solving\nspeciﬁc problems are volitional cues.\nIn the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17]."
          },
          {
            "meta": {
              "score": 8.043002409460168,
              "Node id": "095f82b1-acd4-48bc-9ef8-e1c02866c875",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "In the context of attention mechanisms in deep learning, volitional\ncues map to queries, keys to nonvolitional cues, and sensory inputs to\nvalue. Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv."
          },
          {
            "meta": {
              "score": 0.8266481025249356,
              "Node id": "97738ad5-500a-4b33-b913-19716048faea",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "15"
            },
            "text": "Every sensory input (value) maps to the nonvolitional cue (key)\nof that sensory input. Attention mechanisms can be thus considered as\na process of biasing selection over values (sensory inputs) via attention\npooling, using the queries (volitional cues) and keys (nonvolitional cues)\nas shown in Fig. 2.3.\nThe attention mechanism is designed in a way to overcome the issues\ndescribed with RNN-based encoder-decoder architecture.\nAs shown in Fig. 2.3, an attention mechanism can be considered as\na memory with keys and values and a layer which, when someone queries\nit, generates an output from value whose keys map the input [17].\nTo formalize, let us consider the memory unit consisting of nkey-\nvalue pairs (k1,v1),..., (kn,vn)withki∈Rdkandvi∈Rdv. The at-\ntention layer receives an input as query q∈Rdqand returns an output\no∈Rdvwith same shape as the value v.\nThe attention layer measures the similarity between the query and\nthe key using a score function αwhich returns scores a1,...,anfor keys\nk1,...,kngiven by\nai=α(q,ki) (2.8)"
          },
          {
            "meta": {
              "score": 0.8195403574584555,
              "Node id": "5df3e553-168a-4aa5-b1e1-ce711a8beca0",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "53"
            },
            "text": "It was shown that the majority\nof self-attention heads do not encode any non-trivial linguistic informa-\ntion directly, since fewer than half of them exhibited the \"heterogeneous\"\npattern2. The vertical pattern was stored in a large portion of the model\n(attention to [CLS], [SEP], and punctuation tokens). Additionally, cer-\ntain BERT heads seem to specialize in particular sorts of syntactic re-\nlations, with heads paying much more attention to words in speciﬁc\nsyntactic places than a random baseline. Other studies discovered that\nno one head contains the whole syntactic tree. Additionally, attention\nweightsareillustrativeofsubject-verbagreementandreﬂexiveanaphora.\nAdditionally, it was shown that even when attention heads specialize in\nmonitoring semantic relations, they do not always help BERT perform\nwell on related tasks.\nFor layer-level knowledge localization, provided that the ﬁrst layer of\nBERT gets representations in the form of a mix of token, segment, and\npositionalembeddingsasinput.Itcomestoreasonthatthebottomlevels\ncontain the most linear information about word order."
          },
          {
            "meta": {
              "score": 8.784267243339285,
              "Node id": "6801564d-423e-4abc-ba0e-e0f68617488c",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "147"
            },
            "text": "Transformer Modiﬁcations ■147\nFigure 5.15 Switch Transformer encoder block illustrating two input to-\nkensx1andx2being processed through the network. The dense FFN is\nreplaced with switching FFN as one of the experts.\nin a sparse model with a substantial computational cost and training in-\nstabilities. Switch transformers address most of these issues with a novel\nrouting algorithm between the experts, enabling an increase in the num-\nber of the parameters without an increase in computational cost [87].\nThe core innovation of switch transformers is replacing the feed-forward\nlayer in the transformer with a switching feed-forward layer, as shown in\nFig. 5.15.\nIn the standard transformer, a single feed-forward network follows\nthe outputs from the multi-head attention layer. It is responsible for\ntranslating the representation token-by-token to the next transformer\ninput block. As shown in Fig. 5.15, in a switch transformer, instead of\none feed-forward network, there are multiple feed-forward networks, also\nknown as the experts."
          },
          {
            "meta": {
              "score": 8.51982096371967,
              "Node id": "76f3d289-61be-45a5-a00b-9975f1f80142",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "173"
            },
            "text": "Pre-trained and Application-Speciﬁc Transformers ■173\nfully-connected attention would be computationally intractable, since\nfull attention already has quadratic complexity for simple sequences.\nThis is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space."
          },
          {
            "meta": {
              "score": 8.359937427074286,
              "Node id": "85904681-4a1c-4bf7-b0ca-23434adc1442",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "173"
            },
            "text": "This is the purpose of the Graph Transformer introduced in [80]. It\naddresses the complexity of self-attention by letting a node attend to\nother nodes in its local neighborhood.\n6.6.1 Positional Encodings in a Graph\nAs discussed in section 5.2.1, scaled-dot product attention mechanisms\nhave quadratic complexity in both time and memory. Since graphs can\nhave a very large number of nodes, to make graph transformers com-\nputationally feasible, there must be local sparsity in the attention for\nany node. The problem with this is that generalgraphs have no no-\ntion of distance between nodes, making it non-trivial to use positional\nencodings to provide a measure of distance or locality, as is common\nin Transformers. As described in [80], this problem is solved by using\nLaplacian positional encodings [81], which are generated via a spectral\nembedding into Euclidean space.\n6.6.1.1 Laplacian positional encodings\nLaplacian positional encodings for a graph with nnodes are computed\nfrom the graph’s adjacency and degree matrices:\n∆=I−D−1/2AD−1/2(6.14)\n=UTΛU, (6.15)\nwhere Ais the adjacency matrix, Dis the degree matrix, Λis the\neigenvalue matrix (diagonal), and Uis the eigenvector matrix."
          },
          {
            "meta": {
              "score": 0.831521490685385,
              "Node id": "ff80244d-1317-4299-abf7-ab21b0b59e92",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "200"
            },
            "text": "The work\nalso answers questions such as “how does the probe design aﬀect probing\ntask performance?” and “can the probes pick spurious signals?”.\nAttention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]."
          },
          {
            "meta": {
              "score": 0.8343060711021176,
              "Node id": "4f076b06-0347-4806-8a5c-f9c5d4cab60a",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "200"
            },
            "text": "Attention Probing Probing either by adding an attention layer on top\nof an existing neural architecture or using existing attention weights\nfrom a layer of deep learning, mapping it to the inputs as “attention\nmaps” to explore the relationship between the two is soon developing as\nan eﬀective explanation technique.\nRocktäschel et al. proposed a neural word-by-word attention mech-\nanism in a sequence-to-sequence network for reasoning over entailments\nof pairs of words and phrases [211]. Visualizations of word-by-word at-\ntention between the premise and hypothesis show that irrelevant parts\nof the premise, such as words capturing little meaning, are correctly ne-\nglected for entailment. The premise and hypothesis connected via deeper\nsemantics show proper relevancy through the attention weights.\nXu et al. use an attention mechanism for automatic image caption-\ning tasks [278]. The work shows that the attention mechanism not only\nachieves state-of-the-art results but highlights salient objects in the im-\nage while generating the corresponding words in the output sequence,\nthus useful for explanations."
          },
          {
            "meta": {
              "score": 0.8209935014856298,
              "Node id": "dfa9c012-299e-42a8-8a50-ce9dcbdd0776",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "206"
            },
            "text": "The authors lay the following three requirements for faithful expla-\nnations for attention mechanisms.\n1. Attention mechanism should be a NECESSARY component for\ngood model performance.\n2. Attention distributions should be hard to manipulate, i.e., if any\ntrained model can vary the distribution of attention weights and\nyet have similar predictions, they may not be suitable for the ex-\nplanation. This directly corresponds to the exclusivity requisite\nfor faithful explanations and will guide the search for adversarial\nmodels.\n3. Attention distributions should work well in uncontextualized set-\ntings. Since the attention weights are typically learned on contex-\ntualized hidden layer outputs, to see the impact on input tokens,\none needs to use uncontextualized settings to judge their useful-\nness.\n7.4.2.1 Is attention necessary for all tasks?\nThe authors use the same three sets of tasks and six classiﬁcation\ndatasets using the BiLSTM model from the Jain and Wallace setting\nand create another model where the attention weights are uniform in\ndistribution compared to the learned weights."
          },
          {
            "meta": {
              "score": 0.8189231416712137,
              "Node id": "199cc21d-5362-48a0-8c19-7f0e7d163579",
              "display_name": "Transformers for Machine Learning_ A Deep Dive.pdf",
              "page_number": "208"
            },
            "text": "208■Transformers for Machine Learning: A Deep Dive\n7.4.2.3 Attention probing\nTo validate if the attention distributions work well in uncontextualized\nsettings, the attention weights from the BiLSTM are imposed on an\nuncontextualized trained MLP layer with the bag of word-vector rep-\nresentation. Thus, high performance in the task implies that attention\nscorescapturetherelationshipbetweentheinputandtheoutput.Except\nfor Stanford Sentiment Treebank (SST) dataset, every task and dataset\nshows the BiLSTM trained attention weights outperforming the MLP\nand the uniform weights, indicating the usefulness of attention weights.\nIn conclusion, the research has laid down three essential components\nfor validating the usefulness of the attention mechanism and three meth-\nodstoquantifyitforfaithfulexplanation.Theusefulnessoftheattention\nmechanism is shown to be task dependent."
          }
        ]
      },
      "answers": {
        "meta": {},
        "items": []
      }
    }
  ]
}